{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Skip-Gram-Notebook( Word2Vec).ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-Vyl-cz-ttb"
      },
      "source": [
        "## Skip-Gram - Word2Vec implementation in Pytorch "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YZOHerH41yM"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch as nn\n",
        "from string import punctuation\n",
        "from nltk.corpus import webtext\n",
        "from nltk.corpus import gutenberg\n",
        "from keras.preprocessing.sequence import skipgrams\n",
        "from keras.preprocessing import text\n",
        "from sklearn.metrics.pairwise import euclidean_distances"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JV-ACE2g5wHM"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('webtext')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdAj2x4WQdZt"
      },
      "source": [
        "corpus = ['The sky is blue and beautiful.',\n",
        "          'Love this blue and beautiful sky!',\n",
        "          'The quick brown fox jumps over the lazy dog.',\n",
        "          \"A king's breakfast has sausages, ham, bacon, eggs, toast and beans\",\n",
        "          'I love green eggs, ham, sausages and bacon!',\n",
        "          'The brown fox is quick and the blue dog is lazy!',\n",
        "          'The sky is very blue and the sky is very beautiful today',\n",
        "          'The dog is lazy but the brown fox is quick!'    \n",
        "]\n",
        "labels = ['weather', 'weather', 'animals', 'food', 'food', 'animals', 'weather', 'animals']\n",
        "\n",
        "corpus = np.array(corpus)\n",
        "corpus_df = pd.DataFrame({'Document': corpus, \n",
        "                          'Category': labels})\n",
        "corpus_df = corpus_df[['Document', 'Category']]\n",
        "corpus_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEhb7xQGgrDx"
      },
      "source": [
        "## PreProcesing Codes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qfzfhbbk5fm5"
      },
      "source": [
        "wordpt = nltk.WordPunctTokenizer()\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "def normalize_document(doc):\n",
        "    # lower case and remove special characters\\whitespaces\n",
        "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
        "    doc = doc.lower()\n",
        "    doc = doc.strip()\n",
        "    # tokenize document\n",
        "    tokens = wordpt.tokenize(doc)\n",
        "    # filter stopwords out of document\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "    # re-create document from filtered tokens\n",
        "    doc = ' '.join(filtered_tokens)\n",
        "    return doc\n",
        "\n",
        "normalize_corpus = np.vectorize(normalize_document)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plBzdTBIgxgD"
      },
      "source": [
        "## Vocabulary "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "binDHq8J5frX"
      },
      "source": [
        "# build a sample vocab\n",
        "vocab = []\n",
        "\n",
        "for fileid in webtext.fileids():\n",
        "    vocab.append(webtext.raw(fileid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVXWAiSyRMeE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIjlS1-J5fvN"
      },
      "source": [
        "tokenizer = text.Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "word2id = tokenizer.word_index\n",
        "\n",
        "word2id['PAD'] = 0\n",
        "id2word = {v:k for k, v in word2id.items()}\n",
        "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in corpus]\n",
        "vocab_size = len(word2id)\n",
        "embed_size = 100\n",
        "window_size = 2 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwH4hsPsQ1rq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9AzKmQig3Bm"
      },
      "source": [
        "## [target_word to context_words pairs and labels]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHxWDy9M646t"
      },
      "source": [
        "skip_grams = [skipgrams(wid, vocabulary_size=vocab_size, window_size=10) for wid in wids]\n",
        "pairs, labels = skip_grams[0][0], skip_grams[0][1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNl4Ez6ng_qa"
      },
      "source": [
        "# Skip Gram Model Architecture "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2egJL9sM44kg"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class skipgram(nn.Module):\n",
        "    \n",
        "  def __init__(self, vocab_size, embedding_dim=100):\n",
        "    super(skipgram, self).__init__()\n",
        "    \n",
        "    self.u_embeddings = nn.Embedding(vocab_size, embedding_dim, sparse=True)   \n",
        "    self.v_embeddings = nn.Embedding(vocab_size, embedding_dim, sparse=True) \n",
        "    self.lin = nn.Linear(embedding_dim,1)\n",
        " \n",
        "  def forward(self, u_pos, v_pos ):\n",
        "\n",
        "    embed_u = self.u_embeddings(torch.Tensor([u_pos]).long())\n",
        "    embed_v = self.v_embeddings(torch.Tensor([v_pos]).long())\n",
        "    score  = torch.mul(embed_u, embed_v)\n",
        "    score = self.lin(score)\n",
        "    print(score)\n",
        "    target = F.sigmoid(score).squeeze()\n",
        "    print(target)\n",
        "    return target\n",
        "\n",
        "model = skipgram(vocab_size)\n",
        "loss_function = nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFe0gluEhEHd"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRBHCiVy5YsT"
      },
      "source": [
        "for epoch in range(1, 10):\n",
        "  tloss = 0\n",
        "  for i, elem in enumerate(skip_grams):\n",
        "    pair_first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n",
        "    pair_second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n",
        "    labels = np.array(elem[1], dtype='int32')\n",
        "    optimizer.zero_grad()\n",
        "    for v,u,l in zip(pair_first_elem,pair_second_elem,labels):            \n",
        "      p = model(v, u).unsqueeze(-1)\n",
        "      loss = loss_function(p,torch.Tensor([1]))\n",
        "      loss += loss.data\n",
        "      loss.backward()\n",
        "      tloss+=loss\n",
        "      optimizer.step()\n",
        "    print('Epoch:', epoch, '\\tLoss:', tloss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5JyNGPu5YuN"
      },
      "source": [
        "weights = model.u_embeddings(torch.Tensor([list(range(0,vocab_size))]).long())\n",
        "pd.DataFrame(weights.view(-1,100).tolist(), index=list(id2word.values())[0:]).head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_aQQvAP7nXR"
      },
      "source": [
        "weights = weights.view(-1,100)\n",
        "distance_matrix = euclidean_distances(weights.detach().numpy())\n",
        "\n",
        "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:6]+1] \n",
        "                   for search_term in [ 'is', 'fox', 'and','brown','lazy']}\n",
        "\n",
        "similar_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6G9sIV-FTQNg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}