{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UypYA6kX7rrH"
   },
   "source": [
    "## CBOW - Word2Vec Implementation Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9nhCCVg-pfuh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import webtext\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import text\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.options.display.max_colwidth = 200\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wtEi1_Ewp38e",
    "outputId": "e35ef7d5-b7b3-4917-a77b-bdcd8caa2dd6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sajid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package webtext to\n",
      "[nltk_data]     C:\\Users\\Sajid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package webtext is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('webtext')\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2iN6sWbdpfum"
   },
   "source": [
    "## Pre-Processing text Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "cB1A2KCQpfuo"
   },
   "outputs": [],
   "source": [
    "wordpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = wordpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "rlEIdkRYpfup",
    "outputId": "a4227224-17ad-4c41-929b-105cfa7ba89d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The sky is blue and beautiful.</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Love this blue and beautiful sky!</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The quick brown fox jumps over the lazy dog.</td>\n",
       "      <td>animals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A king's breakfast has sausages, ham, bacon, eggs, toast and beans</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I love green eggs, ham, sausages and bacon!</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The brown fox is quick and the blue dog is lazy!</td>\n",
       "      <td>animals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The sky is very blue and the sky is very beautiful today</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The dog is lazy but the brown fox is quick!</td>\n",
       "      <td>animals</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             Document Category\n",
       "0                                      The sky is blue and beautiful.  weather\n",
       "1                                   Love this blue and beautiful sky!  weather\n",
       "2                        The quick brown fox jumps over the lazy dog.  animals\n",
       "3  A king's breakfast has sausages, ham, bacon, eggs, toast and beans     food\n",
       "4                         I love green eggs, ham, sausages and bacon!     food\n",
       "5                    The brown fox is quick and the blue dog is lazy!  animals\n",
       "6            The sky is very blue and the sky is very beautiful today  weather\n",
       "7                         The dog is lazy but the brown fox is quick!  animals"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = ['The sky is blue and beautiful.',\n",
    "          'Love this blue and beautiful sky!',\n",
    "          'The quick brown fox jumps over the lazy dog.',\n",
    "          \"A king's breakfast has sausages, ham, bacon, eggs, toast and beans\",\n",
    "          'I love green eggs, ham, sausages and bacon!',\n",
    "          'The brown fox is quick and the blue dog is lazy!',\n",
    "          'The sky is very blue and the sky is very beautiful today',\n",
    "          'The dog is lazy but the brown fox is quick!'    \n",
    "]\n",
    "labels = ['weather', 'weather', 'animals', 'food', 'food', 'animals', 'weather', 'animals']\n",
    "\n",
    "corpus = np.array(corpus)\n",
    "corpus_df = pd.DataFrame({'Document': corpus, \n",
    "                          'Category': labels})\n",
    "corpus_df = corpus_df[['Document', 'Category']]\n",
    "corpus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FghWaqo1pfuq",
    "outputId": "7a9fc7ac-c911-482d-b6c7-b44c83f32d73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['firefox.txt', 'grail.txt', 'overheard.txt', 'pirates.txt', 'singles.txt', 'wine.txt']\n",
      "564601\n"
     ]
    }
   ],
   "source": [
    "# build a sample vocab\n",
    "vocab = []\n",
    "print(webtext.fileids())\n",
    "print(len(webtext.raw('firefox.txt'))) \n",
    "for fileid in webtext.fileids():\n",
    "    vocab.append(webtext.raw('firefox.txt'))\n",
    "\n",
    "    #print(brown.raw('cb01').strip()[:1000])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgL7JfiPpfuq"
   },
   "source": [
    "### text preprocessing (Remove tags e.g HTML,Remove special characters, Remove stopwords) === Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iCm0sMaspfur",
    "outputId": "b14788d5-98c6-4d1d-aa83-e5cfd1edf8a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 31\n",
      "Vocabulary Sample: [('the', 1), ('is', 2), ('and', 3), ('sky', 4), ('blue', 5), ('beautiful', 6), ('quick', 7), ('brown', 8), ('fox', 9), ('lazy', 10)]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "word2id = tokenizer.word_index\n",
    "\n",
    "word2id['PAD'] = 0\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in corpus]\n",
    "\n",
    "vocab_size = len(word2id)\n",
    "embed_size = 100\n",
    "window_size = 2\n",
    "\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Vocabulary Sample:', list(word2id.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HT4Hrfo_pfus"
   },
   "source": [
    "### [context_words, target_word] pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "hdWqc3v4pfut"
   },
   "outputs": [],
   "source": [
    "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
    "    X = []\n",
    "    Y = []\n",
    "    context_length = window_size*2\n",
    "    for words in wids:\n",
    "        sentence_length = len(words)\n",
    "        for index, word in enumerate(words):           \n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "            context = [words[i] for i in range(start, end)if 0 <= i < sentence_length and i != index]\n",
    "            x = sequence.pad_sequences([context], maxlen=context_length)\n",
    "            X.append(x)\n",
    "            Y.append(word)\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HAgVUa1Qpfuu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EsdpsBjQpfuv"
   },
   "source": [
    "## CBOW (Contineous bag of Words Model architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UqCLwsVGpfuv",
    "outputId": "449b5450-8632-45f3-c1b5-b3b371446c90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "embeddings.weight \t torch.Size([31, 100])\n",
      "linear1.weight \t torch.Size([100, 100])\n",
      "linear1.bias \t torch.Size([100])\n",
      "linear2.weight \t torch.Size([31, 100])\n",
      "linear2.bias \t torch.Size([31])\n",
      "Optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.001, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [2698026040488, 2698026040416, 2698026040560, 2698026040632, 2698026040704]}]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class CBOW(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, inp_size , vocab_size, embedding_dim=100):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, 100)\n",
    "        self.activation_function1 = nn.ReLU()        \n",
    "        self.linear2 = nn.Linear(100, vocab_size)\n",
    "        self.activation_function2 = nn.LogSoftmax(dim = -1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embeds = sum(self.embeddings(torch.from_numpy(inputs).long().cuda())).view(1,-1)\n",
    "        out = self.linear1(embeds)\n",
    "        out = self.activation_function1(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.activation_function2(out)\n",
    "        return out\n",
    "\n",
    "    \n",
    "model = CBOW(window_size*2,vocab_size).cuda()\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])\n",
    "\n",
    "torch.save(model.state_dict(), \"Cbow_Weights\")\n",
    "\n",
    "# model = TheModelClass(*args, **kwargs)\n",
    "# model.load_state_dict(torch.load(PATH))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vE_4ZisS_JZo",
    "outputId": "5c570f7b-e127-42b1-e9e5-34e2d276bcf4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tLoss: tensor(9.2254, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 2 \tLoss: tensor(8.5779, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 3 \tLoss: tensor(8.2109, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 4 \tLoss: tensor(7.9904, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 5 \tLoss: tensor(7.7918, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 6 \tLoss: tensor(7.5436, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 7 \tLoss: tensor(7.2815, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 8 \tLoss: tensor(7.0081, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 9 \tLoss: tensor(6.7421, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 10 \tLoss: tensor(6.4857, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 11 \tLoss: tensor(6.2545, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 12 \tLoss: tensor(6.0239, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 13 \tLoss: tensor(5.8107, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 14 \tLoss: tensor(5.6046, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 15 \tLoss: tensor(5.4042, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 16 \tLoss: tensor(5.1982, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 17 \tLoss: tensor(4.9976, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 18 \tLoss: tensor(4.8298, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 19 \tLoss: tensor(4.6601, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 20 \tLoss: tensor(4.4939, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 21 \tLoss: tensor(4.3244, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 22 \tLoss: tensor(4.1742, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 23 \tLoss: tensor(4.0358, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 24 \tLoss: tensor(3.8993, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 25 \tLoss: tensor(3.7774, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 26 \tLoss: tensor(3.6487, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 27 \tLoss: tensor(3.5332, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 28 \tLoss: tensor(3.4131, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 29 \tLoss: tensor(3.3018, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 30 \tLoss: tensor(3.1894, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 31 \tLoss: tensor(3.1003, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 32 \tLoss: tensor(2.9886, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 33 \tLoss: tensor(2.8810, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 34 \tLoss: tensor(2.7895, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 35 \tLoss: tensor(2.7036, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 36 \tLoss: tensor(2.6050, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 37 \tLoss: tensor(2.5222, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 38 \tLoss: tensor(2.4287, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 39 \tLoss: tensor(2.3682, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 40 \tLoss: tensor(2.2722, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 41 \tLoss: tensor(2.2021, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 42 \tLoss: tensor(2.1161, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 43 \tLoss: tensor(2.0561, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 44 \tLoss: tensor(1.9930, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 45 \tLoss: tensor(1.9222, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 46 \tLoss: tensor(1.8632, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 47 \tLoss: tensor(1.8039, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 48 \tLoss: tensor(1.7517, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 49 \tLoss: tensor(1.6934, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 50 \tLoss: tensor(1.6377, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 51 \tLoss: tensor(1.5981, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 52 \tLoss: tensor(1.5484, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 53 \tLoss: tensor(1.5058, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 54 \tLoss: tensor(1.4659, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 55 \tLoss: tensor(1.4334, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 56 \tLoss: tensor(1.3839, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 57 \tLoss: tensor(1.3567, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 58 \tLoss: tensor(1.3153, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 59 \tLoss: tensor(1.2859, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 60 \tLoss: tensor(1.2496, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 61 \tLoss: tensor(1.2234, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 62 \tLoss: tensor(1.1865, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 63 \tLoss: tensor(1.1618, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 64 \tLoss: tensor(1.1363, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 65 \tLoss: tensor(1.1131, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 66 \tLoss: tensor(1.0826, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 67 \tLoss: tensor(1.0598, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 68 \tLoss: tensor(1.0445, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 69 \tLoss: tensor(1.0177, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 70 \tLoss: tensor(0.9955, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 71 \tLoss: tensor(0.9746, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 72 \tLoss: tensor(0.9605, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 73 \tLoss: tensor(0.9352, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 74 \tLoss: tensor(0.9175, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 75 \tLoss: tensor(0.8981, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 76 \tLoss: tensor(0.8771, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 77 \tLoss: tensor(0.8657, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 78 \tLoss: tensor(0.8464, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 79 \tLoss: tensor(0.8320, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 80 \tLoss: tensor(0.8159, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 81 \tLoss: tensor(0.8013, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 82 \tLoss: tensor(0.7874, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 83 \tLoss: tensor(0.7712, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 84 \tLoss: tensor(0.7602, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 85 \tLoss: tensor(0.7422, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 86 \tLoss: tensor(0.7300, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 87 \tLoss: tensor(0.7193, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 88 \tLoss: tensor(0.7039, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 89 \tLoss: tensor(0.6964, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 90 \tLoss: tensor(0.6841, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 91 \tLoss: tensor(0.6689, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 92 \tLoss: tensor(0.6588, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 93 \tLoss: tensor(0.6504, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 94 \tLoss: tensor(0.6386, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 95 \tLoss: tensor(0.6283, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 96 \tLoss: tensor(0.6193, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 97 \tLoss: tensor(0.6101, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 98 \tLoss: tensor(0.5996, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 99 \tLoss: tensor(0.5918, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 100):\n",
    "    loss = 0\n",
    "    i = 0\n",
    "    X,Y = generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size)\n",
    "    for x, y in zip(X,Y):\n",
    "        i += 1\n",
    "        optimizer.zero_grad()\n",
    "        log_probs = model(x[0])\n",
    "        loss = loss_function(log_probs,torch.Tensor([y]).long().cuda())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss += loss.data\n",
    "    print('Epoch:', epoch, '\\tLoss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "KDeourI1pfuw"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.246579</td>\n",
       "      <td>-0.562335</td>\n",
       "      <td>0.043182</td>\n",
       "      <td>1.905390</td>\n",
       "      <td>2.897158</td>\n",
       "      <td>-0.945870</td>\n",
       "      <td>-1.171224</td>\n",
       "      <td>-0.234848</td>\n",
       "      <td>-0.303074</td>\n",
       "      <td>-1.106695</td>\n",
       "      <td>...</td>\n",
       "      <td>0.398234</td>\n",
       "      <td>2.262729</td>\n",
       "      <td>-1.240966</td>\n",
       "      <td>-1.615071</td>\n",
       "      <td>0.430559</td>\n",
       "      <td>1.204429</td>\n",
       "      <td>1.962709</td>\n",
       "      <td>-0.705347</td>\n",
       "      <td>2.702054</td>\n",
       "      <td>0.223823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>-0.128564</td>\n",
       "      <td>-0.437242</td>\n",
       "      <td>-0.335543</td>\n",
       "      <td>-0.279052</td>\n",
       "      <td>-1.862769</td>\n",
       "      <td>-1.401087</td>\n",
       "      <td>2.301530</td>\n",
       "      <td>0.005477</td>\n",
       "      <td>0.660442</td>\n",
       "      <td>0.530043</td>\n",
       "      <td>...</td>\n",
       "      <td>0.830228</td>\n",
       "      <td>1.139872</td>\n",
       "      <td>1.804603</td>\n",
       "      <td>1.048361</td>\n",
       "      <td>0.612678</td>\n",
       "      <td>0.142685</td>\n",
       "      <td>-0.640898</td>\n",
       "      <td>0.269062</td>\n",
       "      <td>-1.383707</td>\n",
       "      <td>0.493586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.147716</td>\n",
       "      <td>1.590648</td>\n",
       "      <td>-0.531452</td>\n",
       "      <td>0.364238</td>\n",
       "      <td>-2.081264</td>\n",
       "      <td>1.953997</td>\n",
       "      <td>0.312316</td>\n",
       "      <td>0.132126</td>\n",
       "      <td>0.342545</td>\n",
       "      <td>1.614952</td>\n",
       "      <td>...</td>\n",
       "      <td>0.516661</td>\n",
       "      <td>1.344398</td>\n",
       "      <td>0.010004</td>\n",
       "      <td>-0.000935</td>\n",
       "      <td>-0.911071</td>\n",
       "      <td>-1.500323</td>\n",
       "      <td>0.006096</td>\n",
       "      <td>-0.908330</td>\n",
       "      <td>0.094563</td>\n",
       "      <td>0.560542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sky</th>\n",
       "      <td>0.798209</td>\n",
       "      <td>0.580648</td>\n",
       "      <td>-0.940805</td>\n",
       "      <td>1.174904</td>\n",
       "      <td>0.018244</td>\n",
       "      <td>0.258995</td>\n",
       "      <td>0.891437</td>\n",
       "      <td>-0.153361</td>\n",
       "      <td>-0.161141</td>\n",
       "      <td>1.603590</td>\n",
       "      <td>...</td>\n",
       "      <td>1.090361</td>\n",
       "      <td>0.094175</td>\n",
       "      <td>-1.023437</td>\n",
       "      <td>-0.071496</td>\n",
       "      <td>0.397970</td>\n",
       "      <td>-0.808378</td>\n",
       "      <td>-0.582422</td>\n",
       "      <td>0.041686</td>\n",
       "      <td>0.695446</td>\n",
       "      <td>-0.152871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blue</th>\n",
       "      <td>0.167987</td>\n",
       "      <td>1.054565</td>\n",
       "      <td>0.945732</td>\n",
       "      <td>1.070180</td>\n",
       "      <td>-0.046181</td>\n",
       "      <td>0.503633</td>\n",
       "      <td>0.014728</td>\n",
       "      <td>-1.516200</td>\n",
       "      <td>0.631779</td>\n",
       "      <td>0.073180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178417</td>\n",
       "      <td>1.100883</td>\n",
       "      <td>-0.255432</td>\n",
       "      <td>0.796993</td>\n",
       "      <td>0.631346</td>\n",
       "      <td>-1.623453</td>\n",
       "      <td>0.385617</td>\n",
       "      <td>-0.030791</td>\n",
       "      <td>1.651315</td>\n",
       "      <td>-1.189646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beautiful</th>\n",
       "      <td>-0.910202</td>\n",
       "      <td>-0.996826</td>\n",
       "      <td>0.262043</td>\n",
       "      <td>-0.553207</td>\n",
       "      <td>1.099605</td>\n",
       "      <td>0.841212</td>\n",
       "      <td>-0.795247</td>\n",
       "      <td>-0.425355</td>\n",
       "      <td>-0.631225</td>\n",
       "      <td>-1.262379</td>\n",
       "      <td>...</td>\n",
       "      <td>0.956158</td>\n",
       "      <td>-1.592699</td>\n",
       "      <td>0.074860</td>\n",
       "      <td>-0.156407</td>\n",
       "      <td>-0.830934</td>\n",
       "      <td>0.879292</td>\n",
       "      <td>1.349590</td>\n",
       "      <td>0.840808</td>\n",
       "      <td>-0.751444</td>\n",
       "      <td>1.213342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quick</th>\n",
       "      <td>1.481157</td>\n",
       "      <td>0.320704</td>\n",
       "      <td>-0.216832</td>\n",
       "      <td>-0.109891</td>\n",
       "      <td>-0.586104</td>\n",
       "      <td>0.447652</td>\n",
       "      <td>-0.056320</td>\n",
       "      <td>-1.332508</td>\n",
       "      <td>1.544559</td>\n",
       "      <td>-0.488736</td>\n",
       "      <td>...</td>\n",
       "      <td>0.380646</td>\n",
       "      <td>0.985778</td>\n",
       "      <td>-0.064054</td>\n",
       "      <td>-1.262825</td>\n",
       "      <td>-1.319752</td>\n",
       "      <td>-0.027141</td>\n",
       "      <td>1.897707</td>\n",
       "      <td>-0.377066</td>\n",
       "      <td>-0.986022</td>\n",
       "      <td>-0.377845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brown</th>\n",
       "      <td>0.343019</td>\n",
       "      <td>0.482020</td>\n",
       "      <td>1.034114</td>\n",
       "      <td>-0.191746</td>\n",
       "      <td>1.860478</td>\n",
       "      <td>0.331806</td>\n",
       "      <td>-1.634989</td>\n",
       "      <td>-0.210357</td>\n",
       "      <td>-0.323028</td>\n",
       "      <td>0.536377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150010</td>\n",
       "      <td>0.171598</td>\n",
       "      <td>0.327116</td>\n",
       "      <td>0.859752</td>\n",
       "      <td>-0.426213</td>\n",
       "      <td>0.081196</td>\n",
       "      <td>0.082185</td>\n",
       "      <td>0.586825</td>\n",
       "      <td>0.126417</td>\n",
       "      <td>1.557284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fox</th>\n",
       "      <td>1.301593</td>\n",
       "      <td>-0.507659</td>\n",
       "      <td>-1.305631</td>\n",
       "      <td>-0.958802</td>\n",
       "      <td>0.064477</td>\n",
       "      <td>-0.490074</td>\n",
       "      <td>-0.459826</td>\n",
       "      <td>-0.253821</td>\n",
       "      <td>-1.890259</td>\n",
       "      <td>0.171855</td>\n",
       "      <td>...</td>\n",
       "      <td>0.673046</td>\n",
       "      <td>1.608326</td>\n",
       "      <td>0.475421</td>\n",
       "      <td>-0.593568</td>\n",
       "      <td>0.390986</td>\n",
       "      <td>1.883014</td>\n",
       "      <td>-1.988344</td>\n",
       "      <td>-1.608087</td>\n",
       "      <td>0.960230</td>\n",
       "      <td>0.491502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lazy</th>\n",
       "      <td>-0.576814</td>\n",
       "      <td>0.395293</td>\n",
       "      <td>-1.719962</td>\n",
       "      <td>-0.775848</td>\n",
       "      <td>1.042978</td>\n",
       "      <td>-0.482781</td>\n",
       "      <td>1.346323</td>\n",
       "      <td>1.020422</td>\n",
       "      <td>0.363774</td>\n",
       "      <td>-1.230762</td>\n",
       "      <td>...</td>\n",
       "      <td>1.426381</td>\n",
       "      <td>-1.102519</td>\n",
       "      <td>0.683267</td>\n",
       "      <td>-0.122452</td>\n",
       "      <td>0.891150</td>\n",
       "      <td>-0.426539</td>\n",
       "      <td>-0.195252</td>\n",
       "      <td>0.478174</td>\n",
       "      <td>-0.834393</td>\n",
       "      <td>0.212006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0         1         2         3         4         5   \\\n",
       "the        0.246579 -0.562335  0.043182  1.905390  2.897158 -0.945870   \n",
       "is        -0.128564 -0.437242 -0.335543 -0.279052 -1.862769 -1.401087   \n",
       "and        0.147716  1.590648 -0.531452  0.364238 -2.081264  1.953997   \n",
       "sky        0.798209  0.580648 -0.940805  1.174904  0.018244  0.258995   \n",
       "blue       0.167987  1.054565  0.945732  1.070180 -0.046181  0.503633   \n",
       "beautiful -0.910202 -0.996826  0.262043 -0.553207  1.099605  0.841212   \n",
       "quick      1.481157  0.320704 -0.216832 -0.109891 -0.586104  0.447652   \n",
       "brown      0.343019  0.482020  1.034114 -0.191746  1.860478  0.331806   \n",
       "fox        1.301593 -0.507659 -1.305631 -0.958802  0.064477 -0.490074   \n",
       "lazy      -0.576814  0.395293 -1.719962 -0.775848  1.042978 -0.482781   \n",
       "\n",
       "                 6         7         8         9     ...           90  \\\n",
       "the       -1.171224 -0.234848 -0.303074 -1.106695    ...     0.398234   \n",
       "is         2.301530  0.005477  0.660442  0.530043    ...     0.830228   \n",
       "and        0.312316  0.132126  0.342545  1.614952    ...     0.516661   \n",
       "sky        0.891437 -0.153361 -0.161141  1.603590    ...     1.090361   \n",
       "blue       0.014728 -1.516200  0.631779  0.073180    ...     0.178417   \n",
       "beautiful -0.795247 -0.425355 -0.631225 -1.262379    ...     0.956158   \n",
       "quick     -0.056320 -1.332508  1.544559 -0.488736    ...     0.380646   \n",
       "brown     -1.634989 -0.210357 -0.323028  0.536377    ...     0.150010   \n",
       "fox       -0.459826 -0.253821 -1.890259  0.171855    ...     0.673046   \n",
       "lazy       1.346323  1.020422  0.363774 -1.230762    ...     1.426381   \n",
       "\n",
       "                 91        92        93        94        95        96  \\\n",
       "the        2.262729 -1.240966 -1.615071  0.430559  1.204429  1.962709   \n",
       "is         1.139872  1.804603  1.048361  0.612678  0.142685 -0.640898   \n",
       "and        1.344398  0.010004 -0.000935 -0.911071 -1.500323  0.006096   \n",
       "sky        0.094175 -1.023437 -0.071496  0.397970 -0.808378 -0.582422   \n",
       "blue       1.100883 -0.255432  0.796993  0.631346 -1.623453  0.385617   \n",
       "beautiful -1.592699  0.074860 -0.156407 -0.830934  0.879292  1.349590   \n",
       "quick      0.985778 -0.064054 -1.262825 -1.319752 -0.027141  1.897707   \n",
       "brown      0.171598  0.327116  0.859752 -0.426213  0.081196  0.082185   \n",
       "fox        1.608326  0.475421 -0.593568  0.390986  1.883014 -1.988344   \n",
       "lazy      -1.102519  0.683267 -0.122452  0.891150 -0.426539 -0.195252   \n",
       "\n",
       "                 97        98        99  \n",
       "the       -0.705347  2.702054  0.223823  \n",
       "is         0.269062 -1.383707  0.493586  \n",
       "and       -0.908330  0.094563  0.560542  \n",
       "sky        0.041686  0.695446 -0.152871  \n",
       "blue      -0.030791  1.651315 -1.189646  \n",
       "beautiful  0.840808 -0.751444  1.213342  \n",
       "quick     -0.377066 -0.986022 -0.377845  \n",
       "brown      0.586825  0.126417  1.557284  \n",
       "fox       -1.608087  0.960230  0.491502  \n",
       "lazy       0.478174 -0.834393  0.212006  \n",
       "\n",
       "[10 rows x 100 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.embeddings(torch.Tensor([list(range(0,vocab_size))]).long().cuda())\n",
    "\n",
    "pd.DataFrame(weights.view(-1,100).tolist(), index=list(id2word.values())[0:]).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9mYcRmygvEO"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CVM5Rq_9pfux",
    "outputId": "11b33fc5-4ae7-4e5c-d349-955a3c2ee987"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': ['quick', 'blue', 'bacon'],\n",
       " 'fox': ['eggs', 'i', 'this'],\n",
       " 'beautiful': ['lazy', 'this', 'a'],\n",
       " 'brown': ['green', 'very', 'ham'],\n",
       " 'lazy': ['beautiful', 'i', 'fox']}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "weights = weights.view(-1,100)\n",
    "distance_matrix = euclidean_distances(weights.cpu().detach().numpy())\n",
    "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:4]+1] \n",
    "                 for search_term in ['the', 'fox', 'beautiful','brown','lazy']}\n",
    "\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OT-1TyE3pfux"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CBOW - Notebook (Word2Vec).ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "e9acf0a5f7b368749e03ba162bc6f8dae21ce9d9285a8a40c5b041a2ff3bab4f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
