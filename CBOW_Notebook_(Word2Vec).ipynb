{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "interpreter": {
      "hash": "e9acf0a5f7b368749e03ba162bc6f8dae21ce9d9285a8a40c5b041a2ff3bab4f"
    },
    "kernelspec": {
      "display_name": "Python 3.6.7 64-bit ('tensorflow': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "CBOW - Notebook (Word2Vec).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UypYA6kX7rrH"
      },
      "source": [
        "## CBOW - Word2Vec Implementation Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nhCCVg-pfuh"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.corpus import webtext\n",
        "from keras.preprocessing import sequence\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing import text\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing import sequence\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pd.options.display.max_colwidth = 200\n",
        "%matplotlib inline"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtEi1_Ewp38e",
        "outputId": "e35ef7d5-b7b3-4917-a77b-bdcd8caa2dd6"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('webtext')\n",
        "from nltk.corpus import brown"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]   Package webtext is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iN6sWbdpfum"
      },
      "source": [
        "## Pre-Processing text Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cB1A2KCQpfuo"
      },
      "source": [
        "wordpt = nltk.WordPunctTokenizer()\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "def normalize_document(doc):\n",
        "    # lower case and remove special characters\\whitespaces\n",
        "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
        "    doc = doc.lower()\n",
        "    doc = doc.strip()\n",
        "    # tokenize document\n",
        "    tokens = wordpt.tokenize(doc)\n",
        "    # filter stopwords out of document\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "    # re-create document from filtered tokens\n",
        "    doc = ' '.join(filtered_tokens)\n",
        "    return doc\n",
        "\n",
        "normalize_corpus = np.vectorize(normalize_document)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "rlEIdkRYpfup",
        "outputId": "a4227224-17ad-4c41-929b-105cfa7ba89d"
      },
      "source": [
        "corpus = ['The sky is blue and beautiful.',\n",
        "          'Love this blue and beautiful sky!',\n",
        "          'The quick brown fox jumps over the lazy dog.',\n",
        "          \"A king's breakfast has sausages, ham, bacon, eggs, toast and beans\",\n",
        "          'I love green eggs, ham, sausages and bacon!',\n",
        "          'The brown fox is quick and the blue dog is lazy!',\n",
        "          'The sky is very blue and the sky is very beautiful today',\n",
        "          'The dog is lazy but the brown fox is quick!'    \n",
        "]\n",
        "labels = ['weather', 'weather', 'animals', 'food', 'food', 'animals', 'weather', 'animals']\n",
        "\n",
        "corpus = np.array(corpus)\n",
        "corpus_df = pd.DataFrame({'Document': corpus, \n",
        "                          'Category': labels})\n",
        "corpus_df = corpus_df[['Document', 'Category']]\n",
        "corpus_df"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Document</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The sky is blue and beautiful.</td>\n",
              "      <td>weather</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Love this blue and beautiful sky!</td>\n",
              "      <td>weather</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The quick brown fox jumps over the lazy dog.</td>\n",
              "      <td>animals</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A king's breakfast has sausages, ham, bacon, eggs, toast and beans</td>\n",
              "      <td>food</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I love green eggs, ham, sausages and bacon!</td>\n",
              "      <td>food</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>The brown fox is quick and the blue dog is lazy!</td>\n",
              "      <td>animals</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>The sky is very blue and the sky is very beautiful today</td>\n",
              "      <td>weather</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>The dog is lazy but the brown fox is quick!</td>\n",
              "      <td>animals</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                             Document Category\n",
              "0                                      The sky is blue and beautiful.  weather\n",
              "1                                   Love this blue and beautiful sky!  weather\n",
              "2                        The quick brown fox jumps over the lazy dog.  animals\n",
              "3  A king's breakfast has sausages, ham, bacon, eggs, toast and beans     food\n",
              "4                         I love green eggs, ham, sausages and bacon!     food\n",
              "5                    The brown fox is quick and the blue dog is lazy!  animals\n",
              "6            The sky is very blue and the sky is very beautiful today  weather\n",
              "7                         The dog is lazy but the brown fox is quick!  animals"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FghWaqo1pfuq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a9fc7ac-c911-482d-b6c7-b44c83f32d73"
      },
      "source": [
        "# build a sample vocab\n",
        "vocab = []\n",
        "print(webtext.fileids())\n",
        "print(len(webtext.raw('firefox.txt'))) \n",
        "for fileid in webtext.fileids():\n",
        "    vocab.append(webtext.raw('firefox.txt'))\n",
        "\n",
        "    #print(brown.raw('cb01').strip()[:1000])  "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['firefox.txt', 'grail.txt', 'overheard.txt', 'pirates.txt', 'singles.txt', 'wine.txt']\n",
            "564601\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgL7JfiPpfuq"
      },
      "source": [
        "### text preprocessing (Remove tags e.g HTML,Remove special characters, Remove stopwords) === Clean data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCm0sMaspfur",
        "outputId": "b14788d5-98c6-4d1d-aa83-e5cfd1edf8a2"
      },
      "source": [
        "tokenizer = text.Tokenizer()\n",
        "tokenizer.fit_on_texts(vocab)\n",
        "word2id = tokenizer.word_index\n",
        "\n",
        "word2id['PAD'] = 0\n",
        "id2word = {v:k for k, v in word2id.items()}\n",
        "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in vocab]\n",
        "\n",
        "vocab_size = len(word2id)\n",
        "embed_size = 100\n",
        "window_size = 2\n",
        "\n",
        "print('Vocabulary Size:', vocab_size)\n",
        "print('Vocabulary Sample:', list(word2id.items())[:10])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 8021\n",
            "Vocabulary Sample: [('in', 1), ('to', 2), ('the', 3), ('\\r', 4), ('not', 5), ('when', 6), ('on', 7), ('a', 8), ('is', 9), ('and', 10)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HT4Hrfo_pfus"
      },
      "source": [
        "### [context_words, target_word] pairs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdWqc3v4pfut"
      },
      "source": [
        "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
        "    X = []\n",
        "    Y = []\n",
        "    context_length = window_size*2\n",
        "    for words in wids:\n",
        "        sentence_length = len(words)\n",
        "        for index, word in enumerate(words):           \n",
        "            start = index - window_size\n",
        "            end = index + window_size + 1\n",
        "            context = [words[i] for i in range(start, end)if 0 <= i < sentence_length and i != index]\n",
        "            x = sequence.pad_sequences([context], maxlen=context_length)\n",
        "            X.append(x)\n",
        "            Y.append(word)\n",
        "    return X,Y"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAgVUa1Qpfuu"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsdpsBjQpfuv"
      },
      "source": [
        "## CBOW (Contineous bag of Words Model architecture)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqCLwsVGpfuv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "449b5450-8632-45f3-c1b5-b3b371446c90"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "class CBOW(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, inp_size , vocab_size, embedding_dim=100):\n",
        "        super(CBOW, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(embedding_dim, 100)\n",
        "        self.activation_function1 = nn.ReLU()        \n",
        "        self.linear2 = nn.Linear(100, vocab_size)\n",
        "        self.activation_function2 = nn.LogSoftmax(dim = -1)\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        embeds = sum(self.embeddings(torch.from_numpy(inputs).long().cuda())).view(1,-1)\n",
        "        out = self.linear1(embeds)\n",
        "        out = self.activation_function1(out)\n",
        "        out = self.linear2(out)\n",
        "        out = self.activation_function2(out)\n",
        "        return out\n",
        "\n",
        "    \n",
        "model = CBOW(window_size*2,vocab_size).cuda()\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "# Print model's state_dict\n",
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
        "\n",
        "# Print optimizer's state_dict\n",
        "print(\"Optimizer's state_dict:\")\n",
        "for var_name in optimizer.state_dict():\n",
        "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])\n",
        "\n",
        "torch.save(model.state_dict(), \"content\")\n",
        "\n",
        "# model = TheModelClass(*args, **kwargs)\n",
        "# model.load_state_dict(torch.load(PATH))\n",
        "# model.eval()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model's state_dict:\n",
            "embeddings.weight \t torch.Size([8021, 100])\n",
            "linear1.weight \t torch.Size([100, 100])\n",
            "linear1.bias \t torch.Size([100])\n",
            "linear2.weight \t torch.Size([8021, 100])\n",
            "linear2.bias \t torch.Size([8021])\n",
            "Optimizer's state_dict:\n",
            "state \t {}\n",
            "param_groups \t [{'lr': 0.001, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [0, 1, 2, 3, 4]}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vE_4ZisS_JZo",
        "outputId": "5c570f7b-e127-42b1-e9e5-34e2d276bcf4"
      },
      "source": [
        "for epoch in range(1, 10):\n",
        "    loss = 0\n",
        "    i = 0\n",
        "    X,Y = generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size)\n",
        "    for x, y in zip(X,Y):\n",
        "        i += 1\n",
        "        optimizer.zero_grad()\n",
        "        log_probs = model(x[0])\n",
        "        loss = loss_function(log_probs,torch.Tensor([y]).long().cuda())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss += loss.data\n",
        "    print('Epoch:', epoch, '\\tLoss:', loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 \tLoss: tensor(13.1420, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 2 \tLoss: tensor(7.6153, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 3 \tLoss: tensor(4.8815, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDeourI1pfuw"
      },
      "source": [
        "weights = model.embeddings(torch.Tensor([list(range(0,vocab_size))]).long())\n",
        "\n",
        "pd.DataFrame(weights.view(-1,100).tolist(), index=list(id2word.values())[0:]).head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9mYcRmygvEO"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVM5Rq_9pfux",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11b33fc5-4ae7-4e5c-d349-955a3c2ee987"
      },
      "source": [
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "weights = weights.view(-1,100)\n",
        "distance_matrix = euclidean_distances(weights.detach().numpy())\n",
        "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:4]+1] \n",
        "                 for search_term in ['the', 'fox', 'beautiful','brown','lazy']}\n",
        "\n",
        "similar_words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'beautiful': ['over', 'but', 'brown'],\n",
              " 'brown': ['has', 'jumps', \"king's\"],\n",
              " 'fox': ['a', 'toast', 'brown'],\n",
              " 'lazy': ['quick', 'a', 'this'],\n",
              " 'the': ['is', 'over', 'quick']}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT-1TyE3pfux"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}