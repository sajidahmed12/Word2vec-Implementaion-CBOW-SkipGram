{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "interpreter": {
      "hash": "e9acf0a5f7b368749e03ba162bc6f8dae21ce9d9285a8a40c5b041a2ff3bab4f"
    },
    "kernelspec": {
      "display_name": "Python 3.6.7 64-bit ('tensorflow': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "CBOW - Notebook (Word2Vec).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UypYA6kX7rrH"
      },
      "source": [
        "## CBOW - Word2Vec Implementation Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nhCCVg-pfuh"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.corpus import webtext\n",
        "from keras.preprocessing import sequence\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing import text\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing import sequence\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pd.options.display.max_colwidth = 200\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtEi1_Ewp38e",
        "outputId": "f0311cb0-c354-4c80-bc17-19780bfd0853"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('webtext')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/webtext.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iN6sWbdpfum"
      },
      "source": [
        "## Pre-Processing text Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cB1A2KCQpfuo"
      },
      "source": [
        "wordpt = nltk.WordPunctTokenizer()\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "def normalize_document(doc):\n",
        "    # lower case and remove special characters\\whitespaces\n",
        "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
        "    doc = doc.lower()\n",
        "    doc = doc.strip()\n",
        "    # tokenize document\n",
        "    tokens = wordpt.tokenize(doc)\n",
        "    # filter stopwords out of document\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "    # re-create document from filtered tokens\n",
        "    doc = ' '.join(filtered_tokens)\n",
        "    return doc\n",
        "\n",
        "normalize_corpus = np.vectorize(normalize_document)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "rlEIdkRYpfup",
        "outputId": "b1af4d3d-b181-4242-baa0-495bdc358bd0"
      },
      "source": [
        "corpus = ['The sky is blue and beautiful.',\n",
        "          'Love this blue and beautiful sky!',\n",
        "          'The quick brown fox jumps over the lazy dog.',\n",
        "          \"A king's breakfast has sausages, ham, bacon, eggs, toast and beans\",\n",
        "          'I love green eggs, ham, sausages and bacon!',\n",
        "          'The brown fox is quick and the blue dog is lazy!',\n",
        "          'The sky is very blue and the sky is very beautiful today',\n",
        "          'The dog is lazy but the brown fox is quick!'    \n",
        "]\n",
        "labels = ['weather', 'weather', 'animals', 'food', 'food', 'animals', 'weather', 'animals']\n",
        "\n",
        "corpus = np.array(corpus)\n",
        "corpus_df = pd.DataFrame({'Document': corpus, \n",
        "                          'Category': labels})\n",
        "corpus_df = corpus_df[['Document', 'Category']]\n",
        "corpus_df"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Document</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The sky is blue and beautiful.</td>\n",
              "      <td>weather</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Love this blue and beautiful sky!</td>\n",
              "      <td>weather</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The quick brown fox jumps over the lazy dog.</td>\n",
              "      <td>animals</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A king's breakfast has sausages, ham, bacon, eggs, toast and beans</td>\n",
              "      <td>food</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I love green eggs, ham, sausages and bacon!</td>\n",
              "      <td>food</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>The brown fox is quick and the blue dog is lazy!</td>\n",
              "      <td>animals</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>The sky is very blue and the sky is very beautiful today</td>\n",
              "      <td>weather</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>The dog is lazy but the brown fox is quick!</td>\n",
              "      <td>animals</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                             Document Category\n",
              "0                                      The sky is blue and beautiful.  weather\n",
              "1                                   Love this blue and beautiful sky!  weather\n",
              "2                        The quick brown fox jumps over the lazy dog.  animals\n",
              "3  A king's breakfast has sausages, ham, bacon, eggs, toast and beans     food\n",
              "4                         I love green eggs, ham, sausages and bacon!     food\n",
              "5                    The brown fox is quick and the blue dog is lazy!  animals\n",
              "6            The sky is very blue and the sky is very beautiful today  weather\n",
              "7                         The dog is lazy but the brown fox is quick!  animals"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FghWaqo1pfuq"
      },
      "source": [
        "# build a sample vocab\n",
        "vocab = []\n",
        "\n",
        "for fileid in webtext.fileids():\n",
        "    vocab.append(webtext.raw(fileid))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgL7JfiPpfuq"
      },
      "source": [
        "### text preprocessing (Remove tags e.g HTML,Remove special characters, Remove stopwords) === Clean data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCm0sMaspfur",
        "outputId": "cbaae817-77c1-4244-8a7f-7c9fd333f36c"
      },
      "source": [
        "tokenizer = text.Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "word2id = tokenizer.word_index\n",
        "\n",
        "word2id['PAD'] = 0\n",
        "id2word = {v:k for k, v in word2id.items()}\n",
        "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in corpus]\n",
        "\n",
        "vocab_size = len(word2id)\n",
        "embed_size = 100\n",
        "window_size = 2\n",
        "\n",
        "print('Vocabulary Size:', vocab_size)\n",
        "print('Vocabulary Sample:', list(word2id.items())[:10])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 31\n",
            "Vocabulary Sample: [('the', 1), ('is', 2), ('and', 3), ('sky', 4), ('blue', 5), ('beautiful', 6), ('quick', 7), ('brown', 8), ('fox', 9), ('lazy', 10)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HT4Hrfo_pfus"
      },
      "source": [
        "### [context_words, target_word] pairs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdWqc3v4pfut"
      },
      "source": [
        "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
        "    X = []\n",
        "    Y = []\n",
        "    context_length = window_size*2\n",
        "    for words in wids:\n",
        "        sentence_length = len(words)\n",
        "        for index, word in enumerate(words):           \n",
        "            start = index - window_size\n",
        "            end = index + window_size + 1\n",
        "            context = [words[i] for i in range(start, end)if 0 <= i < sentence_length and i != index]\n",
        "            x = sequence.pad_sequences([context], maxlen=context_length)\n",
        "            X.append(x)\n",
        "            Y.append(word)\n",
        "    return X,Y"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAgVUa1Qpfuu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsdpsBjQpfuv"
      },
      "source": [
        "## CBOW (Contineous bag of Words Model architecture)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqCLwsVGpfuv"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "class CBOW(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, inp_size , vocab_size, embedding_dim=100):\n",
        "        super(CBOW, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(embedding_dim, 100)\n",
        "        self.activation_function1 = nn.ReLU()        \n",
        "        self.linear2 = nn.Linear(100, vocab_size)\n",
        "        self.activation_function2 = nn.LogSoftmax(dim = -1)\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        embeds = sum(self.embeddings(torch.from_numpy(inputs).long())).view(1,-1)\n",
        "        out = self.linear1(embeds)\n",
        "        out = self.activation_function1(out)\n",
        "        out = self.linear2(out)\n",
        "        out = self.activation_function2(out)\n",
        "        return out\n",
        "    \n",
        "model = CBOW(window_size*2,vocab_size)\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vE_4ZisS_JZo",
        "outputId": "e9c49cf1-28b8-4ed4-9d13-13e03aeb977e"
      },
      "source": [
        "for epoch in range(1, 1000):\n",
        "    loss = 0\n",
        "    i = 0\n",
        "    X,Y = generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=10000)\n",
        "    for x, y in zip(X,Y):\n",
        "        i += 1\n",
        "        optimizer.zero_grad()\n",
        "        log_probs = model(x[0])\n",
        "        loss = loss_function(log_probs,torch.Tensor([y]).long())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss += loss.data\n",
        "    print('Epoch:', epoch, '\\tLoss:', loss)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 \tLoss: tensor(5.1038, grad_fn=<AddBackward0>)\n",
            "Epoch: 2 \tLoss: tensor(4.9339, grad_fn=<AddBackward0>)\n",
            "Epoch: 3 \tLoss: tensor(4.7704, grad_fn=<AddBackward0>)\n",
            "Epoch: 4 \tLoss: tensor(4.6138, grad_fn=<AddBackward0>)\n",
            "Epoch: 5 \tLoss: tensor(4.4486, grad_fn=<AddBackward0>)\n",
            "Epoch: 6 \tLoss: tensor(4.3001, grad_fn=<AddBackward0>)\n",
            "Epoch: 7 \tLoss: tensor(4.1443, grad_fn=<AddBackward0>)\n",
            "Epoch: 8 \tLoss: tensor(3.9982, grad_fn=<AddBackward0>)\n",
            "Epoch: 9 \tLoss: tensor(3.8371, grad_fn=<AddBackward0>)\n",
            "Epoch: 10 \tLoss: tensor(3.6970, grad_fn=<AddBackward0>)\n",
            "Epoch: 11 \tLoss: tensor(3.5633, grad_fn=<AddBackward0>)\n",
            "Epoch: 12 \tLoss: tensor(3.4263, grad_fn=<AddBackward0>)\n",
            "Epoch: 13 \tLoss: tensor(3.2965, grad_fn=<AddBackward0>)\n",
            "Epoch: 14 \tLoss: tensor(3.1613, grad_fn=<AddBackward0>)\n",
            "Epoch: 15 \tLoss: tensor(3.0402, grad_fn=<AddBackward0>)\n",
            "Epoch: 16 \tLoss: tensor(2.9310, grad_fn=<AddBackward0>)\n",
            "Epoch: 17 \tLoss: tensor(2.8105, grad_fn=<AddBackward0>)\n",
            "Epoch: 18 \tLoss: tensor(2.7157, grad_fn=<AddBackward0>)\n",
            "Epoch: 19 \tLoss: tensor(2.6166, grad_fn=<AddBackward0>)\n",
            "Epoch: 20 \tLoss: tensor(2.5191, grad_fn=<AddBackward0>)\n",
            "Epoch: 21 \tLoss: tensor(2.4299, grad_fn=<AddBackward0>)\n",
            "Epoch: 22 \tLoss: tensor(2.3375, grad_fn=<AddBackward0>)\n",
            "Epoch: 23 \tLoss: tensor(2.2559, grad_fn=<AddBackward0>)\n",
            "Epoch: 24 \tLoss: tensor(2.1827, grad_fn=<AddBackward0>)\n",
            "Epoch: 25 \tLoss: tensor(2.1093, grad_fn=<AddBackward0>)\n",
            "Epoch: 26 \tLoss: tensor(2.0334, grad_fn=<AddBackward0>)\n",
            "Epoch: 27 \tLoss: tensor(1.9674, grad_fn=<AddBackward0>)\n",
            "Epoch: 28 \tLoss: tensor(1.8962, grad_fn=<AddBackward0>)\n",
            "Epoch: 29 \tLoss: tensor(1.8367, grad_fn=<AddBackward0>)\n",
            "Epoch: 30 \tLoss: tensor(1.7718, grad_fn=<AddBackward0>)\n",
            "Epoch: 31 \tLoss: tensor(1.7266, grad_fn=<AddBackward0>)\n",
            "Epoch: 32 \tLoss: tensor(1.6691, grad_fn=<AddBackward0>)\n",
            "Epoch: 33 \tLoss: tensor(1.6173, grad_fn=<AddBackward0>)\n",
            "Epoch: 34 \tLoss: tensor(1.5663, grad_fn=<AddBackward0>)\n",
            "Epoch: 35 \tLoss: tensor(1.5243, grad_fn=<AddBackward0>)\n",
            "Epoch: 36 \tLoss: tensor(1.4827, grad_fn=<AddBackward0>)\n",
            "Epoch: 37 \tLoss: tensor(1.4521, grad_fn=<AddBackward0>)\n",
            "Epoch: 38 \tLoss: tensor(1.4030, grad_fn=<AddBackward0>)\n",
            "Epoch: 39 \tLoss: tensor(1.3615, grad_fn=<AddBackward0>)\n",
            "Epoch: 40 \tLoss: tensor(1.3320, grad_fn=<AddBackward0>)\n",
            "Epoch: 41 \tLoss: tensor(1.2949, grad_fn=<AddBackward0>)\n",
            "Epoch: 42 \tLoss: tensor(1.2635, grad_fn=<AddBackward0>)\n",
            "Epoch: 43 \tLoss: tensor(1.2339, grad_fn=<AddBackward0>)\n",
            "Epoch: 44 \tLoss: tensor(1.2040, grad_fn=<AddBackward0>)\n",
            "Epoch: 45 \tLoss: tensor(1.1719, grad_fn=<AddBackward0>)\n",
            "Epoch: 46 \tLoss: tensor(1.1496, grad_fn=<AddBackward0>)\n",
            "Epoch: 47 \tLoss: tensor(1.1193, grad_fn=<AddBackward0>)\n",
            "Epoch: 48 \tLoss: tensor(1.0949, grad_fn=<AddBackward0>)\n",
            "Epoch: 49 \tLoss: tensor(1.0740, grad_fn=<AddBackward0>)\n",
            "Epoch: 50 \tLoss: tensor(1.0461, grad_fn=<AddBackward0>)\n",
            "Epoch: 51 \tLoss: tensor(1.0259, grad_fn=<AddBackward0>)\n",
            "Epoch: 52 \tLoss: tensor(1.0078, grad_fn=<AddBackward0>)\n",
            "Epoch: 53 \tLoss: tensor(0.9880, grad_fn=<AddBackward0>)\n",
            "Epoch: 54 \tLoss: tensor(0.9681, grad_fn=<AddBackward0>)\n",
            "Epoch: 55 \tLoss: tensor(0.9467, grad_fn=<AddBackward0>)\n",
            "Epoch: 56 \tLoss: tensor(0.9238, grad_fn=<AddBackward0>)\n",
            "Epoch: 57 \tLoss: tensor(0.9119, grad_fn=<AddBackward0>)\n",
            "Epoch: 58 \tLoss: tensor(0.8934, grad_fn=<AddBackward0>)\n",
            "Epoch: 59 \tLoss: tensor(0.8747, grad_fn=<AddBackward0>)\n",
            "Epoch: 60 \tLoss: tensor(0.8606, grad_fn=<AddBackward0>)\n",
            "Epoch: 61 \tLoss: tensor(0.8443, grad_fn=<AddBackward0>)\n",
            "Epoch: 62 \tLoss: tensor(0.8280, grad_fn=<AddBackward0>)\n",
            "Epoch: 63 \tLoss: tensor(0.8160, grad_fn=<AddBackward0>)\n",
            "Epoch: 64 \tLoss: tensor(0.7996, grad_fn=<AddBackward0>)\n",
            "Epoch: 65 \tLoss: tensor(0.7831, grad_fn=<AddBackward0>)\n",
            "Epoch: 66 \tLoss: tensor(0.7715, grad_fn=<AddBackward0>)\n",
            "Epoch: 67 \tLoss: tensor(0.7568, grad_fn=<AddBackward0>)\n",
            "Epoch: 68 \tLoss: tensor(0.7431, grad_fn=<AddBackward0>)\n",
            "Epoch: 69 \tLoss: tensor(0.7319, grad_fn=<AddBackward0>)\n",
            "Epoch: 70 \tLoss: tensor(0.7205, grad_fn=<AddBackward0>)\n",
            "Epoch: 71 \tLoss: tensor(0.7064, grad_fn=<AddBackward0>)\n",
            "Epoch: 72 \tLoss: tensor(0.6955, grad_fn=<AddBackward0>)\n",
            "Epoch: 73 \tLoss: tensor(0.6857, grad_fn=<AddBackward0>)\n",
            "Epoch: 74 \tLoss: tensor(0.6733, grad_fn=<AddBackward0>)\n",
            "Epoch: 75 \tLoss: tensor(0.6658, grad_fn=<AddBackward0>)\n",
            "Epoch: 76 \tLoss: tensor(0.6527, grad_fn=<AddBackward0>)\n",
            "Epoch: 77 \tLoss: tensor(0.6454, grad_fn=<AddBackward0>)\n",
            "Epoch: 78 \tLoss: tensor(0.6342, grad_fn=<AddBackward0>)\n",
            "Epoch: 79 \tLoss: tensor(0.6261, grad_fn=<AddBackward0>)\n",
            "Epoch: 80 \tLoss: tensor(0.6160, grad_fn=<AddBackward0>)\n",
            "Epoch: 81 \tLoss: tensor(0.6067, grad_fn=<AddBackward0>)\n",
            "Epoch: 82 \tLoss: tensor(0.5974, grad_fn=<AddBackward0>)\n",
            "Epoch: 83 \tLoss: tensor(0.5895, grad_fn=<AddBackward0>)\n",
            "Epoch: 84 \tLoss: tensor(0.5822, grad_fn=<AddBackward0>)\n",
            "Epoch: 85 \tLoss: tensor(0.5722, grad_fn=<AddBackward0>)\n",
            "Epoch: 86 \tLoss: tensor(0.5657, grad_fn=<AddBackward0>)\n",
            "Epoch: 87 \tLoss: tensor(0.5579, grad_fn=<AddBackward0>)\n",
            "Epoch: 88 \tLoss: tensor(0.5497, grad_fn=<AddBackward0>)\n",
            "Epoch: 89 \tLoss: tensor(0.5388, grad_fn=<AddBackward0>)\n",
            "Epoch: 90 \tLoss: tensor(0.5342, grad_fn=<AddBackward0>)\n",
            "Epoch: 91 \tLoss: tensor(0.5254, grad_fn=<AddBackward0>)\n",
            "Epoch: 92 \tLoss: tensor(0.5189, grad_fn=<AddBackward0>)\n",
            "Epoch: 93 \tLoss: tensor(0.5136, grad_fn=<AddBackward0>)\n",
            "Epoch: 94 \tLoss: tensor(0.5061, grad_fn=<AddBackward0>)\n",
            "Epoch: 95 \tLoss: tensor(0.5000, grad_fn=<AddBackward0>)\n",
            "Epoch: 96 \tLoss: tensor(0.4938, grad_fn=<AddBackward0>)\n",
            "Epoch: 97 \tLoss: tensor(0.4844, grad_fn=<AddBackward0>)\n",
            "Epoch: 98 \tLoss: tensor(0.4810, grad_fn=<AddBackward0>)\n",
            "Epoch: 99 \tLoss: tensor(0.4751, grad_fn=<AddBackward0>)\n",
            "Epoch: 100 \tLoss: tensor(0.4683, grad_fn=<AddBackward0>)\n",
            "Epoch: 101 \tLoss: tensor(0.4623, grad_fn=<AddBackward0>)\n",
            "Epoch: 102 \tLoss: tensor(0.4560, grad_fn=<AddBackward0>)\n",
            "Epoch: 103 \tLoss: tensor(0.4498, grad_fn=<AddBackward0>)\n",
            "Epoch: 104 \tLoss: tensor(0.4459, grad_fn=<AddBackward0>)\n",
            "Epoch: 105 \tLoss: tensor(0.4394, grad_fn=<AddBackward0>)\n",
            "Epoch: 106 \tLoss: tensor(0.4347, grad_fn=<AddBackward0>)\n",
            "Epoch: 107 \tLoss: tensor(0.4287, grad_fn=<AddBackward0>)\n",
            "Epoch: 108 \tLoss: tensor(0.4241, grad_fn=<AddBackward0>)\n",
            "Epoch: 109 \tLoss: tensor(0.4203, grad_fn=<AddBackward0>)\n",
            "Epoch: 110 \tLoss: tensor(0.4143, grad_fn=<AddBackward0>)\n",
            "Epoch: 111 \tLoss: tensor(0.4080, grad_fn=<AddBackward0>)\n",
            "Epoch: 112 \tLoss: tensor(0.4043, grad_fn=<AddBackward0>)\n",
            "Epoch: 113 \tLoss: tensor(0.3975, grad_fn=<AddBackward0>)\n",
            "Epoch: 114 \tLoss: tensor(0.3942, grad_fn=<AddBackward0>)\n",
            "Epoch: 115 \tLoss: tensor(0.3907, grad_fn=<AddBackward0>)\n",
            "Epoch: 116 \tLoss: tensor(0.3857, grad_fn=<AddBackward0>)\n",
            "Epoch: 117 \tLoss: tensor(0.3816, grad_fn=<AddBackward0>)\n",
            "Epoch: 118 \tLoss: tensor(0.3797, grad_fn=<AddBackward0>)\n",
            "Epoch: 119 \tLoss: tensor(0.3742, grad_fn=<AddBackward0>)\n",
            "Epoch: 120 \tLoss: tensor(0.3695, grad_fn=<AddBackward0>)\n",
            "Epoch: 121 \tLoss: tensor(0.3630, grad_fn=<AddBackward0>)\n",
            "Epoch: 122 \tLoss: tensor(0.3612, grad_fn=<AddBackward0>)\n",
            "Epoch: 123 \tLoss: tensor(0.3566, grad_fn=<AddBackward0>)\n",
            "Epoch: 124 \tLoss: tensor(0.3541, grad_fn=<AddBackward0>)\n",
            "Epoch: 125 \tLoss: tensor(0.3504, grad_fn=<AddBackward0>)\n",
            "Epoch: 126 \tLoss: tensor(0.3461, grad_fn=<AddBackward0>)\n",
            "Epoch: 127 \tLoss: tensor(0.3416, grad_fn=<AddBackward0>)\n",
            "Epoch: 128 \tLoss: tensor(0.3384, grad_fn=<AddBackward0>)\n",
            "Epoch: 129 \tLoss: tensor(0.3372, grad_fn=<AddBackward0>)\n",
            "Epoch: 130 \tLoss: tensor(0.3327, grad_fn=<AddBackward0>)\n",
            "Epoch: 131 \tLoss: tensor(0.3273, grad_fn=<AddBackward0>)\n",
            "Epoch: 132 \tLoss: tensor(0.3257, grad_fn=<AddBackward0>)\n",
            "Epoch: 133 \tLoss: tensor(0.3223, grad_fn=<AddBackward0>)\n",
            "Epoch: 134 \tLoss: tensor(0.3181, grad_fn=<AddBackward0>)\n",
            "Epoch: 135 \tLoss: tensor(0.3156, grad_fn=<AddBackward0>)\n",
            "Epoch: 136 \tLoss: tensor(0.3126, grad_fn=<AddBackward0>)\n",
            "Epoch: 137 \tLoss: tensor(0.3097, grad_fn=<AddBackward0>)\n",
            "Epoch: 138 \tLoss: tensor(0.3069, grad_fn=<AddBackward0>)\n",
            "Epoch: 139 \tLoss: tensor(0.3039, grad_fn=<AddBackward0>)\n",
            "Epoch: 140 \tLoss: tensor(0.3011, grad_fn=<AddBackward0>)\n",
            "Epoch: 141 \tLoss: tensor(0.2991, grad_fn=<AddBackward0>)\n",
            "Epoch: 142 \tLoss: tensor(0.2951, grad_fn=<AddBackward0>)\n",
            "Epoch: 143 \tLoss: tensor(0.2916, grad_fn=<AddBackward0>)\n",
            "Epoch: 144 \tLoss: tensor(0.2904, grad_fn=<AddBackward0>)\n",
            "Epoch: 145 \tLoss: tensor(0.2883, grad_fn=<AddBackward0>)\n",
            "Epoch: 146 \tLoss: tensor(0.2857, grad_fn=<AddBackward0>)\n",
            "Epoch: 147 \tLoss: tensor(0.2827, grad_fn=<AddBackward0>)\n",
            "Epoch: 148 \tLoss: tensor(0.2798, grad_fn=<AddBackward0>)\n",
            "Epoch: 149 \tLoss: tensor(0.2770, grad_fn=<AddBackward0>)\n",
            "Epoch: 150 \tLoss: tensor(0.2753, grad_fn=<AddBackward0>)\n",
            "Epoch: 151 \tLoss: tensor(0.2729, grad_fn=<AddBackward0>)\n",
            "Epoch: 152 \tLoss: tensor(0.2694, grad_fn=<AddBackward0>)\n",
            "Epoch: 153 \tLoss: tensor(0.2672, grad_fn=<AddBackward0>)\n",
            "Epoch: 154 \tLoss: tensor(0.2647, grad_fn=<AddBackward0>)\n",
            "Epoch: 155 \tLoss: tensor(0.2637, grad_fn=<AddBackward0>)\n",
            "Epoch: 156 \tLoss: tensor(0.2610, grad_fn=<AddBackward0>)\n",
            "Epoch: 157 \tLoss: tensor(0.2584, grad_fn=<AddBackward0>)\n",
            "Epoch: 158 \tLoss: tensor(0.2560, grad_fn=<AddBackward0>)\n",
            "Epoch: 159 \tLoss: tensor(0.2539, grad_fn=<AddBackward0>)\n",
            "Epoch: 160 \tLoss: tensor(0.2528, grad_fn=<AddBackward0>)\n",
            "Epoch: 161 \tLoss: tensor(0.2504, grad_fn=<AddBackward0>)\n",
            "Epoch: 162 \tLoss: tensor(0.2482, grad_fn=<AddBackward0>)\n",
            "Epoch: 163 \tLoss: tensor(0.2458, grad_fn=<AddBackward0>)\n",
            "Epoch: 164 \tLoss: tensor(0.2440, grad_fn=<AddBackward0>)\n",
            "Epoch: 165 \tLoss: tensor(0.2436, grad_fn=<AddBackward0>)\n",
            "Epoch: 166 \tLoss: tensor(0.2405, grad_fn=<AddBackward0>)\n",
            "Epoch: 167 \tLoss: tensor(0.2383, grad_fn=<AddBackward0>)\n",
            "Epoch: 168 \tLoss: tensor(0.2357, grad_fn=<AddBackward0>)\n",
            "Epoch: 169 \tLoss: tensor(0.2343, grad_fn=<AddBackward0>)\n",
            "Epoch: 170 \tLoss: tensor(0.2332, grad_fn=<AddBackward0>)\n",
            "Epoch: 171 \tLoss: tensor(0.2314, grad_fn=<AddBackward0>)\n",
            "Epoch: 172 \tLoss: tensor(0.2295, grad_fn=<AddBackward0>)\n",
            "Epoch: 173 \tLoss: tensor(0.2269, grad_fn=<AddBackward0>)\n",
            "Epoch: 174 \tLoss: tensor(0.2256, grad_fn=<AddBackward0>)\n",
            "Epoch: 175 \tLoss: tensor(0.2240, grad_fn=<AddBackward0>)\n",
            "Epoch: 176 \tLoss: tensor(0.2228, grad_fn=<AddBackward0>)\n",
            "Epoch: 177 \tLoss: tensor(0.2212, grad_fn=<AddBackward0>)\n",
            "Epoch: 178 \tLoss: tensor(0.2186, grad_fn=<AddBackward0>)\n",
            "Epoch: 179 \tLoss: tensor(0.2176, grad_fn=<AddBackward0>)\n",
            "Epoch: 180 \tLoss: tensor(0.2156, grad_fn=<AddBackward0>)\n",
            "Epoch: 181 \tLoss: tensor(0.2149, grad_fn=<AddBackward0>)\n",
            "Epoch: 182 \tLoss: tensor(0.2135, grad_fn=<AddBackward0>)\n",
            "Epoch: 183 \tLoss: tensor(0.2126, grad_fn=<AddBackward0>)\n",
            "Epoch: 184 \tLoss: tensor(0.2103, grad_fn=<AddBackward0>)\n",
            "Epoch: 185 \tLoss: tensor(0.2099, grad_fn=<AddBackward0>)\n",
            "Epoch: 186 \tLoss: tensor(0.2080, grad_fn=<AddBackward0>)\n",
            "Epoch: 187 \tLoss: tensor(0.2059, grad_fn=<AddBackward0>)\n",
            "Epoch: 188 \tLoss: tensor(0.2053, grad_fn=<AddBackward0>)\n",
            "Epoch: 189 \tLoss: tensor(0.2038, grad_fn=<AddBackward0>)\n",
            "Epoch: 190 \tLoss: tensor(0.2025, grad_fn=<AddBackward0>)\n",
            "Epoch: 191 \tLoss: tensor(0.2006, grad_fn=<AddBackward0>)\n",
            "Epoch: 192 \tLoss: tensor(0.1992, grad_fn=<AddBackward0>)\n",
            "Epoch: 193 \tLoss: tensor(0.1977, grad_fn=<AddBackward0>)\n",
            "Epoch: 194 \tLoss: tensor(0.1974, grad_fn=<AddBackward0>)\n",
            "Epoch: 195 \tLoss: tensor(0.1957, grad_fn=<AddBackward0>)\n",
            "Epoch: 196 \tLoss: tensor(0.1941, grad_fn=<AddBackward0>)\n",
            "Epoch: 197 \tLoss: tensor(0.1927, grad_fn=<AddBackward0>)\n",
            "Epoch: 198 \tLoss: tensor(0.1914, grad_fn=<AddBackward0>)\n",
            "Epoch: 199 \tLoss: tensor(0.1908, grad_fn=<AddBackward0>)\n",
            "Epoch: 200 \tLoss: tensor(0.1890, grad_fn=<AddBackward0>)\n",
            "Epoch: 201 \tLoss: tensor(0.1884, grad_fn=<AddBackward0>)\n",
            "Epoch: 202 \tLoss: tensor(0.1868, grad_fn=<AddBackward0>)\n",
            "Epoch: 203 \tLoss: tensor(0.1858, grad_fn=<AddBackward0>)\n",
            "Epoch: 204 \tLoss: tensor(0.1850, grad_fn=<AddBackward0>)\n",
            "Epoch: 205 \tLoss: tensor(0.1832, grad_fn=<AddBackward0>)\n",
            "Epoch: 206 \tLoss: tensor(0.1831, grad_fn=<AddBackward0>)\n",
            "Epoch: 207 \tLoss: tensor(0.1815, grad_fn=<AddBackward0>)\n",
            "Epoch: 208 \tLoss: tensor(0.1812, grad_fn=<AddBackward0>)\n",
            "Epoch: 209 \tLoss: tensor(0.1797, grad_fn=<AddBackward0>)\n",
            "Epoch: 210 \tLoss: tensor(0.1788, grad_fn=<AddBackward0>)\n",
            "Epoch: 211 \tLoss: tensor(0.1772, grad_fn=<AddBackward0>)\n",
            "Epoch: 212 \tLoss: tensor(0.1767, grad_fn=<AddBackward0>)\n",
            "Epoch: 213 \tLoss: tensor(0.1752, grad_fn=<AddBackward0>)\n",
            "Epoch: 214 \tLoss: tensor(0.1742, grad_fn=<AddBackward0>)\n",
            "Epoch: 215 \tLoss: tensor(0.1734, grad_fn=<AddBackward0>)\n",
            "Epoch: 216 \tLoss: tensor(0.1724, grad_fn=<AddBackward0>)\n",
            "Epoch: 217 \tLoss: tensor(0.1713, grad_fn=<AddBackward0>)\n",
            "Epoch: 218 \tLoss: tensor(0.1701, grad_fn=<AddBackward0>)\n",
            "Epoch: 219 \tLoss: tensor(0.1688, grad_fn=<AddBackward0>)\n",
            "Epoch: 220 \tLoss: tensor(0.1682, grad_fn=<AddBackward0>)\n",
            "Epoch: 221 \tLoss: tensor(0.1672, grad_fn=<AddBackward0>)\n",
            "Epoch: 222 \tLoss: tensor(0.1663, grad_fn=<AddBackward0>)\n",
            "Epoch: 223 \tLoss: tensor(0.1660, grad_fn=<AddBackward0>)\n",
            "Epoch: 224 \tLoss: tensor(0.1645, grad_fn=<AddBackward0>)\n",
            "Epoch: 225 \tLoss: tensor(0.1635, grad_fn=<AddBackward0>)\n",
            "Epoch: 226 \tLoss: tensor(0.1623, grad_fn=<AddBackward0>)\n",
            "Epoch: 227 \tLoss: tensor(0.1619, grad_fn=<AddBackward0>)\n",
            "Epoch: 228 \tLoss: tensor(0.1606, grad_fn=<AddBackward0>)\n",
            "Epoch: 229 \tLoss: tensor(0.1599, grad_fn=<AddBackward0>)\n",
            "Epoch: 230 \tLoss: tensor(0.1589, grad_fn=<AddBackward0>)\n",
            "Epoch: 231 \tLoss: tensor(0.1590, grad_fn=<AddBackward0>)\n",
            "Epoch: 232 \tLoss: tensor(0.1576, grad_fn=<AddBackward0>)\n",
            "Epoch: 233 \tLoss: tensor(0.1566, grad_fn=<AddBackward0>)\n",
            "Epoch: 234 \tLoss: tensor(0.1563, grad_fn=<AddBackward0>)\n",
            "Epoch: 235 \tLoss: tensor(0.1560, grad_fn=<AddBackward0>)\n",
            "Epoch: 236 \tLoss: tensor(0.1547, grad_fn=<AddBackward0>)\n",
            "Epoch: 237 \tLoss: tensor(0.1537, grad_fn=<AddBackward0>)\n",
            "Epoch: 238 \tLoss: tensor(0.1526, grad_fn=<AddBackward0>)\n",
            "Epoch: 239 \tLoss: tensor(0.1524, grad_fn=<AddBackward0>)\n",
            "Epoch: 240 \tLoss: tensor(0.1512, grad_fn=<AddBackward0>)\n",
            "Epoch: 241 \tLoss: tensor(0.1505, grad_fn=<AddBackward0>)\n",
            "Epoch: 242 \tLoss: tensor(0.1495, grad_fn=<AddBackward0>)\n",
            "Epoch: 243 \tLoss: tensor(0.1485, grad_fn=<AddBackward0>)\n",
            "Epoch: 244 \tLoss: tensor(0.1481, grad_fn=<AddBackward0>)\n",
            "Epoch: 245 \tLoss: tensor(0.1471, grad_fn=<AddBackward0>)\n",
            "Epoch: 246 \tLoss: tensor(0.1463, grad_fn=<AddBackward0>)\n",
            "Epoch: 247 \tLoss: tensor(0.1458, grad_fn=<AddBackward0>)\n",
            "Epoch: 248 \tLoss: tensor(0.1450, grad_fn=<AddBackward0>)\n",
            "Epoch: 249 \tLoss: tensor(0.1444, grad_fn=<AddBackward0>)\n",
            "Epoch: 250 \tLoss: tensor(0.1435, grad_fn=<AddBackward0>)\n",
            "Epoch: 251 \tLoss: tensor(0.1428, grad_fn=<AddBackward0>)\n",
            "Epoch: 252 \tLoss: tensor(0.1423, grad_fn=<AddBackward0>)\n",
            "Epoch: 253 \tLoss: tensor(0.1416, grad_fn=<AddBackward0>)\n",
            "Epoch: 254 \tLoss: tensor(0.1407, grad_fn=<AddBackward0>)\n",
            "Epoch: 255 \tLoss: tensor(0.1400, grad_fn=<AddBackward0>)\n",
            "Epoch: 256 \tLoss: tensor(0.1397, grad_fn=<AddBackward0>)\n",
            "Epoch: 257 \tLoss: tensor(0.1391, grad_fn=<AddBackward0>)\n",
            "Epoch: 258 \tLoss: tensor(0.1385, grad_fn=<AddBackward0>)\n",
            "Epoch: 259 \tLoss: tensor(0.1377, grad_fn=<AddBackward0>)\n",
            "Epoch: 260 \tLoss: tensor(0.1368, grad_fn=<AddBackward0>)\n",
            "Epoch: 261 \tLoss: tensor(0.1361, grad_fn=<AddBackward0>)\n",
            "Epoch: 262 \tLoss: tensor(0.1358, grad_fn=<AddBackward0>)\n",
            "Epoch: 263 \tLoss: tensor(0.1350, grad_fn=<AddBackward0>)\n",
            "Epoch: 264 \tLoss: tensor(0.1342, grad_fn=<AddBackward0>)\n",
            "Epoch: 265 \tLoss: tensor(0.1343, grad_fn=<AddBackward0>)\n",
            "Epoch: 266 \tLoss: tensor(0.1334, grad_fn=<AddBackward0>)\n",
            "Epoch: 267 \tLoss: tensor(0.1335, grad_fn=<AddBackward0>)\n",
            "Epoch: 268 \tLoss: tensor(0.1325, grad_fn=<AddBackward0>)\n",
            "Epoch: 269 \tLoss: tensor(0.1316, grad_fn=<AddBackward0>)\n",
            "Epoch: 270 \tLoss: tensor(0.1310, grad_fn=<AddBackward0>)\n",
            "Epoch: 271 \tLoss: tensor(0.1304, grad_fn=<AddBackward0>)\n",
            "Epoch: 272 \tLoss: tensor(0.1295, grad_fn=<AddBackward0>)\n",
            "Epoch: 273 \tLoss: tensor(0.1289, grad_fn=<AddBackward0>)\n",
            "Epoch: 274 \tLoss: tensor(0.1281, grad_fn=<AddBackward0>)\n",
            "Epoch: 275 \tLoss: tensor(0.1284, grad_fn=<AddBackward0>)\n",
            "Epoch: 276 \tLoss: tensor(0.1276, grad_fn=<AddBackward0>)\n",
            "Epoch: 277 \tLoss: tensor(0.1267, grad_fn=<AddBackward0>)\n",
            "Epoch: 278 \tLoss: tensor(0.1262, grad_fn=<AddBackward0>)\n",
            "Epoch: 279 \tLoss: tensor(0.1257, grad_fn=<AddBackward0>)\n",
            "Epoch: 280 \tLoss: tensor(0.1250, grad_fn=<AddBackward0>)\n",
            "Epoch: 281 \tLoss: tensor(0.1244, grad_fn=<AddBackward0>)\n",
            "Epoch: 282 \tLoss: tensor(0.1239, grad_fn=<AddBackward0>)\n",
            "Epoch: 283 \tLoss: tensor(0.1233, grad_fn=<AddBackward0>)\n",
            "Epoch: 284 \tLoss: tensor(0.1227, grad_fn=<AddBackward0>)\n",
            "Epoch: 285 \tLoss: tensor(0.1226, grad_fn=<AddBackward0>)\n",
            "Epoch: 286 \tLoss: tensor(0.1219, grad_fn=<AddBackward0>)\n",
            "Epoch: 287 \tLoss: tensor(0.1215, grad_fn=<AddBackward0>)\n",
            "Epoch: 288 \tLoss: tensor(0.1208, grad_fn=<AddBackward0>)\n",
            "Epoch: 289 \tLoss: tensor(0.1202, grad_fn=<AddBackward0>)\n",
            "Epoch: 290 \tLoss: tensor(0.1199, grad_fn=<AddBackward0>)\n",
            "Epoch: 291 \tLoss: tensor(0.1193, grad_fn=<AddBackward0>)\n",
            "Epoch: 292 \tLoss: tensor(0.1188, grad_fn=<AddBackward0>)\n",
            "Epoch: 293 \tLoss: tensor(0.1182, grad_fn=<AddBackward0>)\n",
            "Epoch: 294 \tLoss: tensor(0.1177, grad_fn=<AddBackward0>)\n",
            "Epoch: 295 \tLoss: tensor(0.1178, grad_fn=<AddBackward0>)\n",
            "Epoch: 296 \tLoss: tensor(0.1171, grad_fn=<AddBackward0>)\n",
            "Epoch: 297 \tLoss: tensor(0.1165, grad_fn=<AddBackward0>)\n",
            "Epoch: 298 \tLoss: tensor(0.1160, grad_fn=<AddBackward0>)\n",
            "Epoch: 299 \tLoss: tensor(0.1158, grad_fn=<AddBackward0>)\n",
            "Epoch: 300 \tLoss: tensor(0.1151, grad_fn=<AddBackward0>)\n",
            "Epoch: 301 \tLoss: tensor(0.1147, grad_fn=<AddBackward0>)\n",
            "Epoch: 302 \tLoss: tensor(0.1141, grad_fn=<AddBackward0>)\n",
            "Epoch: 303 \tLoss: tensor(0.1135, grad_fn=<AddBackward0>)\n",
            "Epoch: 304 \tLoss: tensor(0.1133, grad_fn=<AddBackward0>)\n",
            "Epoch: 305 \tLoss: tensor(0.1126, grad_fn=<AddBackward0>)\n",
            "Epoch: 306 \tLoss: tensor(0.1129, grad_fn=<AddBackward0>)\n",
            "Epoch: 307 \tLoss: tensor(0.1123, grad_fn=<AddBackward0>)\n",
            "Epoch: 308 \tLoss: tensor(0.1118, grad_fn=<AddBackward0>)\n",
            "Epoch: 309 \tLoss: tensor(0.1113, grad_fn=<AddBackward0>)\n",
            "Epoch: 310 \tLoss: tensor(0.1107, grad_fn=<AddBackward0>)\n",
            "Epoch: 311 \tLoss: tensor(0.1102, grad_fn=<AddBackward0>)\n",
            "Epoch: 312 \tLoss: tensor(0.1096, grad_fn=<AddBackward0>)\n",
            "Epoch: 313 \tLoss: tensor(0.1094, grad_fn=<AddBackward0>)\n",
            "Epoch: 314 \tLoss: tensor(0.1087, grad_fn=<AddBackward0>)\n",
            "Epoch: 315 \tLoss: tensor(0.1083, grad_fn=<AddBackward0>)\n",
            "Epoch: 316 \tLoss: tensor(0.1080, grad_fn=<AddBackward0>)\n",
            "Epoch: 317 \tLoss: tensor(0.1076, grad_fn=<AddBackward0>)\n",
            "Epoch: 318 \tLoss: tensor(0.1074, grad_fn=<AddBackward0>)\n",
            "Epoch: 319 \tLoss: tensor(0.1069, grad_fn=<AddBackward0>)\n",
            "Epoch: 320 \tLoss: tensor(0.1065, grad_fn=<AddBackward0>)\n",
            "Epoch: 321 \tLoss: tensor(0.1061, grad_fn=<AddBackward0>)\n",
            "Epoch: 322 \tLoss: tensor(0.1056, grad_fn=<AddBackward0>)\n",
            "Epoch: 323 \tLoss: tensor(0.1053, grad_fn=<AddBackward0>)\n",
            "Epoch: 324 \tLoss: tensor(0.1048, grad_fn=<AddBackward0>)\n",
            "Epoch: 325 \tLoss: tensor(0.1044, grad_fn=<AddBackward0>)\n",
            "Epoch: 326 \tLoss: tensor(0.1041, grad_fn=<AddBackward0>)\n",
            "Epoch: 327 \tLoss: tensor(0.1037, grad_fn=<AddBackward0>)\n",
            "Epoch: 328 \tLoss: tensor(0.1033, grad_fn=<AddBackward0>)\n",
            "Epoch: 329 \tLoss: tensor(0.1030, grad_fn=<AddBackward0>)\n",
            "Epoch: 330 \tLoss: tensor(0.1030, grad_fn=<AddBackward0>)\n",
            "Epoch: 331 \tLoss: tensor(0.1025, grad_fn=<AddBackward0>)\n",
            "Epoch: 332 \tLoss: tensor(0.1021, grad_fn=<AddBackward0>)\n",
            "Epoch: 333 \tLoss: tensor(0.1015, grad_fn=<AddBackward0>)\n",
            "Epoch: 334 \tLoss: tensor(0.1014, grad_fn=<AddBackward0>)\n",
            "Epoch: 335 \tLoss: tensor(0.1009, grad_fn=<AddBackward0>)\n",
            "Epoch: 336 \tLoss: tensor(0.1006, grad_fn=<AddBackward0>)\n",
            "Epoch: 337 \tLoss: tensor(0.1000, grad_fn=<AddBackward0>)\n",
            "Epoch: 338 \tLoss: tensor(0.0996, grad_fn=<AddBackward0>)\n",
            "Epoch: 339 \tLoss: tensor(0.0994, grad_fn=<AddBackward0>)\n",
            "Epoch: 340 \tLoss: tensor(0.0990, grad_fn=<AddBackward0>)\n",
            "Epoch: 341 \tLoss: tensor(0.0987, grad_fn=<AddBackward0>)\n",
            "Epoch: 342 \tLoss: tensor(0.0984, grad_fn=<AddBackward0>)\n",
            "Epoch: 343 \tLoss: tensor(0.0985, grad_fn=<AddBackward0>)\n",
            "Epoch: 344 \tLoss: tensor(0.0980, grad_fn=<AddBackward0>)\n",
            "Epoch: 345 \tLoss: tensor(0.0975, grad_fn=<AddBackward0>)\n",
            "Epoch: 346 \tLoss: tensor(0.0974, grad_fn=<AddBackward0>)\n",
            "Epoch: 347 \tLoss: tensor(0.0969, grad_fn=<AddBackward0>)\n",
            "Epoch: 348 \tLoss: tensor(0.0965, grad_fn=<AddBackward0>)\n",
            "Epoch: 349 \tLoss: tensor(0.0962, grad_fn=<AddBackward0>)\n",
            "Epoch: 350 \tLoss: tensor(0.0960, grad_fn=<AddBackward0>)\n",
            "Epoch: 351 \tLoss: tensor(0.0955, grad_fn=<AddBackward0>)\n",
            "Epoch: 352 \tLoss: tensor(0.0956, grad_fn=<AddBackward0>)\n",
            "Epoch: 353 \tLoss: tensor(0.0951, grad_fn=<AddBackward0>)\n",
            "Epoch: 354 \tLoss: tensor(0.0947, grad_fn=<AddBackward0>)\n",
            "Epoch: 355 \tLoss: tensor(0.0944, grad_fn=<AddBackward0>)\n",
            "Epoch: 356 \tLoss: tensor(0.0943, grad_fn=<AddBackward0>)\n",
            "Epoch: 357 \tLoss: tensor(0.0939, grad_fn=<AddBackward0>)\n",
            "Epoch: 358 \tLoss: tensor(0.0935, grad_fn=<AddBackward0>)\n",
            "Epoch: 359 \tLoss: tensor(0.0932, grad_fn=<AddBackward0>)\n",
            "Epoch: 360 \tLoss: tensor(0.0927, grad_fn=<AddBackward0>)\n",
            "Epoch: 361 \tLoss: tensor(0.0924, grad_fn=<AddBackward0>)\n",
            "Epoch: 362 \tLoss: tensor(0.0922, grad_fn=<AddBackward0>)\n",
            "Epoch: 363 \tLoss: tensor(0.0919, grad_fn=<AddBackward0>)\n",
            "Epoch: 364 \tLoss: tensor(0.0916, grad_fn=<AddBackward0>)\n",
            "Epoch: 365 \tLoss: tensor(0.0911, grad_fn=<AddBackward0>)\n",
            "Epoch: 366 \tLoss: tensor(0.0909, grad_fn=<AddBackward0>)\n",
            "Epoch: 367 \tLoss: tensor(0.0906, grad_fn=<AddBackward0>)\n",
            "Epoch: 368 \tLoss: tensor(0.0903, grad_fn=<AddBackward0>)\n",
            "Epoch: 369 \tLoss: tensor(0.0901, grad_fn=<AddBackward0>)\n",
            "Epoch: 370 \tLoss: tensor(0.0900, grad_fn=<AddBackward0>)\n",
            "Epoch: 371 \tLoss: tensor(0.0897, grad_fn=<AddBackward0>)\n",
            "Epoch: 372 \tLoss: tensor(0.0894, grad_fn=<AddBackward0>)\n",
            "Epoch: 373 \tLoss: tensor(0.0891, grad_fn=<AddBackward0>)\n",
            "Epoch: 374 \tLoss: tensor(0.0888, grad_fn=<AddBackward0>)\n",
            "Epoch: 375 \tLoss: tensor(0.0886, grad_fn=<AddBackward0>)\n",
            "Epoch: 376 \tLoss: tensor(0.0882, grad_fn=<AddBackward0>)\n",
            "Epoch: 377 \tLoss: tensor(0.0879, grad_fn=<AddBackward0>)\n",
            "Epoch: 378 \tLoss: tensor(0.0876, grad_fn=<AddBackward0>)\n",
            "Epoch: 379 \tLoss: tensor(0.0874, grad_fn=<AddBackward0>)\n",
            "Epoch: 380 \tLoss: tensor(0.0871, grad_fn=<AddBackward0>)\n",
            "Epoch: 381 \tLoss: tensor(0.0868, grad_fn=<AddBackward0>)\n",
            "Epoch: 382 \tLoss: tensor(0.0865, grad_fn=<AddBackward0>)\n",
            "Epoch: 383 \tLoss: tensor(0.0862, grad_fn=<AddBackward0>)\n",
            "Epoch: 384 \tLoss: tensor(0.0863, grad_fn=<AddBackward0>)\n",
            "Epoch: 385 \tLoss: tensor(0.0860, grad_fn=<AddBackward0>)\n",
            "Epoch: 386 \tLoss: tensor(0.0856, grad_fn=<AddBackward0>)\n",
            "Epoch: 387 \tLoss: tensor(0.0855, grad_fn=<AddBackward0>)\n",
            "Epoch: 388 \tLoss: tensor(0.0852, grad_fn=<AddBackward0>)\n",
            "Epoch: 389 \tLoss: tensor(0.0848, grad_fn=<AddBackward0>)\n",
            "Epoch: 390 \tLoss: tensor(0.0844, grad_fn=<AddBackward0>)\n",
            "Epoch: 391 \tLoss: tensor(0.0842, grad_fn=<AddBackward0>)\n",
            "Epoch: 392 \tLoss: tensor(0.0842, grad_fn=<AddBackward0>)\n",
            "Epoch: 393 \tLoss: tensor(0.0838, grad_fn=<AddBackward0>)\n",
            "Epoch: 394 \tLoss: tensor(0.0836, grad_fn=<AddBackward0>)\n",
            "Epoch: 395 \tLoss: tensor(0.0832, grad_fn=<AddBackward0>)\n",
            "Epoch: 396 \tLoss: tensor(0.0830, grad_fn=<AddBackward0>)\n",
            "Epoch: 397 \tLoss: tensor(0.0829, grad_fn=<AddBackward0>)\n",
            "Epoch: 398 \tLoss: tensor(0.0825, grad_fn=<AddBackward0>)\n",
            "Epoch: 399 \tLoss: tensor(0.0826, grad_fn=<AddBackward0>)\n",
            "Epoch: 400 \tLoss: tensor(0.0825, grad_fn=<AddBackward0>)\n",
            "Epoch: 401 \tLoss: tensor(0.0822, grad_fn=<AddBackward0>)\n",
            "Epoch: 402 \tLoss: tensor(0.0818, grad_fn=<AddBackward0>)\n",
            "Epoch: 403 \tLoss: tensor(0.0816, grad_fn=<AddBackward0>)\n",
            "Epoch: 404 \tLoss: tensor(0.0814, grad_fn=<AddBackward0>)\n",
            "Epoch: 405 \tLoss: tensor(0.0811, grad_fn=<AddBackward0>)\n",
            "Epoch: 406 \tLoss: tensor(0.0812, grad_fn=<AddBackward0>)\n",
            "Epoch: 407 \tLoss: tensor(0.0808, grad_fn=<AddBackward0>)\n",
            "Epoch: 408 \tLoss: tensor(0.0805, grad_fn=<AddBackward0>)\n",
            "Epoch: 409 \tLoss: tensor(0.0803, grad_fn=<AddBackward0>)\n",
            "Epoch: 410 \tLoss: tensor(0.0799, grad_fn=<AddBackward0>)\n",
            "Epoch: 411 \tLoss: tensor(0.0797, grad_fn=<AddBackward0>)\n",
            "Epoch: 412 \tLoss: tensor(0.0795, grad_fn=<AddBackward0>)\n",
            "Epoch: 413 \tLoss: tensor(0.0791, grad_fn=<AddBackward0>)\n",
            "Epoch: 414 \tLoss: tensor(0.0789, grad_fn=<AddBackward0>)\n",
            "Epoch: 415 \tLoss: tensor(0.0789, grad_fn=<AddBackward0>)\n",
            "Epoch: 416 \tLoss: tensor(0.0787, grad_fn=<AddBackward0>)\n",
            "Epoch: 417 \tLoss: tensor(0.0784, grad_fn=<AddBackward0>)\n",
            "Epoch: 418 \tLoss: tensor(0.0781, grad_fn=<AddBackward0>)\n",
            "Epoch: 419 \tLoss: tensor(0.0779, grad_fn=<AddBackward0>)\n",
            "Epoch: 420 \tLoss: tensor(0.0776, grad_fn=<AddBackward0>)\n",
            "Epoch: 421 \tLoss: tensor(0.0775, grad_fn=<AddBackward0>)\n",
            "Epoch: 422 \tLoss: tensor(0.0772, grad_fn=<AddBackward0>)\n",
            "Epoch: 423 \tLoss: tensor(0.0771, grad_fn=<AddBackward0>)\n",
            "Epoch: 424 \tLoss: tensor(0.0768, grad_fn=<AddBackward0>)\n",
            "Epoch: 425 \tLoss: tensor(0.0765, grad_fn=<AddBackward0>)\n",
            "Epoch: 426 \tLoss: tensor(0.0763, grad_fn=<AddBackward0>)\n",
            "Epoch: 427 \tLoss: tensor(0.0761, grad_fn=<AddBackward0>)\n",
            "Epoch: 428 \tLoss: tensor(0.0759, grad_fn=<AddBackward0>)\n",
            "Epoch: 429 \tLoss: tensor(0.0757, grad_fn=<AddBackward0>)\n",
            "Epoch: 430 \tLoss: tensor(0.0755, grad_fn=<AddBackward0>)\n",
            "Epoch: 431 \tLoss: tensor(0.0752, grad_fn=<AddBackward0>)\n",
            "Epoch: 432 \tLoss: tensor(0.0754, grad_fn=<AddBackward0>)\n",
            "Epoch: 433 \tLoss: tensor(0.0751, grad_fn=<AddBackward0>)\n",
            "Epoch: 434 \tLoss: tensor(0.0748, grad_fn=<AddBackward0>)\n",
            "Epoch: 435 \tLoss: tensor(0.0745, grad_fn=<AddBackward0>)\n",
            "Epoch: 436 \tLoss: tensor(0.0744, grad_fn=<AddBackward0>)\n",
            "Epoch: 437 \tLoss: tensor(0.0741, grad_fn=<AddBackward0>)\n",
            "Epoch: 438 \tLoss: tensor(0.0740, grad_fn=<AddBackward0>)\n",
            "Epoch: 439 \tLoss: tensor(0.0739, grad_fn=<AddBackward0>)\n",
            "Epoch: 440 \tLoss: tensor(0.0735, grad_fn=<AddBackward0>)\n",
            "Epoch: 441 \tLoss: tensor(0.0733, grad_fn=<AddBackward0>)\n",
            "Epoch: 442 \tLoss: tensor(0.0731, grad_fn=<AddBackward0>)\n",
            "Epoch: 443 \tLoss: tensor(0.0729, grad_fn=<AddBackward0>)\n",
            "Epoch: 444 \tLoss: tensor(0.0727, grad_fn=<AddBackward0>)\n",
            "Epoch: 445 \tLoss: tensor(0.0725, grad_fn=<AddBackward0>)\n",
            "Epoch: 446 \tLoss: tensor(0.0724, grad_fn=<AddBackward0>)\n",
            "Epoch: 447 \tLoss: tensor(0.0722, grad_fn=<AddBackward0>)\n",
            "Epoch: 448 \tLoss: tensor(0.0720, grad_fn=<AddBackward0>)\n",
            "Epoch: 449 \tLoss: tensor(0.0713, grad_fn=<AddBackward0>)\n",
            "Epoch: 450 \tLoss: tensor(0.0715, grad_fn=<AddBackward0>)\n",
            "Epoch: 451 \tLoss: tensor(0.0713, grad_fn=<AddBackward0>)\n",
            "Epoch: 452 \tLoss: tensor(0.0711, grad_fn=<AddBackward0>)\n",
            "Epoch: 453 \tLoss: tensor(0.0709, grad_fn=<AddBackward0>)\n",
            "Epoch: 454 \tLoss: tensor(0.0708, grad_fn=<AddBackward0>)\n",
            "Epoch: 455 \tLoss: tensor(0.0706, grad_fn=<AddBackward0>)\n",
            "Epoch: 456 \tLoss: tensor(0.0705, grad_fn=<AddBackward0>)\n",
            "Epoch: 457 \tLoss: tensor(0.0704, grad_fn=<AddBackward0>)\n",
            "Epoch: 458 \tLoss: tensor(0.0701, grad_fn=<AddBackward0>)\n",
            "Epoch: 459 \tLoss: tensor(0.0700, grad_fn=<AddBackward0>)\n",
            "Epoch: 460 \tLoss: tensor(0.0697, grad_fn=<AddBackward0>)\n",
            "Epoch: 461 \tLoss: tensor(0.0696, grad_fn=<AddBackward0>)\n",
            "Epoch: 462 \tLoss: tensor(0.0695, grad_fn=<AddBackward0>)\n",
            "Epoch: 463 \tLoss: tensor(0.0692, grad_fn=<AddBackward0>)\n",
            "Epoch: 464 \tLoss: tensor(0.0690, grad_fn=<AddBackward0>)\n",
            "Epoch: 465 \tLoss: tensor(0.0689, grad_fn=<AddBackward0>)\n",
            "Epoch: 466 \tLoss: tensor(0.0688, grad_fn=<AddBackward0>)\n",
            "Epoch: 467 \tLoss: tensor(0.0686, grad_fn=<AddBackward0>)\n",
            "Epoch: 468 \tLoss: tensor(0.0689, grad_fn=<AddBackward0>)\n",
            "Epoch: 469 \tLoss: tensor(0.0687, grad_fn=<AddBackward0>)\n",
            "Epoch: 470 \tLoss: tensor(0.0686, grad_fn=<AddBackward0>)\n",
            "Epoch: 471 \tLoss: tensor(0.0683, grad_fn=<AddBackward0>)\n",
            "Epoch: 472 \tLoss: tensor(0.0682, grad_fn=<AddBackward0>)\n",
            "Epoch: 473 \tLoss: tensor(0.0680, grad_fn=<AddBackward0>)\n",
            "Epoch: 474 \tLoss: tensor(0.0677, grad_fn=<AddBackward0>)\n",
            "Epoch: 475 \tLoss: tensor(0.0675, grad_fn=<AddBackward0>)\n",
            "Epoch: 476 \tLoss: tensor(0.0674, grad_fn=<AddBackward0>)\n",
            "Epoch: 477 \tLoss: tensor(0.0671, grad_fn=<AddBackward0>)\n",
            "Epoch: 478 \tLoss: tensor(0.0670, grad_fn=<AddBackward0>)\n",
            "Epoch: 479 \tLoss: tensor(0.0669, grad_fn=<AddBackward0>)\n",
            "Epoch: 480 \tLoss: tensor(0.0668, grad_fn=<AddBackward0>)\n",
            "Epoch: 481 \tLoss: tensor(0.0665, grad_fn=<AddBackward0>)\n",
            "Epoch: 482 \tLoss: tensor(0.0663, grad_fn=<AddBackward0>)\n",
            "Epoch: 483 \tLoss: tensor(0.0662, grad_fn=<AddBackward0>)\n",
            "Epoch: 484 \tLoss: tensor(0.0660, grad_fn=<AddBackward0>)\n",
            "Epoch: 485 \tLoss: tensor(0.0658, grad_fn=<AddBackward0>)\n",
            "Epoch: 486 \tLoss: tensor(0.0657, grad_fn=<AddBackward0>)\n",
            "Epoch: 487 \tLoss: tensor(0.0655, grad_fn=<AddBackward0>)\n",
            "Epoch: 488 \tLoss: tensor(0.0656, grad_fn=<AddBackward0>)\n",
            "Epoch: 489 \tLoss: tensor(0.0654, grad_fn=<AddBackward0>)\n",
            "Epoch: 490 \tLoss: tensor(0.0652, grad_fn=<AddBackward0>)\n",
            "Epoch: 491 \tLoss: tensor(0.0650, grad_fn=<AddBackward0>)\n",
            "Epoch: 492 \tLoss: tensor(0.0648, grad_fn=<AddBackward0>)\n",
            "Epoch: 493 \tLoss: tensor(0.0647, grad_fn=<AddBackward0>)\n",
            "Epoch: 494 \tLoss: tensor(0.0645, grad_fn=<AddBackward0>)\n",
            "Epoch: 495 \tLoss: tensor(0.0643, grad_fn=<AddBackward0>)\n",
            "Epoch: 496 \tLoss: tensor(0.0641, grad_fn=<AddBackward0>)\n",
            "Epoch: 497 \tLoss: tensor(0.0640, grad_fn=<AddBackward0>)\n",
            "Epoch: 498 \tLoss: tensor(0.0638, grad_fn=<AddBackward0>)\n",
            "Epoch: 499 \tLoss: tensor(0.0637, grad_fn=<AddBackward0>)\n",
            "Epoch: 500 \tLoss: tensor(0.0635, grad_fn=<AddBackward0>)\n",
            "Epoch: 501 \tLoss: tensor(0.0633, grad_fn=<AddBackward0>)\n",
            "Epoch: 502 \tLoss: tensor(0.0632, grad_fn=<AddBackward0>)\n",
            "Epoch: 503 \tLoss: tensor(0.0631, grad_fn=<AddBackward0>)\n",
            "Epoch: 504 \tLoss: tensor(0.0629, grad_fn=<AddBackward0>)\n",
            "Epoch: 505 \tLoss: tensor(0.0627, grad_fn=<AddBackward0>)\n",
            "Epoch: 506 \tLoss: tensor(0.0625, grad_fn=<AddBackward0>)\n",
            "Epoch: 507 \tLoss: tensor(0.0625, grad_fn=<AddBackward0>)\n",
            "Epoch: 508 \tLoss: tensor(0.0626, grad_fn=<AddBackward0>)\n",
            "Epoch: 509 \tLoss: tensor(0.0624, grad_fn=<AddBackward0>)\n",
            "Epoch: 510 \tLoss: tensor(0.0623, grad_fn=<AddBackward0>)\n",
            "Epoch: 511 \tLoss: tensor(0.0621, grad_fn=<AddBackward0>)\n",
            "Epoch: 512 \tLoss: tensor(0.0619, grad_fn=<AddBackward0>)\n",
            "Epoch: 513 \tLoss: tensor(0.0618, grad_fn=<AddBackward0>)\n",
            "Epoch: 514 \tLoss: tensor(0.0617, grad_fn=<AddBackward0>)\n",
            "Epoch: 515 \tLoss: tensor(0.0615, grad_fn=<AddBackward0>)\n",
            "Epoch: 516 \tLoss: tensor(0.0613, grad_fn=<AddBackward0>)\n",
            "Epoch: 517 \tLoss: tensor(0.0612, grad_fn=<AddBackward0>)\n",
            "Epoch: 518 \tLoss: tensor(0.0611, grad_fn=<AddBackward0>)\n",
            "Epoch: 519 \tLoss: tensor(0.0609, grad_fn=<AddBackward0>)\n",
            "Epoch: 520 \tLoss: tensor(0.0608, grad_fn=<AddBackward0>)\n",
            "Epoch: 521 \tLoss: tensor(0.0607, grad_fn=<AddBackward0>)\n",
            "Epoch: 522 \tLoss: tensor(0.0605, grad_fn=<AddBackward0>)\n",
            "Epoch: 523 \tLoss: tensor(0.0604, grad_fn=<AddBackward0>)\n",
            "Epoch: 524 \tLoss: tensor(0.0603, grad_fn=<AddBackward0>)\n",
            "Epoch: 525 \tLoss: tensor(0.0601, grad_fn=<AddBackward0>)\n",
            "Epoch: 526 \tLoss: tensor(0.0600, grad_fn=<AddBackward0>)\n",
            "Epoch: 527 \tLoss: tensor(0.0598, grad_fn=<AddBackward0>)\n",
            "Epoch: 528 \tLoss: tensor(0.0597, grad_fn=<AddBackward0>)\n",
            "Epoch: 529 \tLoss: tensor(0.0596, grad_fn=<AddBackward0>)\n",
            "Epoch: 530 \tLoss: tensor(0.0598, grad_fn=<AddBackward0>)\n",
            "Epoch: 531 \tLoss: tensor(0.0596, grad_fn=<AddBackward0>)\n",
            "Epoch: 532 \tLoss: tensor(0.0594, grad_fn=<AddBackward0>)\n",
            "Epoch: 533 \tLoss: tensor(0.0593, grad_fn=<AddBackward0>)\n",
            "Epoch: 534 \tLoss: tensor(0.0591, grad_fn=<AddBackward0>)\n",
            "Epoch: 535 \tLoss: tensor(0.0590, grad_fn=<AddBackward0>)\n",
            "Epoch: 536 \tLoss: tensor(0.0591, grad_fn=<AddBackward0>)\n",
            "Epoch: 537 \tLoss: tensor(0.0590, grad_fn=<AddBackward0>)\n",
            "Epoch: 538 \tLoss: tensor(0.0588, grad_fn=<AddBackward0>)\n",
            "Epoch: 539 \tLoss: tensor(0.0586, grad_fn=<AddBackward0>)\n",
            "Epoch: 540 \tLoss: tensor(0.0586, grad_fn=<AddBackward0>)\n",
            "Epoch: 541 \tLoss: tensor(0.0584, grad_fn=<AddBackward0>)\n",
            "Epoch: 542 \tLoss: tensor(0.0582, grad_fn=<AddBackward0>)\n",
            "Epoch: 543 \tLoss: tensor(0.0581, grad_fn=<AddBackward0>)\n",
            "Epoch: 544 \tLoss: tensor(0.0580, grad_fn=<AddBackward0>)\n",
            "Epoch: 545 \tLoss: tensor(0.0578, grad_fn=<AddBackward0>)\n",
            "Epoch: 546 \tLoss: tensor(0.0578, grad_fn=<AddBackward0>)\n",
            "Epoch: 547 \tLoss: tensor(0.0576, grad_fn=<AddBackward0>)\n",
            "Epoch: 548 \tLoss: tensor(0.0574, grad_fn=<AddBackward0>)\n",
            "Epoch: 549 \tLoss: tensor(0.0573, grad_fn=<AddBackward0>)\n",
            "Epoch: 550 \tLoss: tensor(0.0572, grad_fn=<AddBackward0>)\n",
            "Epoch: 551 \tLoss: tensor(0.0571, grad_fn=<AddBackward0>)\n",
            "Epoch: 552 \tLoss: tensor(0.0571, grad_fn=<AddBackward0>)\n",
            "Epoch: 553 \tLoss: tensor(0.0570, grad_fn=<AddBackward0>)\n",
            "Epoch: 554 \tLoss: tensor(0.0569, grad_fn=<AddBackward0>)\n",
            "Epoch: 555 \tLoss: tensor(0.0568, grad_fn=<AddBackward0>)\n",
            "Epoch: 556 \tLoss: tensor(0.0567, grad_fn=<AddBackward0>)\n",
            "Epoch: 557 \tLoss: tensor(0.0565, grad_fn=<AddBackward0>)\n",
            "Epoch: 558 \tLoss: tensor(0.0564, grad_fn=<AddBackward0>)\n",
            "Epoch: 559 \tLoss: tensor(0.0563, grad_fn=<AddBackward0>)\n",
            "Epoch: 560 \tLoss: tensor(0.0562, grad_fn=<AddBackward0>)\n",
            "Epoch: 561 \tLoss: tensor(0.0560, grad_fn=<AddBackward0>)\n",
            "Epoch: 562 \tLoss: tensor(0.0559, grad_fn=<AddBackward0>)\n",
            "Epoch: 563 \tLoss: tensor(0.0558, grad_fn=<AddBackward0>)\n",
            "Epoch: 564 \tLoss: tensor(0.0557, grad_fn=<AddBackward0>)\n",
            "Epoch: 565 \tLoss: tensor(0.0555, grad_fn=<AddBackward0>)\n",
            "Epoch: 566 \tLoss: tensor(0.0555, grad_fn=<AddBackward0>)\n",
            "Epoch: 567 \tLoss: tensor(0.0553, grad_fn=<AddBackward0>)\n",
            "Epoch: 568 \tLoss: tensor(0.0552, grad_fn=<AddBackward0>)\n",
            "Epoch: 569 \tLoss: tensor(0.0551, grad_fn=<AddBackward0>)\n",
            "Epoch: 570 \tLoss: tensor(0.0550, grad_fn=<AddBackward0>)\n",
            "Epoch: 571 \tLoss: tensor(0.0549, grad_fn=<AddBackward0>)\n",
            "Epoch: 572 \tLoss: tensor(0.0548, grad_fn=<AddBackward0>)\n",
            "Epoch: 573 \tLoss: tensor(0.0547, grad_fn=<AddBackward0>)\n",
            "Epoch: 574 \tLoss: tensor(0.0546, grad_fn=<AddBackward0>)\n",
            "Epoch: 575 \tLoss: tensor(0.0545, grad_fn=<AddBackward0>)\n",
            "Epoch: 576 \tLoss: tensor(0.0546, grad_fn=<AddBackward0>)\n",
            "Epoch: 577 \tLoss: tensor(0.0544, grad_fn=<AddBackward0>)\n",
            "Epoch: 578 \tLoss: tensor(0.0543, grad_fn=<AddBackward0>)\n",
            "Epoch: 579 \tLoss: tensor(0.0542, grad_fn=<AddBackward0>)\n",
            "Epoch: 580 \tLoss: tensor(0.0541, grad_fn=<AddBackward0>)\n",
            "Epoch: 581 \tLoss: tensor(0.0540, grad_fn=<AddBackward0>)\n",
            "Epoch: 582 \tLoss: tensor(0.0539, grad_fn=<AddBackward0>)\n",
            "Epoch: 583 \tLoss: tensor(0.0538, grad_fn=<AddBackward0>)\n",
            "Epoch: 584 \tLoss: tensor(0.0536, grad_fn=<AddBackward0>)\n",
            "Epoch: 585 \tLoss: tensor(0.0535, grad_fn=<AddBackward0>)\n",
            "Epoch: 586 \tLoss: tensor(0.0534, grad_fn=<AddBackward0>)\n",
            "Epoch: 587 \tLoss: tensor(0.0533, grad_fn=<AddBackward0>)\n",
            "Epoch: 588 \tLoss: tensor(0.0533, grad_fn=<AddBackward0>)\n",
            "Epoch: 589 \tLoss: tensor(0.0532, grad_fn=<AddBackward0>)\n",
            "Epoch: 590 \tLoss: tensor(0.0530, grad_fn=<AddBackward0>)\n",
            "Epoch: 591 \tLoss: tensor(0.0529, grad_fn=<AddBackward0>)\n",
            "Epoch: 592 \tLoss: tensor(0.0528, grad_fn=<AddBackward0>)\n",
            "Epoch: 593 \tLoss: tensor(0.0527, grad_fn=<AddBackward0>)\n",
            "Epoch: 594 \tLoss: tensor(0.0525, grad_fn=<AddBackward0>)\n",
            "Epoch: 595 \tLoss: tensor(0.0525, grad_fn=<AddBackward0>)\n",
            "Epoch: 596 \tLoss: tensor(0.0523, grad_fn=<AddBackward0>)\n",
            "Epoch: 597 \tLoss: tensor(0.0523, grad_fn=<AddBackward0>)\n",
            "Epoch: 598 \tLoss: tensor(0.0522, grad_fn=<AddBackward0>)\n",
            "Epoch: 599 \tLoss: tensor(0.0521, grad_fn=<AddBackward0>)\n",
            "Epoch: 600 \tLoss: tensor(0.0519, grad_fn=<AddBackward0>)\n",
            "Epoch: 601 \tLoss: tensor(0.0521, grad_fn=<AddBackward0>)\n",
            "Epoch: 602 \tLoss: tensor(0.0520, grad_fn=<AddBackward0>)\n",
            "Epoch: 603 \tLoss: tensor(0.0519, grad_fn=<AddBackward0>)\n",
            "Epoch: 604 \tLoss: tensor(0.0518, grad_fn=<AddBackward0>)\n",
            "Epoch: 605 \tLoss: tensor(0.0517, grad_fn=<AddBackward0>)\n",
            "Epoch: 606 \tLoss: tensor(0.0516, grad_fn=<AddBackward0>)\n",
            "Epoch: 607 \tLoss: tensor(0.0515, grad_fn=<AddBackward0>)\n",
            "Epoch: 608 \tLoss: tensor(0.0514, grad_fn=<AddBackward0>)\n",
            "Epoch: 609 \tLoss: tensor(0.0512, grad_fn=<AddBackward0>)\n",
            "Epoch: 610 \tLoss: tensor(0.0512, grad_fn=<AddBackward0>)\n",
            "Epoch: 611 \tLoss: tensor(0.0510, grad_fn=<AddBackward0>)\n",
            "Epoch: 612 \tLoss: tensor(0.0509, grad_fn=<AddBackward0>)\n",
            "Epoch: 613 \tLoss: tensor(0.0509, grad_fn=<AddBackward0>)\n",
            "Epoch: 614 \tLoss: tensor(0.0508, grad_fn=<AddBackward0>)\n",
            "Epoch: 615 \tLoss: tensor(0.0509, grad_fn=<AddBackward0>)\n",
            "Epoch: 616 \tLoss: tensor(0.0507, grad_fn=<AddBackward0>)\n",
            "Epoch: 617 \tLoss: tensor(0.0506, grad_fn=<AddBackward0>)\n",
            "Epoch: 618 \tLoss: tensor(0.0505, grad_fn=<AddBackward0>)\n",
            "Epoch: 619 \tLoss: tensor(0.0504, grad_fn=<AddBackward0>)\n",
            "Epoch: 620 \tLoss: tensor(0.0503, grad_fn=<AddBackward0>)\n",
            "Epoch: 621 \tLoss: tensor(0.0502, grad_fn=<AddBackward0>)\n",
            "Epoch: 622 \tLoss: tensor(0.0502, grad_fn=<AddBackward0>)\n",
            "Epoch: 623 \tLoss: tensor(0.0501, grad_fn=<AddBackward0>)\n",
            "Epoch: 624 \tLoss: tensor(0.0500, grad_fn=<AddBackward0>)\n",
            "Epoch: 625 \tLoss: tensor(0.0499, grad_fn=<AddBackward0>)\n",
            "Epoch: 626 \tLoss: tensor(0.0498, grad_fn=<AddBackward0>)\n",
            "Epoch: 627 \tLoss: tensor(0.0499, grad_fn=<AddBackward0>)\n",
            "Epoch: 628 \tLoss: tensor(0.0498, grad_fn=<AddBackward0>)\n",
            "Epoch: 629 \tLoss: tensor(0.0497, grad_fn=<AddBackward0>)\n",
            "Epoch: 630 \tLoss: tensor(0.0496, grad_fn=<AddBackward0>)\n",
            "Epoch: 631 \tLoss: tensor(0.0495, grad_fn=<AddBackward0>)\n",
            "Epoch: 632 \tLoss: tensor(0.0494, grad_fn=<AddBackward0>)\n",
            "Epoch: 633 \tLoss: tensor(0.0493, grad_fn=<AddBackward0>)\n",
            "Epoch: 634 \tLoss: tensor(0.0492, grad_fn=<AddBackward0>)\n",
            "Epoch: 635 \tLoss: tensor(0.0491, grad_fn=<AddBackward0>)\n",
            "Epoch: 636 \tLoss: tensor(0.0490, grad_fn=<AddBackward0>)\n",
            "Epoch: 637 \tLoss: tensor(0.0489, grad_fn=<AddBackward0>)\n",
            "Epoch: 638 \tLoss: tensor(0.0487, grad_fn=<AddBackward0>)\n",
            "Epoch: 639 \tLoss: tensor(0.0486, grad_fn=<AddBackward0>)\n",
            "Epoch: 640 \tLoss: tensor(0.0485, grad_fn=<AddBackward0>)\n",
            "Epoch: 641 \tLoss: tensor(0.0484, grad_fn=<AddBackward0>)\n",
            "Epoch: 642 \tLoss: tensor(0.0484, grad_fn=<AddBackward0>)\n",
            "Epoch: 643 \tLoss: tensor(0.0483, grad_fn=<AddBackward0>)\n",
            "Epoch: 644 \tLoss: tensor(0.0482, grad_fn=<AddBackward0>)\n",
            "Epoch: 645 \tLoss: tensor(0.0481, grad_fn=<AddBackward0>)\n",
            "Epoch: 646 \tLoss: tensor(0.0481, grad_fn=<AddBackward0>)\n",
            "Epoch: 647 \tLoss: tensor(0.0480, grad_fn=<AddBackward0>)\n",
            "Epoch: 648 \tLoss: tensor(0.0480, grad_fn=<AddBackward0>)\n",
            "Epoch: 649 \tLoss: tensor(0.0478, grad_fn=<AddBackward0>)\n",
            "Epoch: 650 \tLoss: tensor(0.0478, grad_fn=<AddBackward0>)\n",
            "Epoch: 651 \tLoss: tensor(0.0477, grad_fn=<AddBackward0>)\n",
            "Epoch: 652 \tLoss: tensor(0.0476, grad_fn=<AddBackward0>)\n",
            "Epoch: 653 \tLoss: tensor(0.0475, grad_fn=<AddBackward0>)\n",
            "Epoch: 654 \tLoss: tensor(0.0477, grad_fn=<AddBackward0>)\n",
            "Epoch: 655 \tLoss: tensor(0.0476, grad_fn=<AddBackward0>)\n",
            "Epoch: 656 \tLoss: tensor(0.0475, grad_fn=<AddBackward0>)\n",
            "Epoch: 657 \tLoss: tensor(0.0474, grad_fn=<AddBackward0>)\n",
            "Epoch: 658 \tLoss: tensor(0.0473, grad_fn=<AddBackward0>)\n",
            "Epoch: 659 \tLoss: tensor(0.0472, grad_fn=<AddBackward0>)\n",
            "Epoch: 660 \tLoss: tensor(0.0471, grad_fn=<AddBackward0>)\n",
            "Epoch: 661 \tLoss: tensor(0.0470, grad_fn=<AddBackward0>)\n",
            "Epoch: 662 \tLoss: tensor(0.0469, grad_fn=<AddBackward0>)\n",
            "Epoch: 663 \tLoss: tensor(0.0469, grad_fn=<AddBackward0>)\n",
            "Epoch: 664 \tLoss: tensor(0.0468, grad_fn=<AddBackward0>)\n",
            "Epoch: 665 \tLoss: tensor(0.0467, grad_fn=<AddBackward0>)\n",
            "Epoch: 666 \tLoss: tensor(0.0466, grad_fn=<AddBackward0>)\n",
            "Epoch: 667 \tLoss: tensor(0.0466, grad_fn=<AddBackward0>)\n",
            "Epoch: 668 \tLoss: tensor(0.0465, grad_fn=<AddBackward0>)\n",
            "Epoch: 669 \tLoss: tensor(0.0464, grad_fn=<AddBackward0>)\n",
            "Epoch: 670 \tLoss: tensor(0.0463, grad_fn=<AddBackward0>)\n",
            "Epoch: 671 \tLoss: tensor(0.0462, grad_fn=<AddBackward0>)\n",
            "Epoch: 672 \tLoss: tensor(0.0462, grad_fn=<AddBackward0>)\n",
            "Epoch: 673 \tLoss: tensor(0.0461, grad_fn=<AddBackward0>)\n",
            "Epoch: 674 \tLoss: tensor(0.0460, grad_fn=<AddBackward0>)\n",
            "Epoch: 675 \tLoss: tensor(0.0459, grad_fn=<AddBackward0>)\n",
            "Epoch: 676 \tLoss: tensor(0.0459, grad_fn=<AddBackward0>)\n",
            "Epoch: 677 \tLoss: tensor(0.0458, grad_fn=<AddBackward0>)\n",
            "Epoch: 678 \tLoss: tensor(0.0457, grad_fn=<AddBackward0>)\n",
            "Epoch: 679 \tLoss: tensor(0.0457, grad_fn=<AddBackward0>)\n",
            "Epoch: 680 \tLoss: tensor(0.0456, grad_fn=<AddBackward0>)\n",
            "Epoch: 681 \tLoss: tensor(0.0455, grad_fn=<AddBackward0>)\n",
            "Epoch: 682 \tLoss: tensor(0.0455, grad_fn=<AddBackward0>)\n",
            "Epoch: 683 \tLoss: tensor(0.0454, grad_fn=<AddBackward0>)\n",
            "Epoch: 684 \tLoss: tensor(0.0454, grad_fn=<AddBackward0>)\n",
            "Epoch: 685 \tLoss: tensor(0.0454, grad_fn=<AddBackward0>)\n",
            "Epoch: 686 \tLoss: tensor(0.0453, grad_fn=<AddBackward0>)\n",
            "Epoch: 687 \tLoss: tensor(0.0452, grad_fn=<AddBackward0>)\n",
            "Epoch: 688 \tLoss: tensor(0.0451, grad_fn=<AddBackward0>)\n",
            "Epoch: 689 \tLoss: tensor(0.0450, grad_fn=<AddBackward0>)\n",
            "Epoch: 690 \tLoss: tensor(0.0450, grad_fn=<AddBackward0>)\n",
            "Epoch: 691 \tLoss: tensor(0.0449, grad_fn=<AddBackward0>)\n",
            "Epoch: 692 \tLoss: tensor(0.0448, grad_fn=<AddBackward0>)\n",
            "Epoch: 693 \tLoss: tensor(0.0448, grad_fn=<AddBackward0>)\n",
            "Epoch: 694 \tLoss: tensor(0.0447, grad_fn=<AddBackward0>)\n",
            "Epoch: 695 \tLoss: tensor(0.0446, grad_fn=<AddBackward0>)\n",
            "Epoch: 696 \tLoss: tensor(0.0445, grad_fn=<AddBackward0>)\n",
            "Epoch: 697 \tLoss: tensor(0.0445, grad_fn=<AddBackward0>)\n",
            "Epoch: 698 \tLoss: tensor(0.0444, grad_fn=<AddBackward0>)\n",
            "Epoch: 699 \tLoss: tensor(0.0443, grad_fn=<AddBackward0>)\n",
            "Epoch: 700 \tLoss: tensor(0.0442, grad_fn=<AddBackward0>)\n",
            "Epoch: 701 \tLoss: tensor(0.0441, grad_fn=<AddBackward0>)\n",
            "Epoch: 702 \tLoss: tensor(0.0443, grad_fn=<AddBackward0>)\n",
            "Epoch: 703 \tLoss: tensor(0.0442, grad_fn=<AddBackward0>)\n",
            "Epoch: 704 \tLoss: tensor(0.0441, grad_fn=<AddBackward0>)\n",
            "Epoch: 705 \tLoss: tensor(0.0440, grad_fn=<AddBackward0>)\n",
            "Epoch: 706 \tLoss: tensor(0.0439, grad_fn=<AddBackward0>)\n",
            "Epoch: 707 \tLoss: tensor(0.0439, grad_fn=<AddBackward0>)\n",
            "Epoch: 708 \tLoss: tensor(0.0438, grad_fn=<AddBackward0>)\n",
            "Epoch: 709 \tLoss: tensor(0.0437, grad_fn=<AddBackward0>)\n",
            "Epoch: 710 \tLoss: tensor(0.0436, grad_fn=<AddBackward0>)\n",
            "Epoch: 711 \tLoss: tensor(0.0436, grad_fn=<AddBackward0>)\n",
            "Epoch: 712 \tLoss: tensor(0.0435, grad_fn=<AddBackward0>)\n",
            "Epoch: 713 \tLoss: tensor(0.0434, grad_fn=<AddBackward0>)\n",
            "Epoch: 714 \tLoss: tensor(0.0434, grad_fn=<AddBackward0>)\n",
            "Epoch: 715 \tLoss: tensor(0.0435, grad_fn=<AddBackward0>)\n",
            "Epoch: 716 \tLoss: tensor(0.0434, grad_fn=<AddBackward0>)\n",
            "Epoch: 717 \tLoss: tensor(0.0433, grad_fn=<AddBackward0>)\n",
            "Epoch: 718 \tLoss: tensor(0.0432, grad_fn=<AddBackward0>)\n",
            "Epoch: 719 \tLoss: tensor(0.0432, grad_fn=<AddBackward0>)\n",
            "Epoch: 720 \tLoss: tensor(0.0431, grad_fn=<AddBackward0>)\n",
            "Epoch: 721 \tLoss: tensor(0.0431, grad_fn=<AddBackward0>)\n",
            "Epoch: 722 \tLoss: tensor(0.0430, grad_fn=<AddBackward0>)\n",
            "Epoch: 723 \tLoss: tensor(0.0429, grad_fn=<AddBackward0>)\n",
            "Epoch: 724 \tLoss: tensor(0.0428, grad_fn=<AddBackward0>)\n",
            "Epoch: 725 \tLoss: tensor(0.0428, grad_fn=<AddBackward0>)\n",
            "Epoch: 726 \tLoss: tensor(0.0427, grad_fn=<AddBackward0>)\n",
            "Epoch: 727 \tLoss: tensor(0.0426, grad_fn=<AddBackward0>)\n",
            "Epoch: 728 \tLoss: tensor(0.0425, grad_fn=<AddBackward0>)\n",
            "Epoch: 729 \tLoss: tensor(0.0425, grad_fn=<AddBackward0>)\n",
            "Epoch: 730 \tLoss: tensor(0.0424, grad_fn=<AddBackward0>)\n",
            "Epoch: 731 \tLoss: tensor(0.0423, grad_fn=<AddBackward0>)\n",
            "Epoch: 732 \tLoss: tensor(0.0422, grad_fn=<AddBackward0>)\n",
            "Epoch: 733 \tLoss: tensor(0.0422, grad_fn=<AddBackward0>)\n",
            "Epoch: 734 \tLoss: tensor(0.0421, grad_fn=<AddBackward0>)\n",
            "Epoch: 735 \tLoss: tensor(0.0421, grad_fn=<AddBackward0>)\n",
            "Epoch: 736 \tLoss: tensor(0.0420, grad_fn=<AddBackward0>)\n",
            "Epoch: 737 \tLoss: tensor(0.0419, grad_fn=<AddBackward0>)\n",
            "Epoch: 738 \tLoss: tensor(0.0420, grad_fn=<AddBackward0>)\n",
            "Epoch: 739 \tLoss: tensor(0.0419, grad_fn=<AddBackward0>)\n",
            "Epoch: 740 \tLoss: tensor(0.0418, grad_fn=<AddBackward0>)\n",
            "Epoch: 741 \tLoss: tensor(0.0418, grad_fn=<AddBackward0>)\n",
            "Epoch: 742 \tLoss: tensor(0.0417, grad_fn=<AddBackward0>)\n",
            "Epoch: 743 \tLoss: tensor(0.0416, grad_fn=<AddBackward0>)\n",
            "Epoch: 744 \tLoss: tensor(0.0415, grad_fn=<AddBackward0>)\n",
            "Epoch: 745 \tLoss: tensor(0.0415, grad_fn=<AddBackward0>)\n",
            "Epoch: 746 \tLoss: tensor(0.0414, grad_fn=<AddBackward0>)\n",
            "Epoch: 747 \tLoss: tensor(0.0413, grad_fn=<AddBackward0>)\n",
            "Epoch: 748 \tLoss: tensor(0.0415, grad_fn=<AddBackward0>)\n",
            "Epoch: 749 \tLoss: tensor(0.0414, grad_fn=<AddBackward0>)\n",
            "Epoch: 750 \tLoss: tensor(0.0413, grad_fn=<AddBackward0>)\n",
            "Epoch: 751 \tLoss: tensor(0.0412, grad_fn=<AddBackward0>)\n",
            "Epoch: 752 \tLoss: tensor(0.0412, grad_fn=<AddBackward0>)\n",
            "Epoch: 753 \tLoss: tensor(0.0411, grad_fn=<AddBackward0>)\n",
            "Epoch: 754 \tLoss: tensor(0.0410, grad_fn=<AddBackward0>)\n",
            "Epoch: 755 \tLoss: tensor(0.0409, grad_fn=<AddBackward0>)\n",
            "Epoch: 756 \tLoss: tensor(0.0409, grad_fn=<AddBackward0>)\n",
            "Epoch: 757 \tLoss: tensor(0.0408, grad_fn=<AddBackward0>)\n",
            "Epoch: 758 \tLoss: tensor(0.0408, grad_fn=<AddBackward0>)\n",
            "Epoch: 759 \tLoss: tensor(0.0407, grad_fn=<AddBackward0>)\n",
            "Epoch: 760 \tLoss: tensor(0.0406, grad_fn=<AddBackward0>)\n",
            "Epoch: 761 \tLoss: tensor(0.0406, grad_fn=<AddBackward0>)\n",
            "Epoch: 762 \tLoss: tensor(0.0405, grad_fn=<AddBackward0>)\n",
            "Epoch: 763 \tLoss: tensor(0.0404, grad_fn=<AddBackward0>)\n",
            "Epoch: 764 \tLoss: tensor(0.0404, grad_fn=<AddBackward0>)\n",
            "Epoch: 765 \tLoss: tensor(0.0403, grad_fn=<AddBackward0>)\n",
            "Epoch: 766 \tLoss: tensor(0.0403, grad_fn=<AddBackward0>)\n",
            "Epoch: 767 \tLoss: tensor(0.0402, grad_fn=<AddBackward0>)\n",
            "Epoch: 768 \tLoss: tensor(0.0402, grad_fn=<AddBackward0>)\n",
            "Epoch: 769 \tLoss: tensor(0.0401, grad_fn=<AddBackward0>)\n",
            "Epoch: 770 \tLoss: tensor(0.0401, grad_fn=<AddBackward0>)\n",
            "Epoch: 771 \tLoss: tensor(0.0400, grad_fn=<AddBackward0>)\n",
            "Epoch: 772 \tLoss: tensor(0.0400, grad_fn=<AddBackward0>)\n",
            "Epoch: 773 \tLoss: tensor(0.0399, grad_fn=<AddBackward0>)\n",
            "Epoch: 774 \tLoss: tensor(0.0398, grad_fn=<AddBackward0>)\n",
            "Epoch: 775 \tLoss: tensor(0.0395, grad_fn=<AddBackward0>)\n",
            "Epoch: 776 \tLoss: tensor(0.0395, grad_fn=<AddBackward0>)\n",
            "Epoch: 777 \tLoss: tensor(0.0394, grad_fn=<AddBackward0>)\n",
            "Epoch: 778 \tLoss: tensor(0.0394, grad_fn=<AddBackward0>)\n",
            "Epoch: 779 \tLoss: tensor(0.0394, grad_fn=<AddBackward0>)\n",
            "Epoch: 780 \tLoss: tensor(0.0393, grad_fn=<AddBackward0>)\n",
            "Epoch: 781 \tLoss: tensor(0.0393, grad_fn=<AddBackward0>)\n",
            "Epoch: 782 \tLoss: tensor(0.0392, grad_fn=<AddBackward0>)\n",
            "Epoch: 783 \tLoss: tensor(0.0392, grad_fn=<AddBackward0>)\n",
            "Epoch: 784 \tLoss: tensor(0.0393, grad_fn=<AddBackward0>)\n",
            "Epoch: 785 \tLoss: tensor(0.0393, grad_fn=<AddBackward0>)\n",
            "Epoch: 786 \tLoss: tensor(0.0393, grad_fn=<AddBackward0>)\n",
            "Epoch: 787 \tLoss: tensor(0.0392, grad_fn=<AddBackward0>)\n",
            "Epoch: 788 \tLoss: tensor(0.0391, grad_fn=<AddBackward0>)\n",
            "Epoch: 789 \tLoss: tensor(0.0391, grad_fn=<AddBackward0>)\n",
            "Epoch: 790 \tLoss: tensor(0.0390, grad_fn=<AddBackward0>)\n",
            "Epoch: 791 \tLoss: tensor(0.0390, grad_fn=<AddBackward0>)\n",
            "Epoch: 792 \tLoss: tensor(0.0389, grad_fn=<AddBackward0>)\n",
            "Epoch: 793 \tLoss: tensor(0.0389, grad_fn=<AddBackward0>)\n",
            "Epoch: 794 \tLoss: tensor(0.0388, grad_fn=<AddBackward0>)\n",
            "Epoch: 795 \tLoss: tensor(0.0388, grad_fn=<AddBackward0>)\n",
            "Epoch: 796 \tLoss: tensor(0.0387, grad_fn=<AddBackward0>)\n",
            "Epoch: 797 \tLoss: tensor(0.0387, grad_fn=<AddBackward0>)\n",
            "Epoch: 798 \tLoss: tensor(0.0386, grad_fn=<AddBackward0>)\n",
            "Epoch: 799 \tLoss: tensor(0.0387, grad_fn=<AddBackward0>)\n",
            "Epoch: 800 \tLoss: tensor(0.0387, grad_fn=<AddBackward0>)\n",
            "Epoch: 801 \tLoss: tensor(0.0386, grad_fn=<AddBackward0>)\n",
            "Epoch: 802 \tLoss: tensor(0.0385, grad_fn=<AddBackward0>)\n",
            "Epoch: 803 \tLoss: tensor(0.0385, grad_fn=<AddBackward0>)\n",
            "Epoch: 804 \tLoss: tensor(0.0384, grad_fn=<AddBackward0>)\n",
            "Epoch: 805 \tLoss: tensor(0.0383, grad_fn=<AddBackward0>)\n",
            "Epoch: 806 \tLoss: tensor(0.0383, grad_fn=<AddBackward0>)\n",
            "Epoch: 807 \tLoss: tensor(0.0382, grad_fn=<AddBackward0>)\n",
            "Epoch: 808 \tLoss: tensor(0.0382, grad_fn=<AddBackward0>)\n",
            "Epoch: 809 \tLoss: tensor(0.0381, grad_fn=<AddBackward0>)\n",
            "Epoch: 810 \tLoss: tensor(0.0381, grad_fn=<AddBackward0>)\n",
            "Epoch: 811 \tLoss: tensor(0.0381, grad_fn=<AddBackward0>)\n",
            "Epoch: 812 \tLoss: tensor(0.0380, grad_fn=<AddBackward0>)\n",
            "Epoch: 813 \tLoss: tensor(0.0380, grad_fn=<AddBackward0>)\n",
            "Epoch: 814 \tLoss: tensor(0.0379, grad_fn=<AddBackward0>)\n",
            "Epoch: 815 \tLoss: tensor(0.0379, grad_fn=<AddBackward0>)\n",
            "Epoch: 816 \tLoss: tensor(0.0378, grad_fn=<AddBackward0>)\n",
            "Epoch: 817 \tLoss: tensor(0.0378, grad_fn=<AddBackward0>)\n",
            "Epoch: 818 \tLoss: tensor(0.0377, grad_fn=<AddBackward0>)\n",
            "Epoch: 819 \tLoss: tensor(0.0377, grad_fn=<AddBackward0>)\n",
            "Epoch: 820 \tLoss: tensor(0.0376, grad_fn=<AddBackward0>)\n",
            "Epoch: 821 \tLoss: tensor(0.0376, grad_fn=<AddBackward0>)\n",
            "Epoch: 822 \tLoss: tensor(0.0377, grad_fn=<AddBackward0>)\n",
            "Epoch: 823 \tLoss: tensor(0.0376, grad_fn=<AddBackward0>)\n",
            "Epoch: 824 \tLoss: tensor(0.0376, grad_fn=<AddBackward0>)\n",
            "Epoch: 825 \tLoss: tensor(0.0375, grad_fn=<AddBackward0>)\n",
            "Epoch: 826 \tLoss: tensor(0.0375, grad_fn=<AddBackward0>)\n",
            "Epoch: 827 \tLoss: tensor(0.0375, grad_fn=<AddBackward0>)\n",
            "Epoch: 828 \tLoss: tensor(0.0374, grad_fn=<AddBackward0>)\n",
            "Epoch: 829 \tLoss: tensor(0.0373, grad_fn=<AddBackward0>)\n",
            "Epoch: 830 \tLoss: tensor(0.0373, grad_fn=<AddBackward0>)\n",
            "Epoch: 831 \tLoss: tensor(0.0372, grad_fn=<AddBackward0>)\n",
            "Epoch: 832 \tLoss: tensor(0.0372, grad_fn=<AddBackward0>)\n",
            "Epoch: 833 \tLoss: tensor(0.0371, grad_fn=<AddBackward0>)\n",
            "Epoch: 834 \tLoss: tensor(0.0371, grad_fn=<AddBackward0>)\n",
            "Epoch: 835 \tLoss: tensor(0.0370, grad_fn=<AddBackward0>)\n",
            "Epoch: 836 \tLoss: tensor(0.0370, grad_fn=<AddBackward0>)\n",
            "Epoch: 837 \tLoss: tensor(0.0369, grad_fn=<AddBackward0>)\n",
            "Epoch: 838 \tLoss: tensor(0.0369, grad_fn=<AddBackward0>)\n",
            "Epoch: 839 \tLoss: tensor(0.0369, grad_fn=<AddBackward0>)\n",
            "Epoch: 840 \tLoss: tensor(0.0368, grad_fn=<AddBackward0>)\n",
            "Epoch: 841 \tLoss: tensor(0.0368, grad_fn=<AddBackward0>)\n",
            "Epoch: 842 \tLoss: tensor(0.0367, grad_fn=<AddBackward0>)\n",
            "Epoch: 843 \tLoss: tensor(0.0367, grad_fn=<AddBackward0>)\n",
            "Epoch: 844 \tLoss: tensor(0.0366, grad_fn=<AddBackward0>)\n",
            "Epoch: 845 \tLoss: tensor(0.0366, grad_fn=<AddBackward0>)\n",
            "Epoch: 846 \tLoss: tensor(0.0365, grad_fn=<AddBackward0>)\n",
            "Epoch: 847 \tLoss: tensor(0.0365, grad_fn=<AddBackward0>)\n",
            "Epoch: 848 \tLoss: tensor(0.0365, grad_fn=<AddBackward0>)\n",
            "Epoch: 849 \tLoss: tensor(0.0364, grad_fn=<AddBackward0>)\n",
            "Epoch: 850 \tLoss: tensor(0.0364, grad_fn=<AddBackward0>)\n",
            "Epoch: 851 \tLoss: tensor(0.0363, grad_fn=<AddBackward0>)\n",
            "Epoch: 852 \tLoss: tensor(0.0363, grad_fn=<AddBackward0>)\n",
            "Epoch: 853 \tLoss: tensor(0.0362, grad_fn=<AddBackward0>)\n",
            "Epoch: 854 \tLoss: tensor(0.0362, grad_fn=<AddBackward0>)\n",
            "Epoch: 855 \tLoss: tensor(0.0361, grad_fn=<AddBackward0>)\n",
            "Epoch: 856 \tLoss: tensor(0.0361, grad_fn=<AddBackward0>)\n",
            "Epoch: 857 \tLoss: tensor(0.0361, grad_fn=<AddBackward0>)\n",
            "Epoch: 858 \tLoss: tensor(0.0360, grad_fn=<AddBackward0>)\n",
            "Epoch: 859 \tLoss: tensor(0.0360, grad_fn=<AddBackward0>)\n",
            "Epoch: 860 \tLoss: tensor(0.0359, grad_fn=<AddBackward0>)\n",
            "Epoch: 861 \tLoss: tensor(0.0360, grad_fn=<AddBackward0>)\n",
            "Epoch: 862 \tLoss: tensor(0.0360, grad_fn=<AddBackward0>)\n",
            "Epoch: 863 \tLoss: tensor(0.0360, grad_fn=<AddBackward0>)\n",
            "Epoch: 864 \tLoss: tensor(0.0359, grad_fn=<AddBackward0>)\n",
            "Epoch: 865 \tLoss: tensor(0.0358, grad_fn=<AddBackward0>)\n",
            "Epoch: 866 \tLoss: tensor(0.0358, grad_fn=<AddBackward0>)\n",
            "Epoch: 867 \tLoss: tensor(0.0358, grad_fn=<AddBackward0>)\n",
            "Epoch: 868 \tLoss: tensor(0.0357, grad_fn=<AddBackward0>)\n",
            "Epoch: 869 \tLoss: tensor(0.0357, grad_fn=<AddBackward0>)\n",
            "Epoch: 870 \tLoss: tensor(0.0357, grad_fn=<AddBackward0>)\n",
            "Epoch: 871 \tLoss: tensor(0.0356, grad_fn=<AddBackward0>)\n",
            "Epoch: 872 \tLoss: tensor(0.0355, grad_fn=<AddBackward0>)\n",
            "Epoch: 873 \tLoss: tensor(0.0355, grad_fn=<AddBackward0>)\n",
            "Epoch: 874 \tLoss: tensor(0.0354, grad_fn=<AddBackward0>)\n",
            "Epoch: 875 \tLoss: tensor(0.0354, grad_fn=<AddBackward0>)\n",
            "Epoch: 876 \tLoss: tensor(0.0353, grad_fn=<AddBackward0>)\n",
            "Epoch: 877 \tLoss: tensor(0.0353, grad_fn=<AddBackward0>)\n",
            "Epoch: 878 \tLoss: tensor(0.0353, grad_fn=<AddBackward0>)\n",
            "Epoch: 879 \tLoss: tensor(0.0352, grad_fn=<AddBackward0>)\n",
            "Epoch: 880 \tLoss: tensor(0.0352, grad_fn=<AddBackward0>)\n",
            "Epoch: 881 \tLoss: tensor(0.0351, grad_fn=<AddBackward0>)\n",
            "Epoch: 882 \tLoss: tensor(0.0351, grad_fn=<AddBackward0>)\n",
            "Epoch: 883 \tLoss: tensor(0.0351, grad_fn=<AddBackward0>)\n",
            "Epoch: 884 \tLoss: tensor(0.0350, grad_fn=<AddBackward0>)\n",
            "Epoch: 885 \tLoss: tensor(0.0350, grad_fn=<AddBackward0>)\n",
            "Epoch: 886 \tLoss: tensor(0.0349, grad_fn=<AddBackward0>)\n",
            "Epoch: 887 \tLoss: tensor(0.0349, grad_fn=<AddBackward0>)\n",
            "Epoch: 888 \tLoss: tensor(0.0349, grad_fn=<AddBackward0>)\n",
            "Epoch: 889 \tLoss: tensor(0.0348, grad_fn=<AddBackward0>)\n",
            "Epoch: 890 \tLoss: tensor(0.0348, grad_fn=<AddBackward0>)\n",
            "Epoch: 891 \tLoss: tensor(0.0347, grad_fn=<AddBackward0>)\n",
            "Epoch: 892 \tLoss: tensor(0.0347, grad_fn=<AddBackward0>)\n",
            "Epoch: 893 \tLoss: tensor(0.0347, grad_fn=<AddBackward0>)\n",
            "Epoch: 894 \tLoss: tensor(0.0346, grad_fn=<AddBackward0>)\n",
            "Epoch: 895 \tLoss: tensor(0.0346, grad_fn=<AddBackward0>)\n",
            "Epoch: 896 \tLoss: tensor(0.0345, grad_fn=<AddBackward0>)\n",
            "Epoch: 897 \tLoss: tensor(0.0345, grad_fn=<AddBackward0>)\n",
            "Epoch: 898 \tLoss: tensor(0.0344, grad_fn=<AddBackward0>)\n",
            "Epoch: 899 \tLoss: tensor(0.0344, grad_fn=<AddBackward0>)\n",
            "Epoch: 900 \tLoss: tensor(0.0343, grad_fn=<AddBackward0>)\n",
            "Epoch: 901 \tLoss: tensor(0.0343, grad_fn=<AddBackward0>)\n",
            "Epoch: 902 \tLoss: tensor(0.0344, grad_fn=<AddBackward0>)\n",
            "Epoch: 903 \tLoss: tensor(0.0343, grad_fn=<AddBackward0>)\n",
            "Epoch: 904 \tLoss: tensor(0.0344, grad_fn=<AddBackward0>)\n",
            "Epoch: 905 \tLoss: tensor(0.0344, grad_fn=<AddBackward0>)\n",
            "Epoch: 906 \tLoss: tensor(0.0344, grad_fn=<AddBackward0>)\n",
            "Epoch: 907 \tLoss: tensor(0.0343, grad_fn=<AddBackward0>)\n",
            "Epoch: 908 \tLoss: tensor(0.0342, grad_fn=<AddBackward0>)\n",
            "Epoch: 909 \tLoss: tensor(0.0342, grad_fn=<AddBackward0>)\n",
            "Epoch: 910 \tLoss: tensor(0.0342, grad_fn=<AddBackward0>)\n",
            "Epoch: 911 \tLoss: tensor(0.0341, grad_fn=<AddBackward0>)\n",
            "Epoch: 912 \tLoss: tensor(0.0341, grad_fn=<AddBackward0>)\n",
            "Epoch: 913 \tLoss: tensor(0.0341, grad_fn=<AddBackward0>)\n",
            "Epoch: 914 \tLoss: tensor(0.0340, grad_fn=<AddBackward0>)\n",
            "Epoch: 915 \tLoss: tensor(0.0339, grad_fn=<AddBackward0>)\n",
            "Epoch: 916 \tLoss: tensor(0.0339, grad_fn=<AddBackward0>)\n",
            "Epoch: 917 \tLoss: tensor(0.0339, grad_fn=<AddBackward0>)\n",
            "Epoch: 918 \tLoss: tensor(0.0339, grad_fn=<AddBackward0>)\n",
            "Epoch: 919 \tLoss: tensor(0.0338, grad_fn=<AddBackward0>)\n",
            "Epoch: 920 \tLoss: tensor(0.0338, grad_fn=<AddBackward0>)\n",
            "Epoch: 921 \tLoss: tensor(0.0337, grad_fn=<AddBackward0>)\n",
            "Epoch: 922 \tLoss: tensor(0.0337, grad_fn=<AddBackward0>)\n",
            "Epoch: 923 \tLoss: tensor(0.0336, grad_fn=<AddBackward0>)\n",
            "Epoch: 924 \tLoss: tensor(0.0336, grad_fn=<AddBackward0>)\n",
            "Epoch: 925 \tLoss: tensor(0.0336, grad_fn=<AddBackward0>)\n",
            "Epoch: 926 \tLoss: tensor(0.0335, grad_fn=<AddBackward0>)\n",
            "Epoch: 927 \tLoss: tensor(0.0335, grad_fn=<AddBackward0>)\n",
            "Epoch: 928 \tLoss: tensor(0.0334, grad_fn=<AddBackward0>)\n",
            "Epoch: 929 \tLoss: tensor(0.0334, grad_fn=<AddBackward0>)\n",
            "Epoch: 930 \tLoss: tensor(0.0334, grad_fn=<AddBackward0>)\n",
            "Epoch: 931 \tLoss: tensor(0.0333, grad_fn=<AddBackward0>)\n",
            "Epoch: 932 \tLoss: tensor(0.0333, grad_fn=<AddBackward0>)\n",
            "Epoch: 933 \tLoss: tensor(0.0332, grad_fn=<AddBackward0>)\n",
            "Epoch: 934 \tLoss: tensor(0.0332, grad_fn=<AddBackward0>)\n",
            "Epoch: 935 \tLoss: tensor(0.0332, grad_fn=<AddBackward0>)\n",
            "Epoch: 936 \tLoss: tensor(0.0331, grad_fn=<AddBackward0>)\n",
            "Epoch: 937 \tLoss: tensor(0.0331, grad_fn=<AddBackward0>)\n",
            "Epoch: 938 \tLoss: tensor(0.0331, grad_fn=<AddBackward0>)\n",
            "Epoch: 939 \tLoss: tensor(0.0330, grad_fn=<AddBackward0>)\n",
            "Epoch: 940 \tLoss: tensor(0.0330, grad_fn=<AddBackward0>)\n",
            "Epoch: 941 \tLoss: tensor(0.0330, grad_fn=<AddBackward0>)\n",
            "Epoch: 942 \tLoss: tensor(0.0329, grad_fn=<AddBackward0>)\n",
            "Epoch: 943 \tLoss: tensor(0.0329, grad_fn=<AddBackward0>)\n",
            "Epoch: 944 \tLoss: tensor(0.0329, grad_fn=<AddBackward0>)\n",
            "Epoch: 945 \tLoss: tensor(0.0328, grad_fn=<AddBackward0>)\n",
            "Epoch: 946 \tLoss: tensor(0.0328, grad_fn=<AddBackward0>)\n",
            "Epoch: 947 \tLoss: tensor(0.0328, grad_fn=<AddBackward0>)\n",
            "Epoch: 948 \tLoss: tensor(0.0327, grad_fn=<AddBackward0>)\n",
            "Epoch: 949 \tLoss: tensor(0.0327, grad_fn=<AddBackward0>)\n",
            "Epoch: 950 \tLoss: tensor(0.0328, grad_fn=<AddBackward0>)\n",
            "Epoch: 951 \tLoss: tensor(0.0328, grad_fn=<AddBackward0>)\n",
            "Epoch: 952 \tLoss: tensor(0.0328, grad_fn=<AddBackward0>)\n",
            "Epoch: 953 \tLoss: tensor(0.0327, grad_fn=<AddBackward0>)\n",
            "Epoch: 954 \tLoss: tensor(0.0327, grad_fn=<AddBackward0>)\n",
            "Epoch: 955 \tLoss: tensor(0.0326, grad_fn=<AddBackward0>)\n",
            "Epoch: 956 \tLoss: tensor(0.0325, grad_fn=<AddBackward0>)\n",
            "Epoch: 957 \tLoss: tensor(0.0325, grad_fn=<AddBackward0>)\n",
            "Epoch: 958 \tLoss: tensor(0.0325, grad_fn=<AddBackward0>)\n",
            "Epoch: 959 \tLoss: tensor(0.0324, grad_fn=<AddBackward0>)\n",
            "Epoch: 960 \tLoss: tensor(0.0324, grad_fn=<AddBackward0>)\n",
            "Epoch: 961 \tLoss: tensor(0.0323, grad_fn=<AddBackward0>)\n",
            "Epoch: 962 \tLoss: tensor(0.0323, grad_fn=<AddBackward0>)\n",
            "Epoch: 963 \tLoss: tensor(0.0323, grad_fn=<AddBackward0>)\n",
            "Epoch: 964 \tLoss: tensor(0.0322, grad_fn=<AddBackward0>)\n",
            "Epoch: 965 \tLoss: tensor(0.0322, grad_fn=<AddBackward0>)\n",
            "Epoch: 966 \tLoss: tensor(0.0321, grad_fn=<AddBackward0>)\n",
            "Epoch: 967 \tLoss: tensor(0.0321, grad_fn=<AddBackward0>)\n",
            "Epoch: 968 \tLoss: tensor(0.0321, grad_fn=<AddBackward0>)\n",
            "Epoch: 969 \tLoss: tensor(0.0320, grad_fn=<AddBackward0>)\n",
            "Epoch: 970 \tLoss: tensor(0.0320, grad_fn=<AddBackward0>)\n",
            "Epoch: 971 \tLoss: tensor(0.0320, grad_fn=<AddBackward0>)\n",
            "Epoch: 972 \tLoss: tensor(0.0319, grad_fn=<AddBackward0>)\n",
            "Epoch: 973 \tLoss: tensor(0.0318, grad_fn=<AddBackward0>)\n",
            "Epoch: 974 \tLoss: tensor(0.0318, grad_fn=<AddBackward0>)\n",
            "Epoch: 975 \tLoss: tensor(0.0318, grad_fn=<AddBackward0>)\n",
            "Epoch: 976 \tLoss: tensor(0.0317, grad_fn=<AddBackward0>)\n",
            "Epoch: 977 \tLoss: tensor(0.0317, grad_fn=<AddBackward0>)\n",
            "Epoch: 978 \tLoss: tensor(0.0317, grad_fn=<AddBackward0>)\n",
            "Epoch: 979 \tLoss: tensor(0.0316, grad_fn=<AddBackward0>)\n",
            "Epoch: 980 \tLoss: tensor(0.0316, grad_fn=<AddBackward0>)\n",
            "Epoch: 981 \tLoss: tensor(0.0316, grad_fn=<AddBackward0>)\n",
            "Epoch: 982 \tLoss: tensor(0.0315, grad_fn=<AddBackward0>)\n",
            "Epoch: 983 \tLoss: tensor(0.0315, grad_fn=<AddBackward0>)\n",
            "Epoch: 984 \tLoss: tensor(0.0315, grad_fn=<AddBackward0>)\n",
            "Epoch: 985 \tLoss: tensor(0.0314, grad_fn=<AddBackward0>)\n",
            "Epoch: 986 \tLoss: tensor(0.0314, grad_fn=<AddBackward0>)\n",
            "Epoch: 987 \tLoss: tensor(0.0314, grad_fn=<AddBackward0>)\n",
            "Epoch: 988 \tLoss: tensor(0.0313, grad_fn=<AddBackward0>)\n",
            "Epoch: 989 \tLoss: tensor(0.0313, grad_fn=<AddBackward0>)\n",
            "Epoch: 990 \tLoss: tensor(0.0313, grad_fn=<AddBackward0>)\n",
            "Epoch: 991 \tLoss: tensor(0.0312, grad_fn=<AddBackward0>)\n",
            "Epoch: 992 \tLoss: tensor(0.0312, grad_fn=<AddBackward0>)\n",
            "Epoch: 993 \tLoss: tensor(0.0312, grad_fn=<AddBackward0>)\n",
            "Epoch: 994 \tLoss: tensor(0.0311, grad_fn=<AddBackward0>)\n",
            "Epoch: 995 \tLoss: tensor(0.0311, grad_fn=<AddBackward0>)\n",
            "Epoch: 996 \tLoss: tensor(0.0311, grad_fn=<AddBackward0>)\n",
            "Epoch: 997 \tLoss: tensor(0.0311, grad_fn=<AddBackward0>)\n",
            "Epoch: 998 \tLoss: tensor(0.0310, grad_fn=<AddBackward0>)\n",
            "Epoch: 999 \tLoss: tensor(0.0310, grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDeourI1pfuw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "outputId": "e8b4ae0d-58a1-42fc-f25c-9d2dc66311aa"
      },
      "source": [
        "weights = model.embeddings(torch.Tensor([list(range(0,vocab_size))]).long())\n",
        "\n",
        "pd.DataFrame(weights.view(-1,100).tolist(), index=list(id2word.values())[0:]).head(10)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>the</th>\n",
              "      <td>-1.020247</td>\n",
              "      <td>0.858016</td>\n",
              "      <td>-0.972954</td>\n",
              "      <td>1.621598</td>\n",
              "      <td>0.015694</td>\n",
              "      <td>0.071058</td>\n",
              "      <td>-0.546601</td>\n",
              "      <td>0.627173</td>\n",
              "      <td>0.070100</td>\n",
              "      <td>0.540836</td>\n",
              "      <td>-1.658709</td>\n",
              "      <td>-0.710162</td>\n",
              "      <td>0.553695</td>\n",
              "      <td>0.681404</td>\n",
              "      <td>0.939940</td>\n",
              "      <td>1.242287</td>\n",
              "      <td>0.095183</td>\n",
              "      <td>-1.436292</td>\n",
              "      <td>0.910662</td>\n",
              "      <td>0.286473</td>\n",
              "      <td>0.412264</td>\n",
              "      <td>0.236389</td>\n",
              "      <td>0.637720</td>\n",
              "      <td>-1.741466</td>\n",
              "      <td>-1.664177</td>\n",
              "      <td>0.194743</td>\n",
              "      <td>0.228008</td>\n",
              "      <td>-0.640350</td>\n",
              "      <td>0.526864</td>\n",
              "      <td>0.281240</td>\n",
              "      <td>2.378866</td>\n",
              "      <td>1.220054</td>\n",
              "      <td>-2.521966</td>\n",
              "      <td>0.165114</td>\n",
              "      <td>-0.357386</td>\n",
              "      <td>0.639706</td>\n",
              "      <td>0.945562</td>\n",
              "      <td>-2.888935</td>\n",
              "      <td>1.225117</td>\n",
              "      <td>0.340856</td>\n",
              "      <td>...</td>\n",
              "      <td>1.069930</td>\n",
              "      <td>0.774617</td>\n",
              "      <td>-0.063797</td>\n",
              "      <td>0.463545</td>\n",
              "      <td>1.626877</td>\n",
              "      <td>1.043870</td>\n",
              "      <td>-0.687107</td>\n",
              "      <td>-1.104190</td>\n",
              "      <td>1.499260</td>\n",
              "      <td>0.428856</td>\n",
              "      <td>-0.612172</td>\n",
              "      <td>-0.791734</td>\n",
              "      <td>1.190712</td>\n",
              "      <td>0.077100</td>\n",
              "      <td>0.949545</td>\n",
              "      <td>-0.929260</td>\n",
              "      <td>0.752076</td>\n",
              "      <td>-0.108310</td>\n",
              "      <td>0.605397</td>\n",
              "      <td>0.636176</td>\n",
              "      <td>-0.630588</td>\n",
              "      <td>-0.763144</td>\n",
              "      <td>-0.626585</td>\n",
              "      <td>1.092433</td>\n",
              "      <td>-0.112572</td>\n",
              "      <td>-0.522684</td>\n",
              "      <td>0.018455</td>\n",
              "      <td>0.125532</td>\n",
              "      <td>-0.902343</td>\n",
              "      <td>1.652762</td>\n",
              "      <td>1.157533</td>\n",
              "      <td>1.456480</td>\n",
              "      <td>1.167684</td>\n",
              "      <td>0.679525</td>\n",
              "      <td>0.390032</td>\n",
              "      <td>0.883022</td>\n",
              "      <td>1.013172</td>\n",
              "      <td>0.127352</td>\n",
              "      <td>-0.370458</td>\n",
              "      <td>0.407416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>is</th>\n",
              "      <td>-0.469278</td>\n",
              "      <td>0.731419</td>\n",
              "      <td>-2.046814</td>\n",
              "      <td>1.107630</td>\n",
              "      <td>-0.602588</td>\n",
              "      <td>0.407658</td>\n",
              "      <td>-0.544868</td>\n",
              "      <td>-1.117358</td>\n",
              "      <td>-2.473301</td>\n",
              "      <td>0.079160</td>\n",
              "      <td>0.954797</td>\n",
              "      <td>-1.084227</td>\n",
              "      <td>-0.515376</td>\n",
              "      <td>-0.235780</td>\n",
              "      <td>-0.376261</td>\n",
              "      <td>-0.193844</td>\n",
              "      <td>-0.043524</td>\n",
              "      <td>0.727588</td>\n",
              "      <td>0.505118</td>\n",
              "      <td>1.006242</td>\n",
              "      <td>-0.607311</td>\n",
              "      <td>-0.436902</td>\n",
              "      <td>1.811525</td>\n",
              "      <td>-0.897329</td>\n",
              "      <td>-0.348212</td>\n",
              "      <td>0.015865</td>\n",
              "      <td>1.939071</td>\n",
              "      <td>0.395484</td>\n",
              "      <td>1.446937</td>\n",
              "      <td>-0.788801</td>\n",
              "      <td>0.875630</td>\n",
              "      <td>0.702702</td>\n",
              "      <td>-1.203162</td>\n",
              "      <td>0.620883</td>\n",
              "      <td>-2.845220</td>\n",
              "      <td>2.736629</td>\n",
              "      <td>-0.391041</td>\n",
              "      <td>-0.136500</td>\n",
              "      <td>0.472429</td>\n",
              "      <td>-1.558961</td>\n",
              "      <td>...</td>\n",
              "      <td>2.048724</td>\n",
              "      <td>-0.249179</td>\n",
              "      <td>-1.945006</td>\n",
              "      <td>0.344417</td>\n",
              "      <td>0.122567</td>\n",
              "      <td>0.743959</td>\n",
              "      <td>-0.539150</td>\n",
              "      <td>0.288773</td>\n",
              "      <td>0.198540</td>\n",
              "      <td>-0.402431</td>\n",
              "      <td>0.996831</td>\n",
              "      <td>-0.342212</td>\n",
              "      <td>1.165547</td>\n",
              "      <td>-0.513374</td>\n",
              "      <td>-0.188044</td>\n",
              "      <td>-0.739049</td>\n",
              "      <td>1.485368</td>\n",
              "      <td>-0.371456</td>\n",
              "      <td>0.615853</td>\n",
              "      <td>0.180818</td>\n",
              "      <td>-1.017328</td>\n",
              "      <td>-0.179646</td>\n",
              "      <td>-0.637363</td>\n",
              "      <td>0.252615</td>\n",
              "      <td>-0.068570</td>\n",
              "      <td>-0.491677</td>\n",
              "      <td>0.869759</td>\n",
              "      <td>0.058729</td>\n",
              "      <td>0.296049</td>\n",
              "      <td>0.852820</td>\n",
              "      <td>-0.529710</td>\n",
              "      <td>-0.124254</td>\n",
              "      <td>0.493743</td>\n",
              "      <td>0.997346</td>\n",
              "      <td>0.847354</td>\n",
              "      <td>1.380898</td>\n",
              "      <td>1.105859</td>\n",
              "      <td>0.468410</td>\n",
              "      <td>-1.300014</td>\n",
              "      <td>0.211676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>and</th>\n",
              "      <td>1.303654</td>\n",
              "      <td>-0.400301</td>\n",
              "      <td>0.474038</td>\n",
              "      <td>1.324667</td>\n",
              "      <td>-2.266770</td>\n",
              "      <td>0.128912</td>\n",
              "      <td>-0.284075</td>\n",
              "      <td>-0.957212</td>\n",
              "      <td>-1.241336</td>\n",
              "      <td>0.697034</td>\n",
              "      <td>0.906708</td>\n",
              "      <td>-0.607447</td>\n",
              "      <td>-0.222340</td>\n",
              "      <td>-2.749931</td>\n",
              "      <td>0.071322</td>\n",
              "      <td>-0.807236</td>\n",
              "      <td>0.392371</td>\n",
              "      <td>-0.240701</td>\n",
              "      <td>0.277111</td>\n",
              "      <td>-0.416700</td>\n",
              "      <td>-0.802208</td>\n",
              "      <td>1.838970</td>\n",
              "      <td>0.603281</td>\n",
              "      <td>-0.184513</td>\n",
              "      <td>0.061163</td>\n",
              "      <td>1.311520</td>\n",
              "      <td>1.531312</td>\n",
              "      <td>-1.966784</td>\n",
              "      <td>0.910535</td>\n",
              "      <td>0.025413</td>\n",
              "      <td>-2.657902</td>\n",
              "      <td>-0.771684</td>\n",
              "      <td>0.841913</td>\n",
              "      <td>1.475178</td>\n",
              "      <td>0.801869</td>\n",
              "      <td>-0.333531</td>\n",
              "      <td>1.314127</td>\n",
              "      <td>0.111316</td>\n",
              "      <td>-0.349497</td>\n",
              "      <td>-0.301652</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.805469</td>\n",
              "      <td>0.663779</td>\n",
              "      <td>0.062341</td>\n",
              "      <td>-1.293513</td>\n",
              "      <td>0.419038</td>\n",
              "      <td>1.360080</td>\n",
              "      <td>-0.428137</td>\n",
              "      <td>-1.014816</td>\n",
              "      <td>2.154513</td>\n",
              "      <td>0.037632</td>\n",
              "      <td>-1.194724</td>\n",
              "      <td>-0.565663</td>\n",
              "      <td>-0.230392</td>\n",
              "      <td>1.560470</td>\n",
              "      <td>0.745605</td>\n",
              "      <td>-1.303745</td>\n",
              "      <td>-0.616904</td>\n",
              "      <td>0.891984</td>\n",
              "      <td>-0.829454</td>\n",
              "      <td>0.009040</td>\n",
              "      <td>-0.167278</td>\n",
              "      <td>0.100520</td>\n",
              "      <td>0.025883</td>\n",
              "      <td>-0.090795</td>\n",
              "      <td>-0.806667</td>\n",
              "      <td>-0.212954</td>\n",
              "      <td>-0.571660</td>\n",
              "      <td>1.301951</td>\n",
              "      <td>-1.995423</td>\n",
              "      <td>-0.757284</td>\n",
              "      <td>-1.709719</td>\n",
              "      <td>-0.327805</td>\n",
              "      <td>-0.244655</td>\n",
              "      <td>-0.214154</td>\n",
              "      <td>0.025581</td>\n",
              "      <td>-0.820590</td>\n",
              "      <td>0.131171</td>\n",
              "      <td>0.425759</td>\n",
              "      <td>-0.504117</td>\n",
              "      <td>1.773358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sky</th>\n",
              "      <td>1.449165</td>\n",
              "      <td>0.153278</td>\n",
              "      <td>-1.509703</td>\n",
              "      <td>-0.340397</td>\n",
              "      <td>-0.808891</td>\n",
              "      <td>0.449833</td>\n",
              "      <td>-0.356767</td>\n",
              "      <td>-1.193008</td>\n",
              "      <td>0.558038</td>\n",
              "      <td>0.764705</td>\n",
              "      <td>0.095494</td>\n",
              "      <td>0.418742</td>\n",
              "      <td>0.503980</td>\n",
              "      <td>0.348790</td>\n",
              "      <td>-1.417122</td>\n",
              "      <td>-0.390010</td>\n",
              "      <td>-1.054957</td>\n",
              "      <td>0.292385</td>\n",
              "      <td>-2.709125</td>\n",
              "      <td>0.646774</td>\n",
              "      <td>-1.062240</td>\n",
              "      <td>-1.409375</td>\n",
              "      <td>-0.158662</td>\n",
              "      <td>0.855864</td>\n",
              "      <td>0.027267</td>\n",
              "      <td>-0.104874</td>\n",
              "      <td>0.945288</td>\n",
              "      <td>1.170664</td>\n",
              "      <td>1.482344</td>\n",
              "      <td>-0.349548</td>\n",
              "      <td>-0.754053</td>\n",
              "      <td>-2.300823</td>\n",
              "      <td>-1.179725</td>\n",
              "      <td>2.561830</td>\n",
              "      <td>-0.392510</td>\n",
              "      <td>2.203889</td>\n",
              "      <td>-1.327304</td>\n",
              "      <td>-0.063887</td>\n",
              "      <td>-0.411009</td>\n",
              "      <td>-1.667183</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.587274</td>\n",
              "      <td>-1.464557</td>\n",
              "      <td>1.315149</td>\n",
              "      <td>-0.181798</td>\n",
              "      <td>-0.215293</td>\n",
              "      <td>-0.064679</td>\n",
              "      <td>-0.125366</td>\n",
              "      <td>1.888723</td>\n",
              "      <td>1.516137</td>\n",
              "      <td>-0.700118</td>\n",
              "      <td>-0.243861</td>\n",
              "      <td>1.494783</td>\n",
              "      <td>-0.556467</td>\n",
              "      <td>0.085412</td>\n",
              "      <td>-2.062423</td>\n",
              "      <td>-1.162731</td>\n",
              "      <td>-0.515584</td>\n",
              "      <td>-1.128738</td>\n",
              "      <td>0.951601</td>\n",
              "      <td>-0.017662</td>\n",
              "      <td>-2.747708</td>\n",
              "      <td>-0.893692</td>\n",
              "      <td>-0.329091</td>\n",
              "      <td>-0.308133</td>\n",
              "      <td>0.417533</td>\n",
              "      <td>-0.089249</td>\n",
              "      <td>-0.588559</td>\n",
              "      <td>0.202914</td>\n",
              "      <td>-0.494305</td>\n",
              "      <td>-0.191850</td>\n",
              "      <td>0.045936</td>\n",
              "      <td>2.844258</td>\n",
              "      <td>0.676560</td>\n",
              "      <td>0.412295</td>\n",
              "      <td>-0.157112</td>\n",
              "      <td>-0.010236</td>\n",
              "      <td>-0.340997</td>\n",
              "      <td>0.800338</td>\n",
              "      <td>0.298349</td>\n",
              "      <td>0.183261</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>blue</th>\n",
              "      <td>-0.469048</td>\n",
              "      <td>-0.312949</td>\n",
              "      <td>0.123316</td>\n",
              "      <td>1.329445</td>\n",
              "      <td>-0.551616</td>\n",
              "      <td>-0.133613</td>\n",
              "      <td>-0.911876</td>\n",
              "      <td>0.007528</td>\n",
              "      <td>2.264735</td>\n",
              "      <td>-0.722735</td>\n",
              "      <td>0.055664</td>\n",
              "      <td>-1.083289</td>\n",
              "      <td>0.455743</td>\n",
              "      <td>-0.041167</td>\n",
              "      <td>-0.014597</td>\n",
              "      <td>-0.915513</td>\n",
              "      <td>0.417387</td>\n",
              "      <td>-0.742937</td>\n",
              "      <td>-0.534138</td>\n",
              "      <td>0.256865</td>\n",
              "      <td>0.496202</td>\n",
              "      <td>0.672305</td>\n",
              "      <td>-0.081591</td>\n",
              "      <td>1.141091</td>\n",
              "      <td>0.126867</td>\n",
              "      <td>-0.233428</td>\n",
              "      <td>-0.047002</td>\n",
              "      <td>0.639444</td>\n",
              "      <td>1.844794</td>\n",
              "      <td>0.935714</td>\n",
              "      <td>2.214909</td>\n",
              "      <td>1.384562</td>\n",
              "      <td>-0.715399</td>\n",
              "      <td>1.816184</td>\n",
              "      <td>0.282828</td>\n",
              "      <td>0.029626</td>\n",
              "      <td>-1.683331</td>\n",
              "      <td>0.306919</td>\n",
              "      <td>0.286198</td>\n",
              "      <td>-0.184079</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.061380</td>\n",
              "      <td>0.460302</td>\n",
              "      <td>-0.739375</td>\n",
              "      <td>1.300935</td>\n",
              "      <td>0.556676</td>\n",
              "      <td>0.095558</td>\n",
              "      <td>-0.119986</td>\n",
              "      <td>-0.871369</td>\n",
              "      <td>-0.106704</td>\n",
              "      <td>-0.253442</td>\n",
              "      <td>-0.041808</td>\n",
              "      <td>0.202927</td>\n",
              "      <td>-0.038642</td>\n",
              "      <td>0.794085</td>\n",
              "      <td>-0.859474</td>\n",
              "      <td>0.157171</td>\n",
              "      <td>1.115904</td>\n",
              "      <td>1.366879</td>\n",
              "      <td>1.360007</td>\n",
              "      <td>-0.997755</td>\n",
              "      <td>0.543524</td>\n",
              "      <td>0.206829</td>\n",
              "      <td>0.667261</td>\n",
              "      <td>-0.277421</td>\n",
              "      <td>-1.065924</td>\n",
              "      <td>1.259926</td>\n",
              "      <td>1.129960</td>\n",
              "      <td>0.334031</td>\n",
              "      <td>-1.456439</td>\n",
              "      <td>-1.293781</td>\n",
              "      <td>-0.815232</td>\n",
              "      <td>-2.548271</td>\n",
              "      <td>-0.507428</td>\n",
              "      <td>0.511041</td>\n",
              "      <td>0.378241</td>\n",
              "      <td>1.546835</td>\n",
              "      <td>-0.228272</td>\n",
              "      <td>0.910393</td>\n",
              "      <td>-2.542138</td>\n",
              "      <td>2.244305</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>beautiful</th>\n",
              "      <td>-0.533289</td>\n",
              "      <td>-1.827823</td>\n",
              "      <td>-1.032799</td>\n",
              "      <td>-0.451846</td>\n",
              "      <td>0.894649</td>\n",
              "      <td>0.748004</td>\n",
              "      <td>1.423167</td>\n",
              "      <td>0.299123</td>\n",
              "      <td>1.135338</td>\n",
              "      <td>0.562234</td>\n",
              "      <td>0.655088</td>\n",
              "      <td>-0.111210</td>\n",
              "      <td>-1.503113</td>\n",
              "      <td>0.532644</td>\n",
              "      <td>-0.929090</td>\n",
              "      <td>0.205524</td>\n",
              "      <td>0.453472</td>\n",
              "      <td>2.669638</td>\n",
              "      <td>1.494946</td>\n",
              "      <td>1.303193</td>\n",
              "      <td>-0.559608</td>\n",
              "      <td>-1.185347</td>\n",
              "      <td>-1.100271</td>\n",
              "      <td>1.257161</td>\n",
              "      <td>-2.833918</td>\n",
              "      <td>1.233855</td>\n",
              "      <td>-0.035600</td>\n",
              "      <td>0.023203</td>\n",
              "      <td>0.544362</td>\n",
              "      <td>-0.163978</td>\n",
              "      <td>0.631034</td>\n",
              "      <td>2.037613</td>\n",
              "      <td>1.275391</td>\n",
              "      <td>1.610185</td>\n",
              "      <td>-1.418666</td>\n",
              "      <td>-2.337459</td>\n",
              "      <td>-0.100569</td>\n",
              "      <td>-0.214049</td>\n",
              "      <td>0.246707</td>\n",
              "      <td>0.769618</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.866297</td>\n",
              "      <td>0.129173</td>\n",
              "      <td>-0.164398</td>\n",
              "      <td>0.627094</td>\n",
              "      <td>-0.244747</td>\n",
              "      <td>-0.768476</td>\n",
              "      <td>0.259487</td>\n",
              "      <td>0.354147</td>\n",
              "      <td>0.333700</td>\n",
              "      <td>-0.829295</td>\n",
              "      <td>0.270319</td>\n",
              "      <td>0.299325</td>\n",
              "      <td>-1.462435</td>\n",
              "      <td>0.618261</td>\n",
              "      <td>0.635193</td>\n",
              "      <td>0.839166</td>\n",
              "      <td>-0.812424</td>\n",
              "      <td>-0.886244</td>\n",
              "      <td>-1.163957</td>\n",
              "      <td>-0.170511</td>\n",
              "      <td>-0.735343</td>\n",
              "      <td>-3.068204</td>\n",
              "      <td>-1.123936</td>\n",
              "      <td>0.032735</td>\n",
              "      <td>0.532889</td>\n",
              "      <td>0.084369</td>\n",
              "      <td>-1.344577</td>\n",
              "      <td>-0.912870</td>\n",
              "      <td>-1.171750</td>\n",
              "      <td>-0.942274</td>\n",
              "      <td>-0.466412</td>\n",
              "      <td>2.093053</td>\n",
              "      <td>1.168456</td>\n",
              "      <td>1.499620</td>\n",
              "      <td>-0.231552</td>\n",
              "      <td>0.603686</td>\n",
              "      <td>0.141400</td>\n",
              "      <td>-0.061871</td>\n",
              "      <td>-0.691325</td>\n",
              "      <td>-0.207963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>quick</th>\n",
              "      <td>-0.558469</td>\n",
              "      <td>-0.937921</td>\n",
              "      <td>-0.695200</td>\n",
              "      <td>2.036233</td>\n",
              "      <td>1.005487</td>\n",
              "      <td>0.400959</td>\n",
              "      <td>0.102566</td>\n",
              "      <td>-1.534967</td>\n",
              "      <td>0.229444</td>\n",
              "      <td>0.213800</td>\n",
              "      <td>-0.716180</td>\n",
              "      <td>-0.071263</td>\n",
              "      <td>0.846729</td>\n",
              "      <td>-0.002873</td>\n",
              "      <td>0.712897</td>\n",
              "      <td>-0.503977</td>\n",
              "      <td>0.684970</td>\n",
              "      <td>1.153641</td>\n",
              "      <td>-0.035872</td>\n",
              "      <td>0.718323</td>\n",
              "      <td>-0.149394</td>\n",
              "      <td>-1.899651</td>\n",
              "      <td>0.425477</td>\n",
              "      <td>-1.223452</td>\n",
              "      <td>-0.281957</td>\n",
              "      <td>-0.086701</td>\n",
              "      <td>-1.098551</td>\n",
              "      <td>-1.214337</td>\n",
              "      <td>0.875268</td>\n",
              "      <td>-0.198583</td>\n",
              "      <td>-0.603760</td>\n",
              "      <td>1.951853</td>\n",
              "      <td>0.452532</td>\n",
              "      <td>-0.478115</td>\n",
              "      <td>0.786527</td>\n",
              "      <td>-0.292773</td>\n",
              "      <td>-1.606256</td>\n",
              "      <td>0.355996</td>\n",
              "      <td>1.234043</td>\n",
              "      <td>0.538837</td>\n",
              "      <td>...</td>\n",
              "      <td>0.407044</td>\n",
              "      <td>-0.616079</td>\n",
              "      <td>-1.552686</td>\n",
              "      <td>0.161480</td>\n",
              "      <td>1.508547</td>\n",
              "      <td>1.535354</td>\n",
              "      <td>-1.188404</td>\n",
              "      <td>0.022315</td>\n",
              "      <td>1.206893</td>\n",
              "      <td>-0.230642</td>\n",
              "      <td>0.525955</td>\n",
              "      <td>-0.411932</td>\n",
              "      <td>1.052186</td>\n",
              "      <td>-1.426858</td>\n",
              "      <td>0.772702</td>\n",
              "      <td>0.410571</td>\n",
              "      <td>2.022690</td>\n",
              "      <td>0.427967</td>\n",
              "      <td>-0.236282</td>\n",
              "      <td>-2.390484</td>\n",
              "      <td>-0.810348</td>\n",
              "      <td>1.703603</td>\n",
              "      <td>1.794683</td>\n",
              "      <td>-0.031043</td>\n",
              "      <td>1.538660</td>\n",
              "      <td>-0.721608</td>\n",
              "      <td>-0.382144</td>\n",
              "      <td>0.785618</td>\n",
              "      <td>-1.141353</td>\n",
              "      <td>0.622182</td>\n",
              "      <td>-0.171468</td>\n",
              "      <td>0.061658</td>\n",
              "      <td>0.587657</td>\n",
              "      <td>-0.942987</td>\n",
              "      <td>-1.038087</td>\n",
              "      <td>-0.553705</td>\n",
              "      <td>-1.465742</td>\n",
              "      <td>0.107394</td>\n",
              "      <td>0.593551</td>\n",
              "      <td>0.867413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>brown</th>\n",
              "      <td>0.393551</td>\n",
              "      <td>0.450134</td>\n",
              "      <td>-0.079574</td>\n",
              "      <td>0.268751</td>\n",
              "      <td>1.278429</td>\n",
              "      <td>0.613324</td>\n",
              "      <td>-0.850955</td>\n",
              "      <td>-1.418713</td>\n",
              "      <td>0.596749</td>\n",
              "      <td>-0.345802</td>\n",
              "      <td>0.950896</td>\n",
              "      <td>-0.025625</td>\n",
              "      <td>-1.168137</td>\n",
              "      <td>0.793924</td>\n",
              "      <td>-2.977073</td>\n",
              "      <td>-0.668861</td>\n",
              "      <td>1.038769</td>\n",
              "      <td>-0.083873</td>\n",
              "      <td>0.679444</td>\n",
              "      <td>-0.738849</td>\n",
              "      <td>-0.430129</td>\n",
              "      <td>-0.074172</td>\n",
              "      <td>0.663841</td>\n",
              "      <td>0.483713</td>\n",
              "      <td>1.198106</td>\n",
              "      <td>1.100216</td>\n",
              "      <td>-2.123839</td>\n",
              "      <td>0.062811</td>\n",
              "      <td>-0.477568</td>\n",
              "      <td>-0.211494</td>\n",
              "      <td>-0.315232</td>\n",
              "      <td>-0.184991</td>\n",
              "      <td>1.263915</td>\n",
              "      <td>1.327454</td>\n",
              "      <td>-0.317354</td>\n",
              "      <td>0.562355</td>\n",
              "      <td>0.205006</td>\n",
              "      <td>-0.359538</td>\n",
              "      <td>-1.399411</td>\n",
              "      <td>0.603990</td>\n",
              "      <td>...</td>\n",
              "      <td>0.245905</td>\n",
              "      <td>0.249995</td>\n",
              "      <td>0.207692</td>\n",
              "      <td>-1.040047</td>\n",
              "      <td>-0.546615</td>\n",
              "      <td>-1.596640</td>\n",
              "      <td>0.017206</td>\n",
              "      <td>-0.460313</td>\n",
              "      <td>-0.473303</td>\n",
              "      <td>-0.445533</td>\n",
              "      <td>1.025541</td>\n",
              "      <td>-0.217998</td>\n",
              "      <td>-0.569339</td>\n",
              "      <td>-1.148984</td>\n",
              "      <td>0.874883</td>\n",
              "      <td>2.073158</td>\n",
              "      <td>0.776481</td>\n",
              "      <td>-0.504930</td>\n",
              "      <td>-0.226141</td>\n",
              "      <td>2.423683</td>\n",
              "      <td>-2.107358</td>\n",
              "      <td>-0.331935</td>\n",
              "      <td>1.325698</td>\n",
              "      <td>-0.067668</td>\n",
              "      <td>0.190376</td>\n",
              "      <td>0.624280</td>\n",
              "      <td>-0.458483</td>\n",
              "      <td>0.667423</td>\n",
              "      <td>1.764585</td>\n",
              "      <td>0.414479</td>\n",
              "      <td>0.610540</td>\n",
              "      <td>1.147712</td>\n",
              "      <td>-0.101477</td>\n",
              "      <td>-0.694855</td>\n",
              "      <td>0.206836</td>\n",
              "      <td>0.363352</td>\n",
              "      <td>-0.757771</td>\n",
              "      <td>1.558180</td>\n",
              "      <td>-0.250275</td>\n",
              "      <td>1.686646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fox</th>\n",
              "      <td>0.797055</td>\n",
              "      <td>1.411196</td>\n",
              "      <td>-0.031755</td>\n",
              "      <td>-0.415710</td>\n",
              "      <td>0.437677</td>\n",
              "      <td>-0.657659</td>\n",
              "      <td>0.354126</td>\n",
              "      <td>0.090269</td>\n",
              "      <td>0.325975</td>\n",
              "      <td>-2.354204</td>\n",
              "      <td>0.107897</td>\n",
              "      <td>-0.545043</td>\n",
              "      <td>-1.713373</td>\n",
              "      <td>-0.675881</td>\n",
              "      <td>1.067379</td>\n",
              "      <td>0.405168</td>\n",
              "      <td>0.817522</td>\n",
              "      <td>-0.046598</td>\n",
              "      <td>-0.891815</td>\n",
              "      <td>0.111980</td>\n",
              "      <td>-0.225963</td>\n",
              "      <td>0.112378</td>\n",
              "      <td>-0.113158</td>\n",
              "      <td>0.944294</td>\n",
              "      <td>-1.158892</td>\n",
              "      <td>1.507765</td>\n",
              "      <td>-1.871655</td>\n",
              "      <td>-0.772381</td>\n",
              "      <td>-3.080200</td>\n",
              "      <td>-1.356650</td>\n",
              "      <td>-1.736288</td>\n",
              "      <td>-1.821512</td>\n",
              "      <td>0.791796</td>\n",
              "      <td>-0.073050</td>\n",
              "      <td>0.336441</td>\n",
              "      <td>0.421689</td>\n",
              "      <td>-1.321763</td>\n",
              "      <td>1.158198</td>\n",
              "      <td>-2.189407</td>\n",
              "      <td>-1.608815</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.141466</td>\n",
              "      <td>-1.450243</td>\n",
              "      <td>-0.314869</td>\n",
              "      <td>1.574350</td>\n",
              "      <td>0.951972</td>\n",
              "      <td>1.280705</td>\n",
              "      <td>0.314330</td>\n",
              "      <td>-1.201877</td>\n",
              "      <td>0.334510</td>\n",
              "      <td>0.089217</td>\n",
              "      <td>0.224379</td>\n",
              "      <td>0.550554</td>\n",
              "      <td>0.538171</td>\n",
              "      <td>0.594383</td>\n",
              "      <td>0.131937</td>\n",
              "      <td>-0.734224</td>\n",
              "      <td>-1.917705</td>\n",
              "      <td>0.283525</td>\n",
              "      <td>-0.938562</td>\n",
              "      <td>-1.972943</td>\n",
              "      <td>0.597334</td>\n",
              "      <td>-1.106533</td>\n",
              "      <td>1.231255</td>\n",
              "      <td>-0.223283</td>\n",
              "      <td>1.583508</td>\n",
              "      <td>0.981517</td>\n",
              "      <td>-0.033549</td>\n",
              "      <td>-2.119381</td>\n",
              "      <td>2.264282</td>\n",
              "      <td>1.134446</td>\n",
              "      <td>-0.785851</td>\n",
              "      <td>1.335509</td>\n",
              "      <td>-1.517941</td>\n",
              "      <td>-1.349340</td>\n",
              "      <td>0.753482</td>\n",
              "      <td>0.640396</td>\n",
              "      <td>-0.159388</td>\n",
              "      <td>1.045616</td>\n",
              "      <td>0.765108</td>\n",
              "      <td>-1.618113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lazy</th>\n",
              "      <td>-1.776232</td>\n",
              "      <td>0.995156</td>\n",
              "      <td>1.236979</td>\n",
              "      <td>1.855693</td>\n",
              "      <td>0.066580</td>\n",
              "      <td>0.742035</td>\n",
              "      <td>0.651343</td>\n",
              "      <td>-2.000114</td>\n",
              "      <td>-0.701359</td>\n",
              "      <td>-1.533831</td>\n",
              "      <td>-1.086544</td>\n",
              "      <td>0.519863</td>\n",
              "      <td>0.184006</td>\n",
              "      <td>-0.274050</td>\n",
              "      <td>2.911425</td>\n",
              "      <td>0.115098</td>\n",
              "      <td>-1.112191</td>\n",
              "      <td>-0.479669</td>\n",
              "      <td>1.260256</td>\n",
              "      <td>-0.132396</td>\n",
              "      <td>-1.566988</td>\n",
              "      <td>-0.787054</td>\n",
              "      <td>0.502047</td>\n",
              "      <td>0.440928</td>\n",
              "      <td>-0.449495</td>\n",
              "      <td>-2.725736</td>\n",
              "      <td>0.063942</td>\n",
              "      <td>-1.871578</td>\n",
              "      <td>-0.684811</td>\n",
              "      <td>-0.386248</td>\n",
              "      <td>-0.146623</td>\n",
              "      <td>-0.403312</td>\n",
              "      <td>0.504547</td>\n",
              "      <td>-2.330001</td>\n",
              "      <td>0.666139</td>\n",
              "      <td>-0.979507</td>\n",
              "      <td>1.063577</td>\n",
              "      <td>1.380405</td>\n",
              "      <td>0.362965</td>\n",
              "      <td>2.558188</td>\n",
              "      <td>...</td>\n",
              "      <td>0.077024</td>\n",
              "      <td>0.204456</td>\n",
              "      <td>0.052451</td>\n",
              "      <td>1.531995</td>\n",
              "      <td>1.100020</td>\n",
              "      <td>-0.782305</td>\n",
              "      <td>0.171052</td>\n",
              "      <td>0.772296</td>\n",
              "      <td>0.057735</td>\n",
              "      <td>1.645802</td>\n",
              "      <td>-0.042587</td>\n",
              "      <td>0.384749</td>\n",
              "      <td>-0.337151</td>\n",
              "      <td>-0.776732</td>\n",
              "      <td>0.059966</td>\n",
              "      <td>-0.806825</td>\n",
              "      <td>0.517419</td>\n",
              "      <td>-0.666449</td>\n",
              "      <td>-1.164875</td>\n",
              "      <td>-0.713597</td>\n",
              "      <td>-0.276371</td>\n",
              "      <td>0.012940</td>\n",
              "      <td>-0.516041</td>\n",
              "      <td>1.522119</td>\n",
              "      <td>0.748600</td>\n",
              "      <td>-1.616673</td>\n",
              "      <td>-0.584538</td>\n",
              "      <td>-0.564948</td>\n",
              "      <td>0.231805</td>\n",
              "      <td>0.781042</td>\n",
              "      <td>-0.329508</td>\n",
              "      <td>0.476513</td>\n",
              "      <td>0.212365</td>\n",
              "      <td>-1.254748</td>\n",
              "      <td>-0.556678</td>\n",
              "      <td>-1.258437</td>\n",
              "      <td>-0.174385</td>\n",
              "      <td>-0.158279</td>\n",
              "      <td>0.970042</td>\n",
              "      <td>-0.846919</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows  100 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 0         1         2   ...        97        98        99\n",
              "the       -1.020247  0.858016 -0.972954  ...  0.127352 -0.370458  0.407416\n",
              "is        -0.469278  0.731419 -2.046814  ...  0.468410 -1.300014  0.211676\n",
              "and        1.303654 -0.400301  0.474038  ...  0.425759 -0.504117  1.773358\n",
              "sky        1.449165  0.153278 -1.509703  ...  0.800338  0.298349  0.183261\n",
              "blue      -0.469048 -0.312949  0.123316  ...  0.910393 -2.542138  2.244305\n",
              "beautiful -0.533289 -1.827823 -1.032799  ... -0.061871 -0.691325 -0.207963\n",
              "quick     -0.558469 -0.937921 -0.695200  ...  0.107394  0.593551  0.867413\n",
              "brown      0.393551  0.450134 -0.079574  ...  1.558180 -0.250275  1.686646\n",
              "fox        0.797055  1.411196 -0.031755  ...  1.045616  0.765108 -1.618113\n",
              "lazy      -1.776232  0.995156  1.236979  ... -0.158279  0.970042 -0.846919\n",
              "\n",
              "[10 rows x 100 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9mYcRmygvEO"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVM5Rq_9pfux",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11b33fc5-4ae7-4e5c-d349-955a3c2ee987"
      },
      "source": [
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "weights = weights.view(-1,100)\n",
        "distance_matrix = euclidean_distances(weights.detach().numpy())\n",
        "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:4]+1] \n",
        "                 for search_term in ['the', 'fox', 'beautiful','brown','lazy']}\n",
        "\n",
        "similar_words"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'beautiful': ['over', 'but', 'brown'],\n",
              " 'brown': ['has', 'jumps', \"king's\"],\n",
              " 'fox': ['a', 'toast', 'brown'],\n",
              " 'lazy': ['quick', 'a', 'this'],\n",
              " 'the': ['is', 'over', 'quick']}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT-1TyE3pfux"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}