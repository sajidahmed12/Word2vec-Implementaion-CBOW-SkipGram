{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UypYA6kX7rrH"
   },
   "source": [
    "## CBOW - Word2Vec Implementation Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9nhCCVg-pfuh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import webtext\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import text\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.options.display.max_colwidth = 200\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wtEi1_Ewp38e",
    "outputId": "e35ef7d5-b7b3-4917-a77b-bdcd8caa2dd6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sajid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package webtext to\n",
      "[nltk_data]     C:\\Users\\Sajid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package webtext is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('webtext')\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2iN6sWbdpfum"
   },
   "source": [
    "## Pre-Processing text Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "cB1A2KCQpfuo"
   },
   "outputs": [],
   "source": [
    "wordpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = wordpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "rlEIdkRYpfup",
    "outputId": "a4227224-17ad-4c41-929b-105cfa7ba89d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The sky is blue and beautiful.</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Love this blue and beautiful sky!</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The quick brown fox jumps over the lazy dog.</td>\n",
       "      <td>animals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A king's breakfast has sausages, ham, bacon, eggs, toast and beans</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I love green eggs, ham, sausages and bacon!</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The brown fox is quick and the blue dog is lazy!</td>\n",
       "      <td>animals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The sky is very blue and the sky is very beautiful today</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The dog is lazy but the brown fox is quick!</td>\n",
       "      <td>animals</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             Document Category\n",
       "0                                      The sky is blue and beautiful.  weather\n",
       "1                                   Love this blue and beautiful sky!  weather\n",
       "2                        The quick brown fox jumps over the lazy dog.  animals\n",
       "3  A king's breakfast has sausages, ham, bacon, eggs, toast and beans     food\n",
       "4                         I love green eggs, ham, sausages and bacon!     food\n",
       "5                    The brown fox is quick and the blue dog is lazy!  animals\n",
       "6            The sky is very blue and the sky is very beautiful today  weather\n",
       "7                         The dog is lazy but the brown fox is quick!  animals"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = ['The sky is blue and beautiful.',\n",
    "          'Love this blue and beautiful sky!',\n",
    "          'The quick brown fox jumps over the lazy dog.',\n",
    "          \"A king's breakfast has sausages, ham, bacon, eggs, toast and beans\",\n",
    "          'I love green eggs, ham, sausages and bacon!',\n",
    "          'The brown fox is quick and the blue dog is lazy!',\n",
    "          'The sky is very blue and the sky is very beautiful today',\n",
    "          'The dog is lazy but the brown fox is quick!'    \n",
    "]\n",
    "labels = ['weather', 'weather', 'animals', 'food', 'food', 'animals', 'weather', 'animals']\n",
    "\n",
    "corpus = np.array(corpus)\n",
    "corpus_df = pd.DataFrame({'Document': corpus, \n",
    "                          'Category': labels})\n",
    "corpus_df = corpus_df[['Document', 'Category']]\n",
    "corpus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FghWaqo1pfuq",
    "outputId": "7a9fc7ac-c911-482d-b6c7-b44c83f32d73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['firefox.txt', 'grail.txt', 'overheard.txt', 'pirates.txt', 'singles.txt', 'wine.txt']\n",
      "564601\n"
     ]
    }
   ],
   "source": [
    "# build a sample vocab\n",
    "vocab = []\n",
    "print(webtext.fileids())\n",
    "print(len(webtext.raw('firefox.txt'))) \n",
    "for fileid in webtext.fileids():\n",
    "    vocab.append(webtext.raw('firefox.txt'))\n",
    "\n",
    "    #print(brown.raw('cb01').strip()[:1000])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgL7JfiPpfuq"
   },
   "source": [
    "### text preprocessing (Remove tags e.g HTML,Remove special characters, Remove stopwords) === Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iCm0sMaspfur",
    "outputId": "b14788d5-98c6-4d1d-aa83-e5cfd1edf8a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 31\n",
      "Vocabulary Sample: [('the', 1), ('is', 2), ('and', 3), ('sky', 4), ('blue', 5), ('beautiful', 6), ('quick', 7), ('brown', 8), ('fox', 9), ('lazy', 10)]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "word2id = tokenizer.word_index\n",
    "\n",
    "word2id['PAD'] = 0\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in corpus]\n",
    "\n",
    "vocab_size = len(word2id)\n",
    "embed_size = 100\n",
    "window_size = 2\n",
    "\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Vocabulary Sample:', list(word2id.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HT4Hrfo_pfus"
   },
   "source": [
    "### [context_words, target_word] pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "hdWqc3v4pfut"
   },
   "outputs": [],
   "source": [
    "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
    "    X = []\n",
    "    Y = []\n",
    "    context_length = window_size*2\n",
    "    for words in wids:\n",
    "        sentence_length = len(words)\n",
    "        for index, word in enumerate(words):           \n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "            context = [words[i] for i in range(start, end)if 0 <= i < sentence_length and i != index]\n",
    "            x = sequence.pad_sequences([context], maxlen=context_length)\n",
    "            X.append(x)\n",
    "            Y.append(word)\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HAgVUa1Qpfuu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EsdpsBjQpfuv"
   },
   "source": [
    "## CBOW (Contineous bag of Words Model architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UqCLwsVGpfuv",
    "outputId": "449b5450-8632-45f3-c1b5-b3b371446c90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "embeddings.weight \t torch.Size([31, 100])\n",
      "linear1.weight \t torch.Size([100, 100])\n",
      "linear1.bias \t torch.Size([100])\n",
      "linear2.weight \t torch.Size([31, 100])\n",
      "linear2.bias \t torch.Size([31])\n",
      "Optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.001, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [2698020534024, 2698020535752, 2698020535320, 2698020533952, 2698020534096]}]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class CBOW(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, inp_size , vocab_size, embedding_dim=100):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, 100)\n",
    "        self.activation_function1 = nn.ReLU()        \n",
    "        self.linear2 = nn.Linear(100, vocab_size)\n",
    "        self.activation_function2 = nn.LogSoftmax(dim = -1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embeds = sum(self.embeddings(torch.from_numpy(inputs).long().cuda())).view(1,-1)\n",
    "        out = self.linear1(embeds)\n",
    "        out = self.activation_function1(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.activation_function2(out)\n",
    "        return out\n",
    "\n",
    "    \n",
    "model = CBOW(window_size*2,vocab_size).cuda()\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])\n",
    "\n",
    "torch.save(model.state_dict(), \"Cbow_Weights\")\n",
    "\n",
    "# model = TheModelClass(*args, **kwargs)\n",
    "# model.load_state_dict(torch.load(PATH))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vE_4ZisS_JZo",
    "outputId": "5c570f7b-e127-42b1-e9e5-34e2d276bcf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tLoss: tensor(4.0768, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 2 \tLoss: tensor(3.9260, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 3 \tLoss: tensor(3.7763, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 4 \tLoss: tensor(3.6316, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 5 \tLoss: tensor(3.4874, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 6 \tLoss: tensor(3.3656, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 7 \tLoss: tensor(3.2325, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 8 \tLoss: tensor(3.1233, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 9 \tLoss: tensor(3.0060, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 10 \tLoss: tensor(2.8863, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 11 \tLoss: tensor(2.7681, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 12 \tLoss: tensor(2.6619, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 13 \tLoss: tensor(2.5569, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 14 \tLoss: tensor(2.4583, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 15 \tLoss: tensor(2.3627, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 16 \tLoss: tensor(2.2636, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 17 \tLoss: tensor(2.1677, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 18 \tLoss: tensor(2.0891, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 19 \tLoss: tensor(2.0068, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 20 \tLoss: tensor(1.9338, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 21 \tLoss: tensor(1.8693, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 22 \tLoss: tensor(1.7979, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 23 \tLoss: tensor(1.7363, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 24 \tLoss: tensor(1.6690, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 25 \tLoss: tensor(1.6069, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 26 \tLoss: tensor(1.5563, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 27 \tLoss: tensor(1.4980, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 28 \tLoss: tensor(1.4453, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 29 \tLoss: tensor(1.4026, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 30 \tLoss: tensor(1.3604, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 31 \tLoss: tensor(1.3104, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 32 \tLoss: tensor(1.2774, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 33 \tLoss: tensor(1.2380, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 34 \tLoss: tensor(1.2015, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 35 \tLoss: tensor(1.1649, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 36 \tLoss: tensor(1.1342, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 37 \tLoss: tensor(1.1031, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 38 \tLoss: tensor(1.0716, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 39 \tLoss: tensor(1.0536, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 40 \tLoss: tensor(1.0231, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 41 \tLoss: tensor(1.0056, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 42 \tLoss: tensor(0.9819, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 43 \tLoss: tensor(0.9607, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 44 \tLoss: tensor(0.9404, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 45 \tLoss: tensor(0.9226, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 46 \tLoss: tensor(0.9090, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 47 \tLoss: tensor(0.8825, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 48 \tLoss: tensor(0.8663, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 49 \tLoss: tensor(0.8537, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 50 \tLoss: tensor(0.8411, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 51 \tLoss: tensor(0.8208, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 52 \tLoss: tensor(0.8100, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 53 \tLoss: tensor(0.7934, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 54 \tLoss: tensor(0.7853, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 55 \tLoss: tensor(0.7660, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 56 \tLoss: tensor(0.7551, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 57 \tLoss: tensor(0.7418, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 58 \tLoss: tensor(0.7303, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 59 \tLoss: tensor(0.7240, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 60 \tLoss: tensor(0.7093, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 61 \tLoss: tensor(0.6977, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 62 \tLoss: tensor(0.6853, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 63 \tLoss: tensor(0.6783, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 64 \tLoss: tensor(0.6662, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 65 \tLoss: tensor(0.6566, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 66 \tLoss: tensor(0.6444, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 67 \tLoss: tensor(0.6410, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 68 \tLoss: tensor(0.6324, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 69 \tLoss: tensor(0.6197, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 70 \tLoss: tensor(0.6133, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 71 \tLoss: tensor(0.6036, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 72 \tLoss: tensor(0.5960, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 73 \tLoss: tensor(0.5902, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 74 \tLoss: tensor(0.5804, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 75 \tLoss: tensor(0.5735, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 76 \tLoss: tensor(0.5645, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 77 \tLoss: tensor(0.5606, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 78 \tLoss: tensor(0.5497, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 79 \tLoss: tensor(0.5453, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 80 \tLoss: tensor(0.5366, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 81 \tLoss: tensor(0.5328, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 82 \tLoss: tensor(0.5277, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 83 \tLoss: tensor(0.5172, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 84 \tLoss: tensor(0.5128, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 85 \tLoss: tensor(0.5114, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 86 \tLoss: tensor(0.4988, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 87 \tLoss: tensor(0.4998, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 88 \tLoss: tensor(0.4850, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 89 \tLoss: tensor(0.4822, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 90 \tLoss: tensor(0.4757, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 91 \tLoss: tensor(0.4714, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 92 \tLoss: tensor(0.4657, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 93 \tLoss: tensor(0.4628, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 94 \tLoss: tensor(0.4575, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 95 \tLoss: tensor(0.4512, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 96 \tLoss: tensor(0.4476, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 97 \tLoss: tensor(0.4417, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 98 \tLoss: tensor(0.4387, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 99 \tLoss: tensor(0.4343, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 100 \tLoss: tensor(0.4297, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 101 \tLoss: tensor(0.4270, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 102 \tLoss: tensor(0.4214, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 103 \tLoss: tensor(0.4181, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 104 \tLoss: tensor(0.4130, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 105 \tLoss: tensor(0.4100, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 106 \tLoss: tensor(0.4114, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 107 \tLoss: tensor(0.3987, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 108 \tLoss: tensor(0.3969, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 109 \tLoss: tensor(0.3929, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 110 \tLoss: tensor(0.3904, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 111 \tLoss: tensor(0.3863, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 112 \tLoss: tensor(0.3831, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 113 \tLoss: tensor(0.3797, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 114 \tLoss: tensor(0.3743, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 115 \tLoss: tensor(0.3716, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 116 \tLoss: tensor(0.3680, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 117 \tLoss: tensor(0.3652, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 118 \tLoss: tensor(0.3605, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 119 \tLoss: tensor(0.3582, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 120 \tLoss: tensor(0.3555, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 121 \tLoss: tensor(0.3512, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 122 \tLoss: tensor(0.3485, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 123 \tLoss: tensor(0.3449, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 124 \tLoss: tensor(0.3423, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 125 \tLoss: tensor(0.3381, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 126 \tLoss: tensor(0.3358, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 127 \tLoss: tensor(0.3333, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 128 \tLoss: tensor(0.3297, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 129 \tLoss: tensor(0.3280, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 130 \tLoss: tensor(0.3238, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 131 \tLoss: tensor(0.3217, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 132 \tLoss: tensor(0.3180, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 133 \tLoss: tensor(0.3165, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 134 \tLoss: tensor(0.3134, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 135 \tLoss: tensor(0.3116, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 136 \tLoss: tensor(0.3091, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 137 \tLoss: tensor(0.3053, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 138 \tLoss: tensor(0.3030, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 139 \tLoss: tensor(0.3003, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 140 \tLoss: tensor(0.2984, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 141 \tLoss: tensor(0.2954, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 142 \tLoss: tensor(0.2933, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 143 \tLoss: tensor(0.2900, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 144 \tLoss: tensor(0.2885, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 145 \tLoss: tensor(0.2868, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 146 \tLoss: tensor(0.2839, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 147 \tLoss: tensor(0.2820, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 148 \tLoss: tensor(0.2794, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 149 \tLoss: tensor(0.2775, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 150 \tLoss: tensor(0.2752, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 151 \tLoss: tensor(0.2745, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 152 \tLoss: tensor(0.2707, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 153 \tLoss: tensor(0.2696, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 154 \tLoss: tensor(0.2678, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 155 \tLoss: tensor(0.2654, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 156 \tLoss: tensor(0.2637, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 157 \tLoss: tensor(0.2617, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 158 \tLoss: tensor(0.2598, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 159 \tLoss: tensor(0.2575, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 160 \tLoss: tensor(0.2562, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 161 \tLoss: tensor(0.2535, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 162 \tLoss: tensor(0.2526, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 163 \tLoss: tensor(0.2497, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 164 \tLoss: tensor(0.2531, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 165 \tLoss: tensor(0.2460, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 166 \tLoss: tensor(0.2443, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 167 \tLoss: tensor(0.2436, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 168 \tLoss: tensor(0.2414, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 169 \tLoss: tensor(0.2409, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 170 \tLoss: tensor(0.2381, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 171 \tLoss: tensor(0.2371, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 172 \tLoss: tensor(0.2351, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 173 \tLoss: tensor(0.2342, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 174 \tLoss: tensor(0.2317, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 175 \tLoss: tensor(0.2307, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 176 \tLoss: tensor(0.2295, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 177 \tLoss: tensor(0.2269, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 178 \tLoss: tensor(0.2256, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 179 \tLoss: tensor(0.2239, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 180 \tLoss: tensor(0.2231, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 181 \tLoss: tensor(0.2212, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 182 \tLoss: tensor(0.2195, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 183 \tLoss: tensor(0.2187, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 184 \tLoss: tensor(0.2171, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 185 \tLoss: tensor(0.2151, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 186 \tLoss: tensor(0.2142, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 187 \tLoss: tensor(0.2134, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 188 \tLoss: tensor(0.2118, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 189 \tLoss: tensor(0.2102, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 190 \tLoss: tensor(0.2084, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 191 \tLoss: tensor(0.2080, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 192 \tLoss: tensor(0.2057, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 193 \tLoss: tensor(0.2053, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 194 \tLoss: tensor(0.2037, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 195 \tLoss: tensor(0.2026, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 196 \tLoss: tensor(0.2016, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 197 \tLoss: tensor(0.2004, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 198 \tLoss: tensor(0.1991, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 199 \tLoss: tensor(0.1971, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 200 \tLoss: tensor(0.1967, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 201 \tLoss: tensor(0.1952, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 202 \tLoss: tensor(0.1945, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 203 \tLoss: tensor(0.1935, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 204 \tLoss: tensor(0.1921, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 205 \tLoss: tensor(0.1910, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 206 \tLoss: tensor(0.1896, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 207 \tLoss: tensor(0.1888, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 208 \tLoss: tensor(0.1875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 209 \tLoss: tensor(0.1867, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 210 \tLoss: tensor(0.1856, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 211 \tLoss: tensor(0.1844, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 212 \tLoss: tensor(0.1838, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 213 \tLoss: tensor(0.1821, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 214 \tLoss: tensor(0.1816, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 215 \tLoss: tensor(0.1802, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 216 \tLoss: tensor(0.1793, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 217 \tLoss: tensor(0.1784, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 218 \tLoss: tensor(0.1774, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 219 \tLoss: tensor(0.1766, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 220 \tLoss: tensor(0.1755, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 221 \tLoss: tensor(0.1751, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 222 \tLoss: tensor(0.1736, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 223 \tLoss: tensor(0.1731, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 224 \tLoss: tensor(0.1720, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 225 \tLoss: tensor(0.1711, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 226 \tLoss: tensor(0.1700, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 227 \tLoss: tensor(0.1695, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 228 \tLoss: tensor(0.1685, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 229 \tLoss: tensor(0.1674, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 230 \tLoss: tensor(0.1666, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 231 \tLoss: tensor(0.1654, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 232 \tLoss: tensor(0.1647, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 233 \tLoss: tensor(0.1637, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 234 \tLoss: tensor(0.1629, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 235 \tLoss: tensor(0.1622, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 236 \tLoss: tensor(0.1612, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 237 \tLoss: tensor(0.1604, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 238 \tLoss: tensor(0.1597, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 239 \tLoss: tensor(0.1587, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 240 \tLoss: tensor(0.1579, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 241 \tLoss: tensor(0.1574, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 242 \tLoss: tensor(0.1561, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 243 \tLoss: tensor(0.1560, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 244 \tLoss: tensor(0.1551, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 245 \tLoss: tensor(0.1540, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 246 \tLoss: tensor(0.1537, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 247 \tLoss: tensor(0.1526, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 248 \tLoss: tensor(0.1520, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 249 \tLoss: tensor(0.1511, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 250 \tLoss: tensor(0.1506, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 251 \tLoss: tensor(0.1497, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 252 \tLoss: tensor(0.1492, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 253 \tLoss: tensor(0.1485, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 254 \tLoss: tensor(0.1478, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 255 \tLoss: tensor(0.1472, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 256 \tLoss: tensor(0.1464, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 257 \tLoss: tensor(0.1460, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 258 \tLoss: tensor(0.1449, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 259 \tLoss: tensor(0.1444, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 260 \tLoss: tensor(0.1438, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 261 \tLoss: tensor(0.1432, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 262 \tLoss: tensor(0.1427, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 263 \tLoss: tensor(0.1418, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 264 \tLoss: tensor(0.1414, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 265 \tLoss: tensor(0.1405, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 266 \tLoss: tensor(0.1401, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 267 \tLoss: tensor(0.1396, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 268 \tLoss: tensor(0.1388, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 269 \tLoss: tensor(0.1384, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 270 \tLoss: tensor(0.1378, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 271 \tLoss: tensor(0.1372, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 272 \tLoss: tensor(0.1366, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 273 \tLoss: tensor(0.1359, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 274 \tLoss: tensor(0.1353, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 275 \tLoss: tensor(0.1350, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 276 \tLoss: tensor(0.1340, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 277 \tLoss: tensor(0.1337, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 278 \tLoss: tensor(0.1334, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 279 \tLoss: tensor(0.1324, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 280 \tLoss: tensor(0.1324, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 281 \tLoss: tensor(0.1315, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 282 \tLoss: tensor(0.1309, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 283 \tLoss: tensor(0.1303, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 284 \tLoss: tensor(0.1299, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 285 \tLoss: tensor(0.1291, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 286 \tLoss: tensor(0.1288, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 287 \tLoss: tensor(0.1283, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 288 \tLoss: tensor(0.1279, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 289 \tLoss: tensor(0.1275, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 290 \tLoss: tensor(0.1267, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 291 \tLoss: tensor(0.1262, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 292 \tLoss: tensor(0.1258, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 293 \tLoss: tensor(0.1252, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 294 \tLoss: tensor(0.1248, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 295 \tLoss: tensor(0.1240, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 296 \tLoss: tensor(0.1236, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 297 \tLoss: tensor(0.1234, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 298 \tLoss: tensor(0.1232, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 299 \tLoss: tensor(0.1225, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 300 \tLoss: tensor(0.1224, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 301 \tLoss: tensor(0.1217, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 302 \tLoss: tensor(0.1214, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 303 \tLoss: tensor(0.1209, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 304 \tLoss: tensor(0.1203, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 305 \tLoss: tensor(0.1197, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 306 \tLoss: tensor(0.1193, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 307 \tLoss: tensor(0.1189, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 308 \tLoss: tensor(0.1185, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 309 \tLoss: tensor(0.1180, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 310 \tLoss: tensor(0.1174, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 311 \tLoss: tensor(0.1169, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 312 \tLoss: tensor(0.1164, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 313 \tLoss: tensor(0.1160, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 314 \tLoss: tensor(0.1155, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 315 \tLoss: tensor(0.1152, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 316 \tLoss: tensor(0.1145, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 317 \tLoss: tensor(0.1143, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 318 \tLoss: tensor(0.1138, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 319 \tLoss: tensor(0.1133, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 320 \tLoss: tensor(0.1132, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 321 \tLoss: tensor(0.1125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 322 \tLoss: tensor(0.1123, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 323 \tLoss: tensor(0.1116, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 324 \tLoss: tensor(0.1113, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 325 \tLoss: tensor(0.1109, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 326 \tLoss: tensor(0.1106, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 327 \tLoss: tensor(0.1101, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 328 \tLoss: tensor(0.1098, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 329 \tLoss: tensor(0.1094, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 330 \tLoss: tensor(0.1090, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 331 \tLoss: tensor(0.1087, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 332 \tLoss: tensor(0.1081, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 333 \tLoss: tensor(0.1081, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 334 \tLoss: tensor(0.1075, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 335 \tLoss: tensor(0.1072, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 336 \tLoss: tensor(0.1067, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 337 \tLoss: tensor(0.1064, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 338 \tLoss: tensor(0.1060, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 339 \tLoss: tensor(0.1058, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 340 \tLoss: tensor(0.1055, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 341 \tLoss: tensor(0.1050, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 342 \tLoss: tensor(0.1046, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 343 \tLoss: tensor(0.1042, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 344 \tLoss: tensor(0.1040, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 345 \tLoss: tensor(0.1036, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 346 \tLoss: tensor(0.1033, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 347 \tLoss: tensor(0.1029, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 348 \tLoss: tensor(0.1026, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 349 \tLoss: tensor(0.1022, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 350 \tLoss: tensor(0.1020, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 351 \tLoss: tensor(0.1016, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 352 \tLoss: tensor(0.1013, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 353 \tLoss: tensor(0.1010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 354 \tLoss: tensor(0.1005, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 355 \tLoss: tensor(0.1004, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 356 \tLoss: tensor(0.0997, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 357 \tLoss: tensor(0.0997, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 358 \tLoss: tensor(0.0992, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 359 \tLoss: tensor(0.0990, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 360 \tLoss: tensor(0.0986, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 361 \tLoss: tensor(0.0983, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 362 \tLoss: tensor(0.0980, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 363 \tLoss: tensor(0.0978, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 364 \tLoss: tensor(0.0975, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 365 \tLoss: tensor(0.0972, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 366 \tLoss: tensor(0.0969, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 367 \tLoss: tensor(0.0966, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 368 \tLoss: tensor(0.0963, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 369 \tLoss: tensor(0.0959, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 370 \tLoss: tensor(0.0957, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 371 \tLoss: tensor(0.0953, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 372 \tLoss: tensor(0.0952, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 373 \tLoss: tensor(0.0948, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 374 \tLoss: tensor(0.0946, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 375 \tLoss: tensor(0.0941, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 376 \tLoss: tensor(0.0941, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 377 \tLoss: tensor(0.0937, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 378 \tLoss: tensor(0.0934, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 379 \tLoss: tensor(0.0933, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 380 \tLoss: tensor(0.0928, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 381 \tLoss: tensor(0.0927, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 382 \tLoss: tensor(0.0922, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 383 \tLoss: tensor(0.0919, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 384 \tLoss: tensor(0.0916, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 385 \tLoss: tensor(0.0915, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 386 \tLoss: tensor(0.0911, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 387 \tLoss: tensor(0.0910, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 388 \tLoss: tensor(0.0906, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 389 \tLoss: tensor(0.0903, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 390 \tLoss: tensor(0.0899, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 391 \tLoss: tensor(0.0898, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 392 \tLoss: tensor(0.0896, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 393 \tLoss: tensor(0.0893, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 394 \tLoss: tensor(0.0890, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 395 \tLoss: tensor(0.0886, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 396 \tLoss: tensor(0.0885, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 397 \tLoss: tensor(0.0881, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 398 \tLoss: tensor(0.0879, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 399 \tLoss: tensor(0.0877, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 400 \tLoss: tensor(0.0875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 401 \tLoss: tensor(0.0871, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 402 \tLoss: tensor(0.0869, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 403 \tLoss: tensor(0.0866, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 404 \tLoss: tensor(0.0864, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 405 \tLoss: tensor(0.0861, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 406 \tLoss: tensor(0.0859, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 407 \tLoss: tensor(0.0857, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 408 \tLoss: tensor(0.0854, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 409 \tLoss: tensor(0.0852, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 410 \tLoss: tensor(0.0849, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 411 \tLoss: tensor(0.0847, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 412 \tLoss: tensor(0.0847, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 413 \tLoss: tensor(0.0845, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 414 \tLoss: tensor(0.0841, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 415 \tLoss: tensor(0.0840, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 416 \tLoss: tensor(0.0837, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 417 \tLoss: tensor(0.0835, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 418 \tLoss: tensor(0.0833, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 419 \tLoss: tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 420 \tLoss: tensor(0.0829, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 421 \tLoss: tensor(0.0827, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 422 \tLoss: tensor(0.0823, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 423 \tLoss: tensor(0.0822, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 424 \tLoss: tensor(0.0819, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 425 \tLoss: tensor(0.0817, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 426 \tLoss: tensor(0.0814, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 427 \tLoss: tensor(0.0813, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 428 \tLoss: tensor(0.0808, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 429 \tLoss: tensor(0.0807, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 430 \tLoss: tensor(0.0805, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 431 \tLoss: tensor(0.0802, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 432 \tLoss: tensor(0.0801, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 433 \tLoss: tensor(0.0798, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 434 \tLoss: tensor(0.0796, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 435 \tLoss: tensor(0.0793, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 436 \tLoss: tensor(0.0791, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 437 \tLoss: tensor(0.0788, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 438 \tLoss: tensor(0.0785, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 439 \tLoss: tensor(0.0784, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 440 \tLoss: tensor(0.0782, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 441 \tLoss: tensor(0.0779, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 442 \tLoss: tensor(0.0777, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 443 \tLoss: tensor(0.0775, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 444 \tLoss: tensor(0.0773, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 445 \tLoss: tensor(0.0770, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 446 \tLoss: tensor(0.0770, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 447 \tLoss: tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 448 \tLoss: tensor(0.0766, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 449 \tLoss: tensor(0.0763, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 450 \tLoss: tensor(0.0762, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 451 \tLoss: tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 452 \tLoss: tensor(0.0757, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 453 \tLoss: tensor(0.0757, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 454 \tLoss: tensor(0.0754, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 455 \tLoss: tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 456 \tLoss: tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 457 \tLoss: tensor(0.0748, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 458 \tLoss: tensor(0.0745, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 459 \tLoss: tensor(0.0744, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 460 \tLoss: tensor(0.0743, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 461 \tLoss: tensor(0.0741, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 462 \tLoss: tensor(0.0739, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 463 \tLoss: tensor(0.0737, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 464 \tLoss: tensor(0.0735, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 465 \tLoss: tensor(0.0733, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 466 \tLoss: tensor(0.0731, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 467 \tLoss: tensor(0.0731, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 468 \tLoss: tensor(0.0728, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 469 \tLoss: tensor(0.0727, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 470 \tLoss: tensor(0.0725, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 471 \tLoss: tensor(0.0723, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 472 \tLoss: tensor(0.0721, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 473 \tLoss: tensor(0.0719, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 474 \tLoss: tensor(0.0718, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 475 \tLoss: tensor(0.0717, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 476 \tLoss: tensor(0.0714, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 477 \tLoss: tensor(0.0713, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 478 \tLoss: tensor(0.0710, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 479 \tLoss: tensor(0.0710, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 480 \tLoss: tensor(0.0707, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 481 \tLoss: tensor(0.0706, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 482 \tLoss: tensor(0.0704, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 483 \tLoss: tensor(0.0702, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 484 \tLoss: tensor(0.0701, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 485 \tLoss: tensor(0.0699, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 486 \tLoss: tensor(0.0697, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 487 \tLoss: tensor(0.0696, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 488 \tLoss: tensor(0.0694, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 489 \tLoss: tensor(0.0694, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 490 \tLoss: tensor(0.0691, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 491 \tLoss: tensor(0.0690, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 492 \tLoss: tensor(0.0688, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 493 \tLoss: tensor(0.0686, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 494 \tLoss: tensor(0.0685, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 495 \tLoss: tensor(0.0684, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 496 \tLoss: tensor(0.0682, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 497 \tLoss: tensor(0.0681, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 498 \tLoss: tensor(0.0679, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 499 \tLoss: tensor(0.0678, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 500 \tLoss: tensor(0.0675, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 501 \tLoss: tensor(0.0674, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 502 \tLoss: tensor(0.0673, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 503 \tLoss: tensor(0.0672, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 504 \tLoss: tensor(0.0671, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 505 \tLoss: tensor(0.0669, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 506 \tLoss: tensor(0.0668, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 507 \tLoss: tensor(0.0665, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 508 \tLoss: tensor(0.0664, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 509 \tLoss: tensor(0.0663, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 510 \tLoss: tensor(0.0662, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 511 \tLoss: tensor(0.0660, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 512 \tLoss: tensor(0.0658, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 513 \tLoss: tensor(0.0657, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 514 \tLoss: tensor(0.0656, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 515 \tLoss: tensor(0.0654, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 516 \tLoss: tensor(0.0653, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 517 \tLoss: tensor(0.0652, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 518 \tLoss: tensor(0.0650, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 519 \tLoss: tensor(0.0649, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 520 \tLoss: tensor(0.0647, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 521 \tLoss: tensor(0.0646, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 522 \tLoss: tensor(0.0644, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 523 \tLoss: tensor(0.0644, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 524 \tLoss: tensor(0.0642, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 525 \tLoss: tensor(0.0640, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 526 \tLoss: tensor(0.0640, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 527 \tLoss: tensor(0.0637, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 528 \tLoss: tensor(0.0637, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 529 \tLoss: tensor(0.0634, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 530 \tLoss: tensor(0.0634, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 531 \tLoss: tensor(0.0632, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 532 \tLoss: tensor(0.0631, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 533 \tLoss: tensor(0.0629, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 534 \tLoss: tensor(0.0629, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 535 \tLoss: tensor(0.0626, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 536 \tLoss: tensor(0.0626, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 537 \tLoss: tensor(0.0624, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 538 \tLoss: tensor(0.0624, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 539 \tLoss: tensor(0.0622, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 540 \tLoss: tensor(0.0621, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 541 \tLoss: tensor(0.0619, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 542 \tLoss: tensor(0.0618, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 543 \tLoss: tensor(0.0617, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 544 \tLoss: tensor(0.0615, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 545 \tLoss: tensor(0.0615, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 546 \tLoss: tensor(0.0614, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 547 \tLoss: tensor(0.0611, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 548 \tLoss: tensor(0.0611, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 549 \tLoss: tensor(0.0609, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 550 \tLoss: tensor(0.0608, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 551 \tLoss: tensor(0.0607, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 552 \tLoss: tensor(0.0605, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 553 \tLoss: tensor(0.0605, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 554 \tLoss: tensor(0.0603, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 555 \tLoss: tensor(0.0602, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 556 \tLoss: tensor(0.0600, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 557 \tLoss: tensor(0.0599, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 558 \tLoss: tensor(0.0598, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 559 \tLoss: tensor(0.0597, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 560 \tLoss: tensor(0.0596, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 561 \tLoss: tensor(0.0595, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 562 \tLoss: tensor(0.0593, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 563 \tLoss: tensor(0.0593, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 564 \tLoss: tensor(0.0591, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 565 \tLoss: tensor(0.0591, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 566 \tLoss: tensor(0.0588, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 567 \tLoss: tensor(0.0589, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 568 \tLoss: tensor(0.0587, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 569 \tLoss: tensor(0.0586, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 570 \tLoss: tensor(0.0584, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 571 \tLoss: tensor(0.0584, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 572 \tLoss: tensor(0.0583, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 573 \tLoss: tensor(0.0583, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 574 \tLoss: tensor(0.0581, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 575 \tLoss: tensor(0.0582, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 576 \tLoss: tensor(0.0580, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 577 \tLoss: tensor(0.0579, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 578 \tLoss: tensor(0.0578, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 579 \tLoss: tensor(0.0577, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 580 \tLoss: tensor(0.0576, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 581 \tLoss: tensor(0.0575, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 582 \tLoss: tensor(0.0574, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 583 \tLoss: tensor(0.0573, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 584 \tLoss: tensor(0.0572, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 585 \tLoss: tensor(0.0570, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 586 \tLoss: tensor(0.0569, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 587 \tLoss: tensor(0.0568, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 588 \tLoss: tensor(0.0567, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 589 \tLoss: tensor(0.0566, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 590 \tLoss: tensor(0.0565, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 591 \tLoss: tensor(0.0564, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 592 \tLoss: tensor(0.0562, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 593 \tLoss: tensor(0.0561, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 594 \tLoss: tensor(0.0560, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 595 \tLoss: tensor(0.0558, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 596 \tLoss: tensor(0.0557, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 597 \tLoss: tensor(0.0555, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 598 \tLoss: tensor(0.0554, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 599 \tLoss: tensor(0.0553, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 600 \tLoss: tensor(0.0552, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 601 \tLoss: tensor(0.0551, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 602 \tLoss: tensor(0.0550, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 603 \tLoss: tensor(0.0549, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 604 \tLoss: tensor(0.0548, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 605 \tLoss: tensor(0.0546, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 606 \tLoss: tensor(0.0546, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 607 \tLoss: tensor(0.0544, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 608 \tLoss: tensor(0.0544, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 609 \tLoss: tensor(0.0542, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 610 \tLoss: tensor(0.0541, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 611 \tLoss: tensor(0.0540, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 612 \tLoss: tensor(0.0539, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 613 \tLoss: tensor(0.0538, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 614 \tLoss: tensor(0.0538, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 615 \tLoss: tensor(0.0536, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 616 \tLoss: tensor(0.0536, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 617 \tLoss: tensor(0.0534, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 618 \tLoss: tensor(0.0534, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 619 \tLoss: tensor(0.0533, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 620 \tLoss: tensor(0.0532, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 621 \tLoss: tensor(0.0531, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 622 \tLoss: tensor(0.0530, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 623 \tLoss: tensor(0.0529, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 624 \tLoss: tensor(0.0528, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 625 \tLoss: tensor(0.0527, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 626 \tLoss: tensor(0.0526, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 627 \tLoss: tensor(0.0525, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 628 \tLoss: tensor(0.0524, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 629 \tLoss: tensor(0.0523, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 630 \tLoss: tensor(0.0522, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 631 \tLoss: tensor(0.0522, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 632 \tLoss: tensor(0.0521, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 633 \tLoss: tensor(0.0520, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 634 \tLoss: tensor(0.0519, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 635 \tLoss: tensor(0.0518, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 636 \tLoss: tensor(0.0517, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 637 \tLoss: tensor(0.0516, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 638 \tLoss: tensor(0.0516, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 639 \tLoss: tensor(0.0515, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 640 \tLoss: tensor(0.0514, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 641 \tLoss: tensor(0.0514, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 642 \tLoss: tensor(0.0512, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 643 \tLoss: tensor(0.0512, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 644 \tLoss: tensor(0.0511, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 645 \tLoss: tensor(0.0510, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 646 \tLoss: tensor(0.0509, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 647 \tLoss: tensor(0.0508, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 648 \tLoss: tensor(0.0507, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 649 \tLoss: tensor(0.0506, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 650 \tLoss: tensor(0.0505, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 651 \tLoss: tensor(0.0505, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 652 \tLoss: tensor(0.0504, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 653 \tLoss: tensor(0.0503, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 654 \tLoss: tensor(0.0502, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 655 \tLoss: tensor(0.0501, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 656 \tLoss: tensor(0.0501, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 657 \tLoss: tensor(0.0499, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 658 \tLoss: tensor(0.0499, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 659 \tLoss: tensor(0.0498, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 660 \tLoss: tensor(0.0497, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 661 \tLoss: tensor(0.0496, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 662 \tLoss: tensor(0.0495, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 663 \tLoss: tensor(0.0494, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 664 \tLoss: tensor(0.0494, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 665 \tLoss: tensor(0.0493, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 666 \tLoss: tensor(0.0493, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 667 \tLoss: tensor(0.0491, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 668 \tLoss: tensor(0.0491, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 669 \tLoss: tensor(0.0490, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 670 \tLoss: tensor(0.0489, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 671 \tLoss: tensor(0.0489, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 672 \tLoss: tensor(0.0488, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 673 \tLoss: tensor(0.0487, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 674 \tLoss: tensor(0.0486, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 675 \tLoss: tensor(0.0485, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 676 \tLoss: tensor(0.0485, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 677 \tLoss: tensor(0.0484, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 678 \tLoss: tensor(0.0483, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 679 \tLoss: tensor(0.0483, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 680 \tLoss: tensor(0.0481, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 681 \tLoss: tensor(0.0481, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 682 \tLoss: tensor(0.0480, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 683 \tLoss: tensor(0.0479, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 684 \tLoss: tensor(0.0479, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 685 \tLoss: tensor(0.0478, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 686 \tLoss: tensor(0.0477, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 687 \tLoss: tensor(0.0477, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 688 \tLoss: tensor(0.0476, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 689 \tLoss: tensor(0.0475, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 690 \tLoss: tensor(0.0474, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 691 \tLoss: tensor(0.0473, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 692 \tLoss: tensor(0.0473, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 693 \tLoss: tensor(0.0472, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 694 \tLoss: tensor(0.0471, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 695 \tLoss: tensor(0.0470, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 696 \tLoss: tensor(0.0470, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 697 \tLoss: tensor(0.0469, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 698 \tLoss: tensor(0.0468, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 699 \tLoss: tensor(0.0468, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 700 \tLoss: tensor(0.0467, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 701 \tLoss: tensor(0.0466, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 702 \tLoss: tensor(0.0465, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 703 \tLoss: tensor(0.0465, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 704 \tLoss: tensor(0.0464, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 705 \tLoss: tensor(0.0463, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 706 \tLoss: tensor(0.0463, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 707 \tLoss: tensor(0.0462, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 708 \tLoss: tensor(0.0461, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 709 \tLoss: tensor(0.0460, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 710 \tLoss: tensor(0.0460, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 711 \tLoss: tensor(0.0459, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 712 \tLoss: tensor(0.0458, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 713 \tLoss: tensor(0.0458, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 714 \tLoss: tensor(0.0457, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 715 \tLoss: tensor(0.0457, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 716 \tLoss: tensor(0.0455, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 717 \tLoss: tensor(0.0455, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 718 \tLoss: tensor(0.0454, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 719 \tLoss: tensor(0.0454, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 720 \tLoss: tensor(0.0453, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 721 \tLoss: tensor(0.0452, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 722 \tLoss: tensor(0.0452, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 723 \tLoss: tensor(0.0452, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 724 \tLoss: tensor(0.0451, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 725 \tLoss: tensor(0.0451, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 726 \tLoss: tensor(0.0450, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 727 \tLoss: tensor(0.0449, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 728 \tLoss: tensor(0.0449, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 729 \tLoss: tensor(0.0448, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 730 \tLoss: tensor(0.0447, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 731 \tLoss: tensor(0.0446, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 732 \tLoss: tensor(0.0446, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 733 \tLoss: tensor(0.0445, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 734 \tLoss: tensor(0.0444, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 735 \tLoss: tensor(0.0444, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 736 \tLoss: tensor(0.0443, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 737 \tLoss: tensor(0.0442, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 738 \tLoss: tensor(0.0442, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 739 \tLoss: tensor(0.0441, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 740 \tLoss: tensor(0.0440, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 741 \tLoss: tensor(0.0440, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 742 \tLoss: tensor(0.0439, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 743 \tLoss: tensor(0.0438, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 744 \tLoss: tensor(0.0437, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 745 \tLoss: tensor(0.0437, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 746 \tLoss: tensor(0.0436, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 747 \tLoss: tensor(0.0436, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 748 \tLoss: tensor(0.0435, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 749 \tLoss: tensor(0.0435, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 750 \tLoss: tensor(0.0434, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 751 \tLoss: tensor(0.0434, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 752 \tLoss: tensor(0.0433, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 753 \tLoss: tensor(0.0432, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 754 \tLoss: tensor(0.0432, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 755 \tLoss: tensor(0.0431, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 756 \tLoss: tensor(0.0431, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 757 \tLoss: tensor(0.0430, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 758 \tLoss: tensor(0.0429, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 759 \tLoss: tensor(0.0428, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 760 \tLoss: tensor(0.0428, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 761 \tLoss: tensor(0.0427, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 762 \tLoss: tensor(0.0427, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 763 \tLoss: tensor(0.0426, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 764 \tLoss: tensor(0.0426, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 765 \tLoss: tensor(0.0425, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 766 \tLoss: tensor(0.0425, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 767 \tLoss: tensor(0.0424, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 768 \tLoss: tensor(0.0424, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 769 \tLoss: tensor(0.0423, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 770 \tLoss: tensor(0.0422, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 771 \tLoss: tensor(0.0422, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 772 \tLoss: tensor(0.0421, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 773 \tLoss: tensor(0.0421, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 774 \tLoss: tensor(0.0420, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 775 \tLoss: tensor(0.0420, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 776 \tLoss: tensor(0.0420, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 777 \tLoss: tensor(0.0420, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 778 \tLoss: tensor(0.0419, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 779 \tLoss: tensor(0.0419, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 780 \tLoss: tensor(0.0419, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 781 \tLoss: tensor(0.0418, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 782 \tLoss: tensor(0.0418, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 783 \tLoss: tensor(0.0417, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 784 \tLoss: tensor(0.0417, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 785 \tLoss: tensor(0.0416, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 786 \tLoss: tensor(0.0416, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 787 \tLoss: tensor(0.0415, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 788 \tLoss: tensor(0.0415, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 789 \tLoss: tensor(0.0414, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 790 \tLoss: tensor(0.0414, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 791 \tLoss: tensor(0.0413, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 792 \tLoss: tensor(0.0412, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 793 \tLoss: tensor(0.0412, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 794 \tLoss: tensor(0.0411, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 795 \tLoss: tensor(0.0411, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 796 \tLoss: tensor(0.0410, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 797 \tLoss: tensor(0.0410, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 798 \tLoss: tensor(0.0409, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 799 \tLoss: tensor(0.0408, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 800 \tLoss: tensor(0.0408, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 801 \tLoss: tensor(0.0407, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 802 \tLoss: tensor(0.0407, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 803 \tLoss: tensor(0.0406, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 804 \tLoss: tensor(0.0405, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 805 \tLoss: tensor(0.0405, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 806 \tLoss: tensor(0.0404, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 807 \tLoss: tensor(0.0403, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 808 \tLoss: tensor(0.0403, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 809 \tLoss: tensor(0.0402, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 810 \tLoss: tensor(0.0402, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 811 \tLoss: tensor(0.0401, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 812 \tLoss: tensor(0.0401, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 813 \tLoss: tensor(0.0400, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 814 \tLoss: tensor(0.0399, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 815 \tLoss: tensor(0.0399, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 816 \tLoss: tensor(0.0398, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 817 \tLoss: tensor(0.0398, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 818 \tLoss: tensor(0.0397, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 819 \tLoss: tensor(0.0397, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 820 \tLoss: tensor(0.0396, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 821 \tLoss: tensor(0.0396, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 822 \tLoss: tensor(0.0395, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 823 \tLoss: tensor(0.0395, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 824 \tLoss: tensor(0.0394, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 825 \tLoss: tensor(0.0393, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 826 \tLoss: tensor(0.0393, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 827 \tLoss: tensor(0.0392, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 828 \tLoss: tensor(0.0392, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 829 \tLoss: tensor(0.0391, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 830 \tLoss: tensor(0.0390, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 831 \tLoss: tensor(0.0390, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 832 \tLoss: tensor(0.0389, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 833 \tLoss: tensor(0.0389, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 834 \tLoss: tensor(0.0388, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 835 \tLoss: tensor(0.0388, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 836 \tLoss: tensor(0.0387, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 837 \tLoss: tensor(0.0387, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 838 \tLoss: tensor(0.0386, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 839 \tLoss: tensor(0.0386, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 840 \tLoss: tensor(0.0385, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 841 \tLoss: tensor(0.0385, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 842 \tLoss: tensor(0.0384, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 843 \tLoss: tensor(0.0384, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 844 \tLoss: tensor(0.0383, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 845 \tLoss: tensor(0.0383, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 846 \tLoss: tensor(0.0382, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 847 \tLoss: tensor(0.0382, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 848 \tLoss: tensor(0.0381, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 849 \tLoss: tensor(0.0381, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 850 \tLoss: tensor(0.0380, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 851 \tLoss: tensor(0.0380, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 852 \tLoss: tensor(0.0380, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 853 \tLoss: tensor(0.0379, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 854 \tLoss: tensor(0.0379, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 855 \tLoss: tensor(0.0378, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 856 \tLoss: tensor(0.0378, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 857 \tLoss: tensor(0.0377, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 858 \tLoss: tensor(0.0377, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 859 \tLoss: tensor(0.0376, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 860 \tLoss: tensor(0.0376, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 861 \tLoss: tensor(0.0375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 862 \tLoss: tensor(0.0375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 863 \tLoss: tensor(0.0374, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 864 \tLoss: tensor(0.0374, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 865 \tLoss: tensor(0.0374, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 866 \tLoss: tensor(0.0373, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 867 \tLoss: tensor(0.0373, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 868 \tLoss: tensor(0.0372, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 869 \tLoss: tensor(0.0372, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 870 \tLoss: tensor(0.0372, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 871 \tLoss: tensor(0.0371, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 872 \tLoss: tensor(0.0371, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 873 \tLoss: tensor(0.0370, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 874 \tLoss: tensor(0.0370, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 875 \tLoss: tensor(0.0369, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 876 \tLoss: tensor(0.0369, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 877 \tLoss: tensor(0.0369, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 878 \tLoss: tensor(0.0368, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 879 \tLoss: tensor(0.0368, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 880 \tLoss: tensor(0.0367, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 881 \tLoss: tensor(0.0367, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 882 \tLoss: tensor(0.0366, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 883 \tLoss: tensor(0.0366, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 884 \tLoss: tensor(0.0365, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 885 \tLoss: tensor(0.0365, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 886 \tLoss: tensor(0.0365, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 887 \tLoss: tensor(0.0364, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 888 \tLoss: tensor(0.0364, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 889 \tLoss: tensor(0.0363, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 890 \tLoss: tensor(0.0363, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 891 \tLoss: tensor(0.0363, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 892 \tLoss: tensor(0.0363, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 893 \tLoss: tensor(0.0362, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 894 \tLoss: tensor(0.0362, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 895 \tLoss: tensor(0.0361, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 896 \tLoss: tensor(0.0361, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 897 \tLoss: tensor(0.0361, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 898 \tLoss: tensor(0.0360, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 899 \tLoss: tensor(0.0360, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 900 \tLoss: tensor(0.0359, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 901 \tLoss: tensor(0.0359, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 902 \tLoss: tensor(0.0359, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 903 \tLoss: tensor(0.0358, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 904 \tLoss: tensor(0.0358, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 905 \tLoss: tensor(0.0358, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 906 \tLoss: tensor(0.0357, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 907 \tLoss: tensor(0.0357, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 908 \tLoss: tensor(0.0356, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 909 \tLoss: tensor(0.0356, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 910 \tLoss: tensor(0.0356, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 911 \tLoss: tensor(0.0355, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 912 \tLoss: tensor(0.0355, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 913 \tLoss: tensor(0.0354, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 914 \tLoss: tensor(0.0354, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 915 \tLoss: tensor(0.0353, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 916 \tLoss: tensor(0.0353, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 917 \tLoss: tensor(0.0353, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 918 \tLoss: tensor(0.0352, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 919 \tLoss: tensor(0.0352, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 920 \tLoss: tensor(0.0352, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 921 \tLoss: tensor(0.0351, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 922 \tLoss: tensor(0.0351, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 923 \tLoss: tensor(0.0351, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 924 \tLoss: tensor(0.0350, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 925 \tLoss: tensor(0.0350, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 926 \tLoss: tensor(0.0349, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 927 \tLoss: tensor(0.0349, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 928 \tLoss: tensor(0.0349, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 929 \tLoss: tensor(0.0348, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 930 \tLoss: tensor(0.0348, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 931 \tLoss: tensor(0.0348, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 932 \tLoss: tensor(0.0347, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 933 \tLoss: tensor(0.0347, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 934 \tLoss: tensor(0.0347, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 935 \tLoss: tensor(0.0346, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 936 \tLoss: tensor(0.0346, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 937 \tLoss: tensor(0.0345, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 938 \tLoss: tensor(0.0345, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 939 \tLoss: tensor(0.0345, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 940 \tLoss: tensor(0.0344, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 941 \tLoss: tensor(0.0344, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 942 \tLoss: tensor(0.0344, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 943 \tLoss: tensor(0.0343, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 944 \tLoss: tensor(0.0343, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 945 \tLoss: tensor(0.0342, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 946 \tLoss: tensor(0.0342, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 947 \tLoss: tensor(0.0341, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 948 \tLoss: tensor(0.0341, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 949 \tLoss: tensor(0.0341, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 950 \tLoss: tensor(0.0340, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 951 \tLoss: tensor(0.0340, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 952 \tLoss: tensor(0.0340, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 953 \tLoss: tensor(0.0339, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 954 \tLoss: tensor(0.0339, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 955 \tLoss: tensor(0.0339, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 956 \tLoss: tensor(0.0338, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 957 \tLoss: tensor(0.0338, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 958 \tLoss: tensor(0.0337, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 959 \tLoss: tensor(0.0337, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 960 \tLoss: tensor(0.0337, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 961 \tLoss: tensor(0.0336, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 962 \tLoss: tensor(0.0336, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 963 \tLoss: tensor(0.0336, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 964 \tLoss: tensor(0.0335, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 965 \tLoss: tensor(0.0335, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 966 \tLoss: tensor(0.0335, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 967 \tLoss: tensor(0.0334, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 968 \tLoss: tensor(0.0334, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 969 \tLoss: tensor(0.0334, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 970 \tLoss: tensor(0.0333, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 971 \tLoss: tensor(0.0333, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 972 \tLoss: tensor(0.0332, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 973 \tLoss: tensor(0.0333, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 974 \tLoss: tensor(0.0332, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 975 \tLoss: tensor(0.0332, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 976 \tLoss: tensor(0.0331, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 977 \tLoss: tensor(0.0331, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 978 \tLoss: tensor(0.0331, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 979 \tLoss: tensor(0.0330, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 980 \tLoss: tensor(0.0330, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 981 \tLoss: tensor(0.0330, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 982 \tLoss: tensor(0.0329, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 983 \tLoss: tensor(0.0329, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 984 \tLoss: tensor(0.0329, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 985 \tLoss: tensor(0.0328, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 986 \tLoss: tensor(0.0328, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 987 \tLoss: tensor(0.0328, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 988 \tLoss: tensor(0.0327, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 989 \tLoss: tensor(0.0327, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 990 \tLoss: tensor(0.0327, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 991 \tLoss: tensor(0.0326, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 992 \tLoss: tensor(0.0326, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 993 \tLoss: tensor(0.0326, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 994 \tLoss: tensor(0.0325, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 995 \tLoss: tensor(0.0325, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 996 \tLoss: tensor(0.0325, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 997 \tLoss: tensor(0.0324, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 998 \tLoss: tensor(0.0324, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch: 999 \tLoss: tensor(0.0324, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 1000):\n",
    "    loss = 0\n",
    "    i = 0\n",
    "    X,Y = generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size)\n",
    "    for x, y in zip(X,Y):\n",
    "        i += 1\n",
    "        optimizer.zero_grad()\n",
    "        log_probs = model(x[0])\n",
    "        loss = loss_function(log_probs,torch.Tensor([y]).long().cuda())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss += loss.data\n",
    "    print('Epoch:', epoch, '\\tLoss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "KDeourI1pfuw"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.949060</td>\n",
       "      <td>0.601485</td>\n",
       "      <td>-0.046146</td>\n",
       "      <td>0.622524</td>\n",
       "      <td>-0.912017</td>\n",
       "      <td>-0.862392</td>\n",
       "      <td>0.208219</td>\n",
       "      <td>-2.355533</td>\n",
       "      <td>-0.207395</td>\n",
       "      <td>-0.069587</td>\n",
       "      <td>...</td>\n",
       "      <td>1.515680</td>\n",
       "      <td>0.225686</td>\n",
       "      <td>0.074628</td>\n",
       "      <td>1.436258</td>\n",
       "      <td>0.316213</td>\n",
       "      <td>-0.044050</td>\n",
       "      <td>-0.192560</td>\n",
       "      <td>-1.278589</td>\n",
       "      <td>-0.644435</td>\n",
       "      <td>0.524522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>-0.204698</td>\n",
       "      <td>-0.714867</td>\n",
       "      <td>-0.355800</td>\n",
       "      <td>1.049916</td>\n",
       "      <td>-2.287774</td>\n",
       "      <td>-1.362642</td>\n",
       "      <td>-2.099994</td>\n",
       "      <td>-1.457708</td>\n",
       "      <td>-0.689214</td>\n",
       "      <td>-0.192812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.914716</td>\n",
       "      <td>-1.746692</td>\n",
       "      <td>0.021791</td>\n",
       "      <td>-1.626508</td>\n",
       "      <td>0.589722</td>\n",
       "      <td>-0.222012</td>\n",
       "      <td>0.293204</td>\n",
       "      <td>0.805142</td>\n",
       "      <td>0.764535</td>\n",
       "      <td>-1.319099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.915585</td>\n",
       "      <td>-0.601855</td>\n",
       "      <td>1.379885</td>\n",
       "      <td>0.307690</td>\n",
       "      <td>-0.102921</td>\n",
       "      <td>-0.692943</td>\n",
       "      <td>-1.256499</td>\n",
       "      <td>-0.317495</td>\n",
       "      <td>-0.090944</td>\n",
       "      <td>-0.349855</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.950687</td>\n",
       "      <td>-1.241490</td>\n",
       "      <td>1.299908</td>\n",
       "      <td>0.771946</td>\n",
       "      <td>-0.955785</td>\n",
       "      <td>-1.169201</td>\n",
       "      <td>0.469003</td>\n",
       "      <td>1.084787</td>\n",
       "      <td>-0.859178</td>\n",
       "      <td>1.236295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sky</th>\n",
       "      <td>0.169044</td>\n",
       "      <td>1.471745</td>\n",
       "      <td>0.013567</td>\n",
       "      <td>1.063449</td>\n",
       "      <td>0.357744</td>\n",
       "      <td>-0.128788</td>\n",
       "      <td>-0.132404</td>\n",
       "      <td>-0.883995</td>\n",
       "      <td>-0.243920</td>\n",
       "      <td>-0.272949</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.402419</td>\n",
       "      <td>-0.239137</td>\n",
       "      <td>0.468500</td>\n",
       "      <td>1.097794</td>\n",
       "      <td>0.945174</td>\n",
       "      <td>0.313995</td>\n",
       "      <td>0.457497</td>\n",
       "      <td>0.411889</td>\n",
       "      <td>-0.075854</td>\n",
       "      <td>-0.986178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blue</th>\n",
       "      <td>0.165463</td>\n",
       "      <td>-0.271816</td>\n",
       "      <td>-2.161096</td>\n",
       "      <td>-0.679418</td>\n",
       "      <td>-0.572815</td>\n",
       "      <td>1.365263</td>\n",
       "      <td>-0.077230</td>\n",
       "      <td>-1.544790</td>\n",
       "      <td>0.917948</td>\n",
       "      <td>0.061966</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.813910</td>\n",
       "      <td>-1.615587</td>\n",
       "      <td>0.085289</td>\n",
       "      <td>-1.780510</td>\n",
       "      <td>-1.223900</td>\n",
       "      <td>0.445015</td>\n",
       "      <td>0.080041</td>\n",
       "      <td>-1.952795</td>\n",
       "      <td>-0.270981</td>\n",
       "      <td>0.730214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beautiful</th>\n",
       "      <td>-0.264317</td>\n",
       "      <td>-0.042613</td>\n",
       "      <td>0.752842</td>\n",
       "      <td>-1.201490</td>\n",
       "      <td>-0.908022</td>\n",
       "      <td>0.917983</td>\n",
       "      <td>1.014262</td>\n",
       "      <td>1.188829</td>\n",
       "      <td>0.028253</td>\n",
       "      <td>0.166835</td>\n",
       "      <td>...</td>\n",
       "      <td>0.274835</td>\n",
       "      <td>-0.640499</td>\n",
       "      <td>-1.518954</td>\n",
       "      <td>1.361498</td>\n",
       "      <td>1.370613</td>\n",
       "      <td>-0.984414</td>\n",
       "      <td>2.618299</td>\n",
       "      <td>0.159707</td>\n",
       "      <td>-0.472797</td>\n",
       "      <td>0.703599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quick</th>\n",
       "      <td>-0.546409</td>\n",
       "      <td>-0.488233</td>\n",
       "      <td>-0.856448</td>\n",
       "      <td>-1.274190</td>\n",
       "      <td>0.915061</td>\n",
       "      <td>-0.738460</td>\n",
       "      <td>-0.933244</td>\n",
       "      <td>1.239057</td>\n",
       "      <td>0.147252</td>\n",
       "      <td>0.333560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.279685</td>\n",
       "      <td>-0.507048</td>\n",
       "      <td>0.570349</td>\n",
       "      <td>-0.287298</td>\n",
       "      <td>-1.963065</td>\n",
       "      <td>1.283551</td>\n",
       "      <td>0.488708</td>\n",
       "      <td>1.301658</td>\n",
       "      <td>-0.927166</td>\n",
       "      <td>0.627487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brown</th>\n",
       "      <td>1.420189</td>\n",
       "      <td>0.066746</td>\n",
       "      <td>-0.777580</td>\n",
       "      <td>0.414225</td>\n",
       "      <td>-1.025514</td>\n",
       "      <td>0.067859</td>\n",
       "      <td>1.813741</td>\n",
       "      <td>-0.857460</td>\n",
       "      <td>1.433506</td>\n",
       "      <td>1.313242</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.817376</td>\n",
       "      <td>0.111046</td>\n",
       "      <td>1.186138</td>\n",
       "      <td>-0.941866</td>\n",
       "      <td>-1.030077</td>\n",
       "      <td>-1.703512</td>\n",
       "      <td>1.113617</td>\n",
       "      <td>-0.363929</td>\n",
       "      <td>-0.682402</td>\n",
       "      <td>0.105342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fox</th>\n",
       "      <td>-0.212950</td>\n",
       "      <td>0.734865</td>\n",
       "      <td>-0.325665</td>\n",
       "      <td>-1.008388</td>\n",
       "      <td>0.757536</td>\n",
       "      <td>0.231809</td>\n",
       "      <td>0.462310</td>\n",
       "      <td>1.629333</td>\n",
       "      <td>-2.066199</td>\n",
       "      <td>1.296143</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.285876</td>\n",
       "      <td>0.069139</td>\n",
       "      <td>-0.854483</td>\n",
       "      <td>-0.238000</td>\n",
       "      <td>1.255570</td>\n",
       "      <td>1.788580</td>\n",
       "      <td>-0.146465</td>\n",
       "      <td>-0.436783</td>\n",
       "      <td>-0.054201</td>\n",
       "      <td>-0.151487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lazy</th>\n",
       "      <td>1.875924</td>\n",
       "      <td>1.042332</td>\n",
       "      <td>-0.434358</td>\n",
       "      <td>-0.422643</td>\n",
       "      <td>-1.084296</td>\n",
       "      <td>-0.435054</td>\n",
       "      <td>0.322612</td>\n",
       "      <td>-0.822072</td>\n",
       "      <td>0.063755</td>\n",
       "      <td>-0.634266</td>\n",
       "      <td>...</td>\n",
       "      <td>0.388472</td>\n",
       "      <td>-0.534687</td>\n",
       "      <td>-0.499022</td>\n",
       "      <td>-0.331182</td>\n",
       "      <td>0.681713</td>\n",
       "      <td>-0.131035</td>\n",
       "      <td>-1.034492</td>\n",
       "      <td>-0.561525</td>\n",
       "      <td>1.541132</td>\n",
       "      <td>-1.750968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows  100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0         1         2         3         4         5   \\\n",
       "the        0.949060  0.601485 -0.046146  0.622524 -0.912017 -0.862392   \n",
       "is        -0.204698 -0.714867 -0.355800  1.049916 -2.287774 -1.362642   \n",
       "and        0.915585 -0.601855  1.379885  0.307690 -0.102921 -0.692943   \n",
       "sky        0.169044  1.471745  0.013567  1.063449  0.357744 -0.128788   \n",
       "blue       0.165463 -0.271816 -2.161096 -0.679418 -0.572815  1.365263   \n",
       "beautiful -0.264317 -0.042613  0.752842 -1.201490 -0.908022  0.917983   \n",
       "quick     -0.546409 -0.488233 -0.856448 -1.274190  0.915061 -0.738460   \n",
       "brown      1.420189  0.066746 -0.777580  0.414225 -1.025514  0.067859   \n",
       "fox       -0.212950  0.734865 -0.325665 -1.008388  0.757536  0.231809   \n",
       "lazy       1.875924  1.042332 -0.434358 -0.422643 -1.084296 -0.435054   \n",
       "\n",
       "                 6         7         8         9     ...           90  \\\n",
       "the        0.208219 -2.355533 -0.207395 -0.069587    ...     1.515680   \n",
       "is        -2.099994 -1.457708 -0.689214 -0.192812    ...     0.914716   \n",
       "and       -1.256499 -0.317495 -0.090944 -0.349855    ...    -0.950687   \n",
       "sky       -0.132404 -0.883995 -0.243920 -0.272949    ...    -0.402419   \n",
       "blue      -0.077230 -1.544790  0.917948  0.061966    ...    -0.813910   \n",
       "beautiful  1.014262  1.188829  0.028253  0.166835    ...     0.274835   \n",
       "quick     -0.933244  1.239057  0.147252  0.333560    ...     0.279685   \n",
       "brown      1.813741 -0.857460  1.433506  1.313242    ...    -0.817376   \n",
       "fox        0.462310  1.629333 -2.066199  1.296143    ...    -1.285876   \n",
       "lazy       0.322612 -0.822072  0.063755 -0.634266    ...     0.388472   \n",
       "\n",
       "                 91        92        93        94        95        96  \\\n",
       "the        0.225686  0.074628  1.436258  0.316213 -0.044050 -0.192560   \n",
       "is        -1.746692  0.021791 -1.626508  0.589722 -0.222012  0.293204   \n",
       "and       -1.241490  1.299908  0.771946 -0.955785 -1.169201  0.469003   \n",
       "sky       -0.239137  0.468500  1.097794  0.945174  0.313995  0.457497   \n",
       "blue      -1.615587  0.085289 -1.780510 -1.223900  0.445015  0.080041   \n",
       "beautiful -0.640499 -1.518954  1.361498  1.370613 -0.984414  2.618299   \n",
       "quick     -0.507048  0.570349 -0.287298 -1.963065  1.283551  0.488708   \n",
       "brown      0.111046  1.186138 -0.941866 -1.030077 -1.703512  1.113617   \n",
       "fox        0.069139 -0.854483 -0.238000  1.255570  1.788580 -0.146465   \n",
       "lazy      -0.534687 -0.499022 -0.331182  0.681713 -0.131035 -1.034492   \n",
       "\n",
       "                 97        98        99  \n",
       "the       -1.278589 -0.644435  0.524522  \n",
       "is         0.805142  0.764535 -1.319099  \n",
       "and        1.084787 -0.859178  1.236295  \n",
       "sky        0.411889 -0.075854 -0.986178  \n",
       "blue      -1.952795 -0.270981  0.730214  \n",
       "beautiful  0.159707 -0.472797  0.703599  \n",
       "quick      1.301658 -0.927166  0.627487  \n",
       "brown     -0.363929 -0.682402  0.105342  \n",
       "fox       -0.436783 -0.054201 -0.151487  \n",
       "lazy      -0.561525  1.541132 -1.750968  \n",
       "\n",
       "[10 rows x 100 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.embeddings(torch.Tensor([list(range(0,vocab_size))]).long().cuda())\n",
    "\n",
    "pd.DataFrame(weights.view(-1,100).tolist(), index=list(id2word.values())[0:]).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CBOW(window_size*2,vocab_size)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "checkpoint = torch.load('Cbow_Weights')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "model.eval()\n",
    "# - or -\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9mYcRmygvEO"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CVM5Rq_9pfux",
    "outputId": "11b33fc5-4ae7-4e5c-d349-955a3c2ee987"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-0d1c3b565fea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdistance_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meuclidean_distances\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:4]+1] \n\u001b[0;32m      6\u001b[0m                  for search_term in ['the', 'fox', 'beautiful','brown','lazy']}\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "weights = weights.view(-1,100)\n",
    "distance_matrix = euclidean_distances(weights.detach().numpy())\n",
    "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:4]+1] \n",
    "                 for search_term in ['the', 'fox', 'beautiful','brown','lazy']}\n",
    "\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OT-1TyE3pfux"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CBOW - Notebook (Word2Vec).ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "e9acf0a5f7b368749e03ba162bc6f8dae21ce9d9285a8a40c5b041a2ff3bab4f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
